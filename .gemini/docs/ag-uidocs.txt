Directory structure:
└── docs/
    ├── README.md
    ├── ag_ui.md
    ├── docs.json
    ├── integrations.mdx
    ├── introduction.mdx
    ├── LICENSE
    ├── package.json
    ├── .prettierignore
    ├── .prettierrc
    ├── concepts/
    │   ├── agents.mdx
    │   ├── architecture.mdx
    │   ├── events.mdx
    │   ├── messages.mdx
    │   ├── state.mdx
    │   └── tools.mdx
    ├── development/
    │   ├── contributing.mdx
    │   ├── roadmap.mdx
    │   └── updates.mdx
    ├── drafts/
    │   ├── activity-events.mdx
    │   ├── generative-ui.mdx
    │   ├── interrupts.mdx
    │   ├── meta-events.mdx
    │   ├── multimodal-messages.mdx
    │   ├── overview.mdx
    │   ├── reasoning.mdx
    │   └── serialization.mdx
    ├── icons/
    │   ├── custom-icons.tsx
    │   └── index.tsx
    ├── images/
    │   └── left-illustration.avif
    ├── quickstart/
    │   ├── applications.mdx
    │   ├── clients.mdx
    │   ├── introduction.mdx
    │   ├── middleware.mdx
    │   └── server.mdx
    ├── sdk/
    │   ├── js/
    │   │   ├── encoder.mdx
    │   │   ├── overview.mdx
    │   │   ├── proto.mdx
    │   │   ├── client/
    │   │   │   ├── abstract-agent.mdx
    │   │   │   ├── http-agent.mdx
    │   │   │   ├── overview.mdx
    │   │   │   └── subscriber.mdx
    │   │   └── core/
    │   │       ├── events.mdx
    │   │       ├── overview.mdx
    │   │       └── types.mdx
    │   └── python/
    │       ├── core/
    │       │   ├── events.mdx
    │       │   ├── overview.mdx
    │       │   └── types.mdx
    │       └── encoder/
    │           └── overview.mdx
    ├── snippets/
    │   └── snippet-intro.mdx
    └── tutorials/
        ├── cursor.mdx
        └── debugging.mdx

================================================
FILE: docs/README.md
================================================
# Agent User Interaction Protocol Documentation

The official documentation for the [Agent User Interaction Protocol](https://ag-ui.com).

### Publishing Changes

Changes will be deployed to production automatically after pushing to the default branch.



================================================
FILE: docs/ag_ui.md
================================================
# AG‑UI: The Agent–User Interaction Protocol

*A horizontal standard to bring AI agents into user‑facing frontend applications.*

AG‑UI is the boundary layer where agents and users meet. It standardizes how agent state, UI intents, and user interactions flow between your model/agent runtime and your app’s frontend—so you can ship reliable, debuggable, user‑friendly agentic features fast.

---

## Built with the ecosystem

**First‑party partnerships & integrations**

> **Logo strip goes here** (e.g., LangGraph • CrewAI • Autogen 2 • LlamaIndex • Mastra • Pydantic AI • Vercel AI SDK • Next.js)

Short blurb: *AG‑UI works across leading agent frameworks and frontend stacks, with shared vocabulary and primitives that keep your UX consistent as your agents evolve.*

---

## Building blocks (today & upcoming)

- **Streaming chat** — Token‑level and tool‑event streaming for responsive UIs.
- **Static generative UI** — Render model output into stable, typed components.
- **Declarative generative UI** — Let agents propose UI trees; app decides what to mount.
- **Frontend tools** — Safe, typed tool calls that bridge agent logic to app actions.
- **Interrupts & human‑in‑the‑loop** — Pause, approve, edit, or steer mid‑flow.
- **In‑chat + in‑app interactions** — Chat commands alongside regular app controls.
- **Attachments & multimodality** — Files, images, audio, and structured payloads.
- **Thinking steps** — Expose summaries/redactions of chain‑of‑thought artifacts to users, safely.
- **Sub‑agent calls** — Orchestrate nested agents and delegate specialized tasks.
- **Agent steering** — Guardrails, policies, and UX affordances to keep agents on track.

> **CTA to deeper docs** → *See the full capability map in the docs.*

---

## Design patterns

Explore reusable interaction patterns for agentic UX:

- **Link‑out:** [AI‑UI Design Patterns →](/patterns) *(placeholder URL)*

---

## Why AG‑UI

**Agentic apps break the classic request/response contract.** Agents run for longer, stream work as they go, and make nondeterministic choices that can affect your UI and state. AG‑UI defines a clean, observable boundary so frontends remain predictable while agents stay flexible.

### What’s hard about user‑facing agents

- Agents are **long‑running** and **stream** intermediate work—often across multi‑turn sessions.
- Agents are **nondeterministic** and can **control UI** in ways that must be supervised.
- Apps must mix **structured + unstructured IO** (text, voice, tool calls, state updates).
- Agents need **composition**: agents **call sub‑agents**, often non-deterministically.

With AG‑UI, these become deliberate, well‑typed interactions rather than ad‑hoc wiring.

---

## Deeper proof (docs, demos, code)

| Framework / Platform    | What works today                       | Docs      | Demo      |
| ----------------------- | -------------------------------------- | --------- | --------- |
| LangGraph               | Streams, tools, interrupts, sub‑agents | [Docs](#) | [Demo](#) |
| CrewAI                  | Tools, action routing, steering        | [Docs](#) | [Demo](#) |
| Autogen 2               | Multi‑agent orchestration, messaging   | [Docs](#) | [Demo](#) |
| LlamaIndex              | Query/agent routing, UI intents        | [Docs](#) | [Demo](#) |
| OpenAI Realtime         | Live stream, events, attachments       | [Docs](#) | [Demo](#) |
| Vercel AI SDK / Next.js | Edge streaming, SSR hydration          | [Docs](#) | [Demo](#) |

> **Note:** Replace placeholders with actual URLs to docs and demos.

---

## Quick links

- **Get started** → */docs/getting-started* (placeholder)
- **Concepts** → */docs/concepts/agent-ui-boundary* (placeholder)
- **Reference** → */docs/reference* (placeholder)
- **Patterns** → */patterns* (placeholder)

---

## Optional section: How AG‑UI fits

- **Protocol**: Events, intents, and payload schemas shared by agents & apps.
- **Runtime adapters**: Bindings for popular agent frameworks.
- **Frontend kit**: Lightweight client + components to handle streaming & interrupts.
- **Observability hooks**: Surface interaction timelines for debugging & learning.

*(Include a simple diagram later: Agent(s) ⇄ AG‑UI Boundary ⇄ App UI/State)*




================================================
FILE: docs/docs.json
================================================
{
  "$schema": "https://mintlify.com/docs.json",
  "theme": "willow",
  "name": "Agent User Interaction Protocol",
  "colors": {
    "primary": "#09090b",
    "light": "#FAFAFA",
    "dark": "#09090b"
  },
  "favicon": "/favicon.svg",
  "navigation": {
    "tabs": [
      {
        "tab": "Documentation",
        "groups": [
          {
            "group": "Get Started",
            "pages": [
              "introduction",
              {
                "group": "Quickstart",
                "pages": [
                  "quickstart/applications",
                  {
                    "group": "Build integrations",
                    "pages": [
                      "quickstart/introduction",
                      "quickstart/server",
                      "quickstart/middleware"
                    ]
                  },
                  "quickstart/clients"
                ]
              }
            ]
          },
          {
            "group": "Concepts",
            "pages": [
              "concepts/architecture",
              "concepts/events",
              "concepts/agents",
              "concepts/messages",
              "concepts/state",
              "concepts/tools"
            ]
          },
          {
            "group": "Draft Proposals",
            "pages": [
              "drafts/overview",
              "drafts/activity-events",
              "drafts/reasoning",
              "drafts/serialization",
              "drafts/multimodal-messages",
              "drafts/interrupts",
              "drafts/generative-ui",
              "drafts/meta-events"
            ]
          },
          {
            "group": "Tutorials",
            "pages": ["tutorials/cursor", "tutorials/debugging"]
          },
          {
            "group": "Development",
            "pages": ["development/updates", "development/roadmap", "development/contributing"]
          }
        ]
      },
      {
        "tab": "SDKs",
        "icon": "book-open",
        "groups": [
          {
            "group": "TypeScript",
            "pages": [
              {
                "group": "@ag-ui/core",
                "pages": ["sdk/js/core/overview", "sdk/js/core/types", "sdk/js/core/events"]
              },
              {
                "group": "@ag-ui/client",
                "pages": [
                  "sdk/js/client/overview",
                  "sdk/js/client/abstract-agent",
                  "sdk/js/client/http-agent",
                  "sdk/js/client/subscriber"
                ]
              },
              "sdk/js/encoder",
              "sdk/js/proto"
            ]
          },
          {
            "group": "Python",
            "pages": [
              {
                "group": "ag_ui.core",
                "pages": [
                  "sdk/python/core/overview",
                  "sdk/python/core/types",
                  "sdk/python/core/events"
                ]
              },
              {
                "group": "ag_ui.encoder",
                "pages": ["sdk/python/encoder/overview"]
              }
            ]
          }
        ]
      }
    ],
    "global": {
      "anchors": [
        {
          "anchor": "TypeScript SDK",
          "href": "https://docs.ag-ui.com/sdk/js/core/overview",
          "icon": "square-js"
        },
        {
          "anchor": "Python SDK",
          "href": "https://docs.ag-ui.com/sdk/python/core/overview",
          "icon": "python"
        }
      ]
    }
  },
  "logo": {
    "light": "/logo/light.svg",
    "dark": "/logo/dark.svg"
  },
  "navbar": {
    "links": [
      {
        "label": "Discord",
        "href": "https://discord.gg/Jd3FzfdJa8",
        "icon": "server"
      }
    ],
    "primary": {
      "type": "github",
      "href": "https://github.com/ag-ui-protocol/ag-ui"
    }
  },
  "seo": {
    "metatags": {
      "og:image": "https://raw.githubusercontent.com/ag-ui-protocol/docs/logo/light.png"
    },
    "indexing": "navigable"
  },
  "integrations": {
    "posthog": {
      "apiKey": "phc_XZdymVYjrph9Mi0xZYGNyCKexxgblXRR1jMENCtdz5Q",
      "apiHost": "https://eu.posthog.com"
    }
  },
  "footer": {
    "socials": {
      "github": "https://github.com/ag-ui-protocol/ag-ui"
    }
  },
  "redirects": [
    {
      "source": "/quickstart",
      "destination": "/quickstart/build"
    },
    {
      "source": "/quickstart/connect",
      "destination": "/quickstart/middleware"
    },
    {
      "source": "/quickstart/build",
      "destination": "/quickstart/server"
    }
  ]
}



================================================
FILE: docs/integrations.mdx
================================================
---
title: Integrations
description: "A list of AG-UI integrations"
---

This page showcases various Agent User Interaction Protocol (AG-UI) integrations
that demonstrate the protocol's capabilities and versatility.

## Frontend Integrations

- **[CopilotKit](https://copilotkit.ai)** - AI Copilots for your product.

## Agent Frameworks

- **[Mastra](https://mastra.ai)** - The TypeScript Agent Framework
- **[LangGraph](https://www.langchain.com/langgraph)** - Balance agent control
  with agency
- **[CrewAI](https://crewai.com)** - Streamline workflows across industries with
  powerful AI agents.
- **[AG2](https://ag2.ai)** - The Open-Source AgentOS
- **[Agno](https://agno.com)** - A full-stack framework for building Multi-Agent Systems

Visit our
[GitHub Discussions](https://github.com/orgs/ag-ui-protocol/discussions) to
engage with the AG-UI community.



================================================
FILE: docs/introduction.mdx
================================================
---
title: AG-UI Overview
description: 
---

# The Agent–User Interaction (AG-UI) Protocol


AG-UI is an <u><strong>open</strong></u>, <u><strong>lightweight</strong></u>, <u><strong>event-based</strong></u> protocol that standardizes how AI agents connect to user-facing applications.


Built for simplicity and flexibility, it standardizes how agent state, UI intents, and user interactions flow between your model/agent runtime and user-facing frontend applications—to allow application developers to ship reliable, debuggable, user‑friendly agentic features fast while focusing on application needs and avoding complex ad-hoc wiring.



<div style={{textAlign: 'center', margin: '2rem 0'}}>
  <img src="/images/ag-ui-overview-with-partners.png" alt="AG-UI Overview" style={{maxWidth: '100%', height: 'auto', borderRadius: '8px'}} />
</div>

---

## Building blocks (today & upcoming)

<div style={{display: 'grid', gridTemplateColumns: 'repeat(2, 1fr)', gap: '1rem', margin: '1.5rem 0'}}>
  <div style={{border: '1px solid #e5e7eb', borderRadius: '8px', overflow: 'hidden', transition: 'all 0.3s ease', cursor: 'pointer'}} onMouseEnter={(e) => { e.currentTarget.style.borderColor = '#3b82f6'; e.currentTarget.style.boxShadow = '0 4px 12px rgba(59, 130, 246, 0.15)'; e.currentTarget.querySelector('.content').style.maxHeight = '200px'; e.currentTarget.querySelector('.content').style.opacity = '1'; }} onMouseLeave={(e) => { e.currentTarget.style.borderColor = '#e5e7eb'; e.currentTarget.style.boxShadow = 'none'; e.currentTarget.querySelector('.content').style.maxHeight = '0px'; e.currentTarget.querySelector('.content').style.opacity = '0'; }}>
    <div style={{padding: '1rem 1.5rem', backgroundColor: '#f9fafb', fontWeight: '600', color: '#374151'}}>
      Streaming chat
    </div>
    <div className="content" style={{maxHeight: '0px', opacity: '0', overflow: 'hidden', transition: 'all 0.3s ease', padding: '0 1.5rem', backgroundColor: '#ffffff'}}>
      <div style={{padding: '1rem 0', color: '#6b7280', lineHeight: '1.6'}}>
        Live token and event streaming for responsive multi turn sessions, with cancel and resume.
      </div>
    </div>
  </div>

  <div style={{border: '1px solid #e5e7eb', borderRadius: '8px', overflow: 'hidden', transition: 'all 0.3s ease', cursor: 'pointer'}} onMouseEnter={(e) => { e.currentTarget.style.borderColor = '#3b82f6'; e.currentTarget.style.boxShadow = '0 4px 12px rgba(59, 130, 246, 0.15)'; e.currentTarget.querySelector('.content').style.maxHeight = '200px'; e.currentTarget.querySelector('.content').style.opacity = '1'; }} onMouseLeave={(e) => { e.currentTarget.style.borderColor = '#e5e7eb'; e.currentTarget.style.boxShadow = 'none'; e.currentTarget.querySelector('.content').style.maxHeight = '0px'; e.currentTarget.querySelector('.content').style.opacity = '0'; }}>
    <div style={{padding: '1rem 1.5rem', backgroundColor: '#f9fafb', fontWeight: '600', color: '#374151'}}>
      Multimodality
    </div>
    <div className="content" style={{maxHeight: '0px', opacity: '0', overflow: 'hidden', transition: 'all 0.3s ease', padding: '0 1.5rem', backgroundColor: '#ffffff'}}>
      <div style={{padding: '1rem 0', color: '#6b7280', lineHeight: '1.6'}}>
        Typed attachments and real time media (files, images, audio, transcripts); supports voice, previews, annotations, provenance.
      </div>
    </div>
  </div>

  <div style={{border: '1px solid #e5e7eb', borderRadius: '8px', overflow: 'hidden', transition: 'all 0.3s ease', cursor: 'pointer'}} onMouseEnter={(e) => { e.currentTarget.style.borderColor = '#3b82f6'; e.currentTarget.style.boxShadow = '0 4px 12px rgba(59, 130, 246, 0.15)'; e.currentTarget.querySelector('.content').style.maxHeight = '200px'; e.currentTarget.querySelector('.content').style.opacity = '1'; }} onMouseLeave={(e) => { e.currentTarget.style.borderColor = '#e5e7eb'; e.currentTarget.style.boxShadow = 'none'; e.currentTarget.querySelector('.content').style.maxHeight = '0px'; e.currentTarget.querySelector('.content').style.opacity = '0'; }}>
    <div style={{padding: '1rem 1.5rem', backgroundColor: '#f9fafb', fontWeight: '600', color: '#374151'}}>
      Generative UI, static
    </div>
    <div className="content" style={{maxHeight: '0px', opacity: '0', overflow: 'hidden', transition: 'all 0.3s ease', padding: '0 1.5rem', backgroundColor: '#ffffff'}}>
      <div style={{padding: '1rem 0', color: '#6b7280', lineHeight: '1.6'}}>
        Render model output as stable, typed components under app control.
      </div>
    </div>
  </div>

  <div style={{border: '1px solid #e5e7eb', borderRadius: '8px', overflow: 'hidden', transition: 'all 0.3s ease', cursor: 'pointer'}} onMouseEnter={(e) => { e.currentTarget.style.borderColor = '#3b82f6'; e.currentTarget.style.boxShadow = '0 4px 12px rgba(59, 130, 246, 0.15)'; e.currentTarget.querySelector('.content').style.maxHeight = '200px'; e.currentTarget.querySelector('.content').style.opacity = '1'; }} onMouseLeave={(e) => { e.currentTarget.style.borderColor = '#e5e7eb'; e.currentTarget.style.boxShadow = 'none'; e.currentTarget.querySelector('.content').style.maxHeight = '0px'; e.currentTarget.querySelector('.content').style.opacity = '0'; }}>
    <div style={{padding: '1rem 1.5rem', backgroundColor: '#f9fafb', fontWeight: '600', color: '#374151'}}>
      Generative UI, declarative
    </div>
    <div className="content" style={{maxHeight: '0px', opacity: '0', overflow: 'hidden', transition: 'all 0.3s ease', padding: '0 1.5rem', backgroundColor: '#ffffff'}}>
      <div style={{padding: '1rem 0', color: '#6b7280', lineHeight: '1.6'}}>
        Small declarative language for constrained yet open-ended agent UIs; agents propose trees and constraints, the app validates and mounts.
      </div>
    </div>
  </div>

  <div style={{border: '1px solid #e5e7eb', borderRadius: '8px', overflow: 'hidden', transition: 'all 0.3s ease', cursor: 'pointer'}} onMouseEnter={(e) => { e.currentTarget.style.borderColor = '#3b82f6'; e.currentTarget.style.boxShadow = '0 4px 12px rgba(59, 130, 246, 0.15)'; e.currentTarget.querySelector('.content').style.maxHeight = '200px'; e.currentTarget.querySelector('.content').style.opacity = '1'; }} onMouseLeave={(e) => { e.currentTarget.style.borderColor = '#e5e7eb'; e.currentTarget.style.boxShadow = 'none'; e.currentTarget.querySelector('.content').style.maxHeight = '0px'; e.currentTarget.querySelector('.content').style.opacity = '0'; }}>
    <div style={{padding: '1rem 1.5rem', backgroundColor: '#f9fafb', fontWeight: '600', color: '#374151'}}>
      Shared state
    </div>
    <div className="content" style={{maxHeight: '0px', opacity: '0', overflow: 'hidden', transition: 'all 0.3s ease', padding: '0 1.5rem', backgroundColor: '#ffffff'}}>
      <div style={{padding: '1rem 0', color: '#6b7280', lineHeight: '1.6'}}>
        (Read-only & read-write). Typed store shared between agent and app, with streamed event-sourced diffs and conflict resolution for snappy collaboration.
      </div>
    </div>
  </div>

    <div style={{border: '1px solid #e5e7eb', borderRadius: '8px', overflow: 'hidden', transition: 'all 0.3s ease', cursor: 'pointer'}} onMouseEnter={(e) => { e.currentTarget.style.borderColor = '#3b82f6'; e.currentTarget.style.boxShadow = '0 4px 12px rgba(59, 130, 246, 0.15)'; e.currentTarget.querySelector('.content').style.maxHeight = '200px'; e.currentTarget.querySelector('.content').style.opacity = '1'; }} onMouseLeave={(e) => { e.currentTarget.style.borderColor = '#e5e7eb'; e.currentTarget.style.boxShadow = 'none'; e.currentTarget.querySelector('.content').style.maxHeight = '0px'; e.currentTarget.querySelector('.content').style.opacity = '0'; }}>
    <div style={{padding: '1rem 1.5rem', backgroundColor: '#f9fafb', fontWeight: '600', color: '#374151'}}>
      Thinking steps
    </div>
    <div className="content" style={{maxHeight: '0px', opacity: '0', overflow: 'hidden', transition: 'all 0.3s ease', padding: '0 1.5rem', backgroundColor: '#ffffff'}}>
      <div style={{padding: '1rem 0', color: '#6b7280', lineHeight: '1.6'}}>
        Visualize intermediate reasoning from traces and tool events; no raw chain of thought.
      </div>
    </div>
  </div>


  <div style={{border: '1px solid #e5e7eb', borderRadius: '8px', overflow: 'hidden', transition: 'all 0.3s ease', cursor: 'pointer'}} onMouseEnter={(e) => { e.currentTarget.style.borderColor = '#3b82f6'; e.currentTarget.style.boxShadow = '0 4px 12px rgba(59, 130, 246, 0.15)'; e.currentTarget.querySelector('.content').style.maxHeight = '200px'; e.currentTarget.querySelector('.content').style.opacity = '1'; }} onMouseLeave={(e) => { e.currentTarget.style.borderColor = '#e5e7eb'; e.currentTarget.style.boxShadow = 'none'; e.currentTarget.querySelector('.content').style.maxHeight = '0px'; e.currentTarget.querySelector('.content').style.opacity = '0'; }}>
    <div style={{padding: '1rem 1.5rem', backgroundColor: '#f9fafb', fontWeight: '600', color: '#374151'}}>
      Frontend tool calls
    </div>
    <div className="content" style={{maxHeight: '0px', opacity: '0', overflow: 'hidden', transition: 'all 0.3s ease', padding: '0 1.5rem', backgroundColor: '#ffffff'}}>
      <div style={{padding: '1rem 0', color: '#6b7280', lineHeight: '1.6'}}>
        Typed handoffs from agent to frontend-executed actions, and back.
      </div>
    </div>
  </div>

  <div style={{border: '1px solid #e5e7eb', borderRadius: '8px', overflow: 'hidden', transition: 'all 0.3s ease', cursor: 'pointer'}} onMouseEnter={(e) => { e.currentTarget.style.borderColor = '#3b82f6'; e.currentTarget.style.boxShadow = '0 4px 12px rgba(59, 130, 246, 0.15)'; e.currentTarget.querySelector('.content').style.maxHeight = '200px'; e.currentTarget.querySelector('.content').style.opacity = '1'; }} onMouseLeave={(e) => { e.currentTarget.style.borderColor = '#e5e7eb'; e.currentTarget.style.boxShadow = 'none'; e.currentTarget.querySelector('.content').style.maxHeight = '0px'; e.currentTarget.querySelector('.content').style.opacity = '0'; }}>
    <div style={{padding: '1rem 1.5rem', backgroundColor: '#f9fafb', fontWeight: '600', color: '#374151'}}>
      Backend tool rendering & side effects
    </div>
    <div className="content" style={{maxHeight: '0px', opacity: '0', overflow: 'hidden', transition: 'all 0.3s ease', padding: '0 1.5rem', backgroundColor: '#ffffff'}}>
      <div style={{padding: '1rem 0', color: '#6b7280', lineHeight: '1.6'}}>
        Visualize backend tool outputs in app and chat, emit side effects as first-class events.
      </div>
    </div>
  </div>

  <div style={{border: '1px solid #e5e7eb', borderRadius: '8px', overflow: 'hidden', transition: 'all 0.3s ease', cursor: 'pointer'}} onMouseEnter={(e) => { e.currentTarget.style.borderColor = '#3b82f6'; e.currentTarget.style.boxShadow = '0 4px 12px rgba(59, 130, 246, 0.15)'; e.currentTarget.querySelector('.content').style.maxHeight = '200px'; e.currentTarget.querySelector('.content').style.opacity = '1'; }} onMouseLeave={(e) => { e.currentTarget.style.borderColor = '#e5e7eb'; e.currentTarget.style.boxShadow = 'none'; e.currentTarget.querySelector('.content').style.maxHeight = '0px'; e.currentTarget.querySelector('.content').style.opacity = '0'; }}>
    <div style={{padding: '1rem 1.5rem', backgroundColor: '#f9fafb', fontWeight: '600', color: '#374151'}}>
      Interrupts (human in the loop)
    </div>
    <div className="content" style={{maxHeight: '0px', opacity: '0', overflow: 'hidden', transition: 'all 0.3s ease', padding: '0 1.5rem', backgroundColor: '#ffffff'}}>
      <div style={{padding: '1rem 0', color: '#6b7280', lineHeight: '1.6'}}>
        Pause, approve, edit, retry, or escalate mid flow without losing state.
      </div>
    </div>
  </div>


  <div style={{border: '1px solid #e5e7eb', borderRadius: '8px', overflow: 'hidden', transition: 'all 0.3s ease', cursor: 'pointer'}} onMouseEnter={(e) => { e.currentTarget.style.borderColor = '#3b82f6'; e.currentTarget.style.boxShadow = '0 4px 12px rgba(59, 130, 246, 0.15)'; e.currentTarget.querySelector('.content').style.maxHeight = '200px'; e.currentTarget.querySelector('.content').style.opacity = '1'; }} onMouseLeave={(e) => { e.currentTarget.style.borderColor = '#e5e7eb'; e.currentTarget.style.boxShadow = 'none'; e.currentTarget.querySelector('.content').style.maxHeight = '0px'; e.currentTarget.querySelector('.content').style.opacity = '0'; }}>
    <div style={{padding: '1rem 1.5rem', backgroundColor: '#f9fafb', fontWeight: '600', color: '#374151'}}>
      Sub-agents and composition
    </div>
    <div className="content" style={{maxHeight: '0px', opacity: '0', overflow: 'hidden', transition: 'all 0.3s ease', padding: '0 1.5rem', backgroundColor: '#ffffff'}}>
      <div style={{padding: '1rem 0', color: '#6b7280', lineHeight: '1.6'}}>
        Nested delegation with scoped state, tracing, and cancellation.
      </div>
    </div>
  </div>

  <div style={{border: '1px solid #e5e7eb', borderRadius: '8px', overflow: 'hidden', transition: 'all 0.3s ease', cursor: 'pointer'}} onMouseEnter={(e) => { e.currentTarget.style.borderColor = '#3b82f6'; e.currentTarget.style.boxShadow = '0 4px 12px rgba(59, 130, 246, 0.15)'; e.currentTarget.querySelector('.content').style.maxHeight = '200px'; e.currentTarget.querySelector('.content').style.opacity = '1'; }} onMouseLeave={(e) => { e.currentTarget.style.borderColor = '#e5e7eb'; e.currentTarget.style.boxShadow = 'none'; e.currentTarget.querySelector('.content').style.maxHeight = '0px'; e.currentTarget.querySelector('.content').style.opacity = '0'; }}>
    <div style={{padding: '1rem 1.5rem', backgroundColor: '#f9fafb', fontWeight: '600', color: '#374151'}}>
      Agent steering
    </div>
    <div className="content" style={{maxHeight: '0px', opacity: '0', overflow: 'hidden', transition: 'all 0.3s ease', padding: '0 1.5rem', backgroundColor: '#ffffff'}}>
      <div style={{padding: '1rem 0', color: '#6b7280', lineHeight: '1.6'}}>
        Dynamically redirect agent execution with real-time user input to guide behavior and outcomes.
      </div>
    </div>
  </div>

  <div style={{border: '1px solid #e5e7eb', borderRadius: '8px', overflow: 'hidden', transition: 'all 0.3s ease', cursor: 'pointer'}} onMouseEnter={(e) => { e.currentTarget.style.borderColor = '#3b82f6'; e.currentTarget.style.boxShadow = '0 4px 12px rgba(59, 130, 246, 0.15)'; e.currentTarget.querySelector('.content').style.maxHeight = '200px'; e.currentTarget.querySelector('.content').style.opacity = '1'; }} onMouseLeave={(e) => { e.currentTarget.style.borderColor = '#e5e7eb'; e.currentTarget.style.boxShadow = 'none'; e.currentTarget.querySelector('.content').style.maxHeight = '0px'; e.currentTarget.querySelector('.content').style.opacity = '0'; }}>
    <div style={{padding: '1rem 1.5rem', backgroundColor: '#f9fafb', fontWeight: '600', color: '#374151'}}>
      Tool output streaming
    </div>
    <div className="content" style={{maxHeight: '0px', opacity: '0', overflow: 'hidden', transition: 'all 0.3s ease', padding: '0 1.5rem', backgroundColor: '#ffffff'}}>
      <div style={{padding: '1rem 0', color: '#6b7280', lineHeight: '1.6'}}>
        Stream tool results and logs so UIs can render long-running effects in real time.
      </div>
    </div>
  </div>

  <div style={{border: '1px solid #e5e7eb', borderRadius: '8px', overflow: 'hidden', transition: 'all 0.3s ease', cursor: 'pointer'}} onMouseEnter={(e) => { e.currentTarget.style.borderColor = '#3b82f6'; e.currentTarget.style.boxShadow = '0 4px 12px rgba(59, 130, 246, 0.15)'; e.currentTarget.querySelector('.content').style.maxHeight = '200px'; e.currentTarget.querySelector('.content').style.opacity = '1'; }} onMouseLeave={(e) => { e.currentTarget.style.borderColor = '#e5e7eb'; e.currentTarget.style.boxShadow = 'none'; e.currentTarget.querySelector('.content').style.maxHeight = '0px'; e.currentTarget.querySelector('.content').style.opacity = '0'; }}>
    <div style={{padding: '1rem 1.5rem', backgroundColor: '#f9fafb', fontWeight: '600', color: '#374151'}}>
      Custom events
    </div>
    <div className="content" style={{maxHeight: '0px', opacity: '0', overflow: 'hidden', transition: 'all 0.3s ease', padding: '0 1.5rem', backgroundColor: '#ffffff'}}>
      <div style={{padding: '1rem 0', color: '#6b7280', lineHeight: '1.6'}}>
        Open-ended data exchange for needs not covered by the protocol.
      </div>
    </div>
  </div>
</div>



---

## Why Agentic Apps need AG-UI

Agentic applications break the simple request/response model that dominated frontend-backend development in the pre-agentic era: a client makes a request, the server returns data, the client renders it, and the interaction ends.

#### The requirements of user‑facing agents

While agents are just software, they exhibit characteristics that make them challenging to serve behind traditional REST/GraphQL APIs:

- Agents are **long‑running** and **stream** intermediate work—often across multi‑turn sessions.
- Agents are **nondeterministic** and can **control application UI nondeterministically**.
- Agents simultanously mix **structured + unstructured IO** (e.g. text & voice, alongside tool calls and state updates).
- Agents need user-interactive **composition**: e.g. they may call sub‑agents, often recursively.
- And more...

AG-UI is an event-based protocol that enables dynamic communication between agentic frontends and backends. It builds on top of the foundational protocols of the web (HTTP, WebSockets) as an abstraction layer designed for the agentic age—bridging the gap between traditional client-server architectures and the dynamic, stateful nature of AI agents.

## The AI protocol landscape

AG-UI has emerged as the 3rd leg of the AI protocol landscape:
<div style={{textAlign: 'center', margin: '2rem 0'}}>
  <img src="/images/ai-protocol-stack.png" alt="AI Protocol Stack" style={{maxWidth: '40%', height: 'auto', borderRadius: '8px'}} />
</div>

- MCP: Connects agents to tool and to context.
- A2A: Connects agents to other agents.
- **AG-UI:** Connects agents to users (through user-facing applications)


These protocols are complimentary and have distinct technical goals; a single agent can and often does use all 3 simultanously.
Where these protocols intersect, there are opportunities for seamless handshakes facilitating interoperability—work on these integration points is actively ongoing.
AG-UI's mandate is to support the full set of building blocks required by modern agentic applications.




---

## AG-UI in action

<div style={{textAlign: 'center', margin: '3rem 0 1rem 0'}}>
  <video 
    width="100%" 
    height="auto" 
    autoPlay 
    muted 
    loop 
    controls 
    style={{maxWidth: '800px', borderRadius: '12px', boxShadow: '0 8px 32px rgba(0, 0, 0, 0.12)'}}
  >
    <source src="/videos/Dojo-overview.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
</div>

<Callout type="info" icon="lightbulb">
You can see demo apps of the AG-UI features with the framework of your choice, with preview, code and walkthrough docs in the [AG-UI Dojo](dojo.ag-ui.com)
</Callout>

---

## Supported Frameworks

AG-UI was born from an initial partnership between CopilotKit, LangGraph and CrewAI - and is steadily gaining integrations across the wider AI developer ecosystem.

| Framework | Docs | Demos |
| :----------------------- | :--- | :---- |
| [LangGraph](https://www.langchain.com/langgraph) | [Documentation](https://docs.copilotkit.ai/langgraph/) | [Live Demo](https://dojo.ag-ui.com/langgraph-fastapi/feature/shared_state) |
| [CrewAI](https://crewai.com/) | [Documentation](https://docs.copilotkit.ai/crewai-flows) | [Live Demo](https://dojo.ag-ui.com/crewai/feature/shared_state) |
| [Mastra](https://mastra.ai/) | [Documentation](https://docs.copilotkit.ai/mastra/) | [Live Demo](https://dojo.ag-ui.com/mastra) |
| [AG2](https://ag2.ai/) | [Documentation](https://docs.copilotkit.ai/ag2/) | Coming Soon |
| [Agno](https://github.com/agno-agi/agno) | [Documentation](https://docs.copilotkit.ai/agno/) | [Live Demo](https://dojo.ag-ui.com/agno) |
| [LlamaIndex](https://github.com/run-llama/llama_index) | [Documentation](https://docs.copilotkit.ai/llamaindex/) | [Live Demo](https://dojo.ag-ui.com/llamaindex/feature/shared_state) |
| [Pydantic AI](https://github.com/pydantic/pydantic-ai) | [Documentation](https://docs.copilotkit.ai/pydantic-ai/) | [Live Demo](https://dojo.ag-ui.com/pydantic-ai/feature/shared_state) |
| [Google ADK](https://google.github.io/adk-docs/get-started/) | [Documentation](https://docs.copilotkit.ai/adk) | [Live Demo](https://dojo.ag-ui.com/adk-middleware) |
| [AWS Bedrock Agents](https://aws.amazon.com/bedrock/agents/) | In Progress | Coming Soon |
| [AWS Strands Agents](https://github.com/strands-agents/sdk-python) | In Progress | Coming Soon |
| [Vercel AI SDK](https://github.com/vercel/ai) | In Progress | Coming Soon |
| [OpenAI Agent SDK](https://openai.github.io/openai-agents-python/) | In Progress | Coming Soon |
| [Cloudflare Agents](https://developers.cloudflare.com/agents/) | Open to Contributions | Coming Soon |

---

## Quick Start

Choose the path that fits your needs:

<CardGroup cols={3}>
  <Card
    title="Build agentic applications"
    icon="rocket"
    href="/quickstart/applications"
    color="#3B82F6"
    iconType="solid"
  >
    Build agentic applications powered by AG-UI compatible agents.
  </Card>

  <Card
    title="Build new AG-UI integrations"
    icon="plug"
    href="/quickstart/introduction"
    color="#3B82F6"
    iconType="solid"
  >
    Build integrations for new agent frameworks, custom in-house solutions, or use AG-UI without any agent framework.
  </Card>

  <Card
    title="Build AG-UI compatible clients"
    icon="desktop"
    href="/quickstart/clients"
    color="#3B82F6"
    iconType="solid"
  >
    Build new clients for AG-UI-compatible agents (web, mobile, slack, messaging, etc.)
  </Card>

</CardGroup>

## Explore AG-UI

Dive deeper into AG-UI's core concepts and capabilities:

<CardGroup cols={2}>
  <Card
    title="Core architecture"
    icon="sitemap"
    iconType="light"
    color="#3B82F6"
    href="/concepts/architecture"
  >
    Understand how AG-UI connects agents, protocols, and front-ends
  </Card>

  <Card
    title="Events"
    icon="bolt"
    iconType="light"
    color="#3B82F6"
    href="/concepts/events"
  >
    Learn about AG-UI's event-driven protocol
  </Card>
</CardGroup>

## Resources

Explore guides, tools, and integrations to help you build, optimize, and extend
your AG-UI implementation. These resources cover everything from practical
development workflows to debugging techniques.

<CardGroup cols={2}>
  <Card
    title="Developing with Cursor"
    icon="rocket"
    iconType="light"
    color="#3B82F6"
    href="/tutorials/cursor"
  >
    Use Cursor to build AG-UI implementations faster
  </Card>
  <Card
    title="Troubleshooting AG-UI"
    icon="bug"
    iconType="light"
    color="#3B82F6"
    href="/tutorials/debugging"
  >
    Fix common issues when working with AG-UI servers and clients
  </Card>
</CardGroup>


## Contributing

Want to contribute? Check out our
[Contributing Guide](/development/contributing) to learn how you can help
improve AG-UI.

## Support and Feedback

Here's how to get help or provide feedback:

- For bug reports and feature requests related to the AG-UI specification, SDKs,
  or documentation (open source), please
  [create a GitHub issue](https://github.com/ag-ui-protocol)
- For discussions or Q&A about the AG-UI specification, use the
  [specification discussions](https://github.com/ag-ui-protocol/specification/discussions)
- For discussions or Q&A about other AG-UI open source components, use the
  [organization discussions](https://github.com/orgs/ag-ui-protocol/discussions)



================================================
FILE: docs/LICENSE
================================================
Copyright (c) 2025 Tawkit Inc.
Copyright (c) 2025 Markus Ecker

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.



================================================
FILE: docs/package.json
================================================
{
  "name": "docs",
  "scripts": {
    "dev": "mintlify dev --port=4000",
    "build": "mintlify build"
  },
  "dependencies": {
    "mintlify": "^4.0.459",
    "react-icons": "^5.3.0",
    "@icons-pack/react-simple-icons": "^11.2.0", 
    "lucide-react": "^0.446.0"
  }
}



================================================
FILE: docs/.prettierignore
================================================




================================================
FILE: docs/.prettierrc
================================================
{
  "printWidth": 100,
  "singleQuote": false,
  "semi": true,
  "tabWidth": 2,
  "overrides": [
    {
      "files": "*.mdx",
      "options": {
        "printWidth": 80,
        "proseWrap": "always",
        "semi": false,
        "singleQuote": false
      }
    }
  ]
}



================================================
FILE: docs/concepts/agents.mdx
================================================
---
title: "Agents"
description: "Learn about agents in the Agent User Interaction Protocol"
---

# Agents

Agents are the core components in the AG-UI protocol that process requests and
generate responses. They establish a standardized way for front-end applications
to communicate with AI services through a consistent interface, regardless of
the underlying implementation.

## What is an Agent?

In AG-UI, an agent is a class that:

1. Manages conversation state and message history
2. Processes incoming messages and context
3. Generates responses through an event-driven streaming interface
4. Follows a standardized protocol for communication

Agents can be implemented to connect with any AI service, including:

- Large language models (LLMs) like GPT-4 or Claude
- Custom AI systems
- Retrieval augmented generation (RAG) systems
- Multi-agent systems

## Agent Architecture

All agents in AG-UI extend the `AbstractAgent` class, which provides the
foundation for:

- State management
- Message history tracking
- Event stream processing
- Tool usage

```typescript
import { AbstractAgent } from "@ag-ui/client"

class MyAgent extends AbstractAgent {
  protected run(input: RunAgentInput): RunAgent {
    // Implementation details
  }
}
```

### Core Components

AG-UI agents have several key components:

1. **Configuration**: Agent ID, thread ID, and initial state
2. **Messages**: Conversation history with user and assistant messages
3. **State**: Structured data that persists across interactions
4. **Events**: Standardized messages for communication with clients
5. **Tools**: Functions that agents can use to interact with external systems

## Agent Types

AG-UI provides different agent implementations to suit various needs:

### AbstractAgent

The base class that all agents extend. It handles core event processing, state
management, and message history.

### HttpAgent

A concrete implementation that connects to remote AI services via HTTP:

```typescript
import { HttpAgent } from "@ag-ui/client"

const agent = new HttpAgent({
  url: "https://your-agent-endpoint.com/agent",
  headers: {
    Authorization: "Bearer your-api-key",
  },
})
```

### Custom Agents

You can create custom agents to integrate with any AI service by extending
`AbstractAgent`:

```typescript
class CustomAgent extends AbstractAgent {
  // Custom properties and methods

  protected run(input: RunAgentInput): RunAgent {
    // Implement the agent's logic
  }
}
```

## Implementing Agents

### Basic Implementation

To create a custom agent, extend the `AbstractAgent` class and implement the
required `run` method:

```typescript
import {
  AbstractAgent,
  RunAgent,
  RunAgentInput,
  EventType,
  BaseEvent,
} from "@ag-ui/client"
import { Observable } from "rxjs"

class SimpleAgent extends AbstractAgent {
  protected run(input: RunAgentInput): RunAgent {
    const { threadId, runId } = input

    return () =>
      new Observable<BaseEvent>((observer) => {
        // Emit RUN_STARTED event
        observer.next({
          type: EventType.RUN_STARTED,
          threadId,
          runId,
        })

        // Send a message
        const messageId = Date.now().toString()

        // Message start
        observer.next({
          type: EventType.TEXT_MESSAGE_START,
          messageId,
          role: "assistant",
        })

        // Message content
        observer.next({
          type: EventType.TEXT_MESSAGE_CONTENT,
          messageId,
          delta: "Hello, world!",
        })

        // Message end
        observer.next({
          type: EventType.TEXT_MESSAGE_END,
          messageId,
        })

        // Emit RUN_FINISHED event
        observer.next({
          type: EventType.RUN_FINISHED,
          threadId,
          runId,
        })

        // Complete the observable
        observer.complete()
      })
  }
}
```

## Agent Capabilities

Agents in the AG-UI protocol provide a rich set of capabilities that enable
sophisticated AI interactions:

### Interactive Communication

Agents establish bi-directional communication channels with front-end
applications through event streams. This enables:

- Real-time streaming responses character-by-character
- Immediate feedback loops between user and AI
- Progress indicators for long-running operations
- Structured data exchange in both directions

### Tool Usage

Agents can use tools to perform actions and access external resources.
Importantly, tools are defined and passed in from the front-end application to
the agent, allowing for a flexible and extensible system:

```typescript
// Tool definition
const confirmAction = {
  name: "confirmAction",
  description: "Ask the user to confirm a specific action before proceeding",
  parameters: {
    type: "object",
    properties: {
      action: {
        type: "string",
        description: "The action that needs user confirmation",
      },
      importance: {
        type: "string",
        enum: ["low", "medium", "high", "critical"],
        description: "The importance level of the action",
      },
      details: {
        type: "string",
        description: "Additional details about the action",
      },
    },
    required: ["action"],
  },
}

// Running an agent with tools from the frontend
agent.runAgent({
  tools: [confirmAction], // Frontend-defined tools passed to the agent
  // other parameters
})
```

Tools are invoked through a sequence of events:

1. `TOOL_CALL_START`: Indicates the beginning of a tool call
2. `TOOL_CALL_ARGS`: Streams the arguments for the tool call
3. `TOOL_CALL_END`: Marks the completion of the tool call

Front-end applications can then execute the tool and provide results back to the
agent. This bidirectional flow enables sophisticated human-in-the-loop workflows
where:

- The agent can request specific actions be performed
- Humans can execute those actions with appropriate judgment
- Results are fed back to the agent for continued reasoning
- The agent maintains awareness of all decisions made in the process

This mechanism is particularly powerful for implementing interfaces where AI and
humans collaborate. For example, [CopilotKit](https://docs.copilotkit.ai/)
leverages this exact pattern with their
[`useCopilotAction`](https://docs.copilotkit.ai/guides/frontend-actions) hook,
which provides a simplified way to define and handle tools in React
applications.

By keeping the AI informed about human decisions through the tool mechanism,
applications can maintain context and create more natural collaborative
experiences between users and AI assistants.

### State Management

Agents maintain a structured state that persists across interactions. This state
can be:

- Updated incrementally through `STATE_DELTA` events
- Completely refreshed with `STATE_SNAPSHOT` events
- Accessed by both the agent and front-end
- Used to store user preferences, conversation context, or application state

```typescript
// Accessing agent state
console.log(agent.state.preferences)

// State is automatically updated during agent runs
agent.runAgent().subscribe((event) => {
  if (event.type === EventType.STATE_DELTA) {
    // State has been updated
    console.log("New state:", agent.state)
  }
})
```

### Multi-Agent Collaboration

AG-UI supports agent-to-agent handoff and collaboration:

- Agents can delegate tasks to other specialized agents
- Multiple agents can work together in a coordinated workflow
- State and context can be transferred between agents
- The front-end maintains a consistent experience across agent transitions

For example, a general assistant agent might hand off to a specialized coding
agent when programming help is needed, passing along the conversation context
and specific requirements.

### Human-in-the-Loop Workflows

Agents support human intervention and assistance:

- Agents can request human input on specific decisions
- Front-ends can pause agent execution and resume it after human feedback
- Human experts can review and modify agent outputs before they're finalized
- Hybrid workflows combine AI efficiency with human judgment

This enables applications where the agent acts as a collaborative partner rather
than an autonomous system.

### Conversational Memory

Agents maintain a complete history of conversation messages:

- Past interactions inform future responses
- Message history is synchronized between client and server
- Messages can include rich content (text, structured data, references)
- The context window can be managed to focus on relevant information

```typescript
// Accessing message history
console.log(agent.messages)

// Adding a new user message
agent.messages.push({
  id: "msg_123",
  role: "user",
  content: "Can you explain that in more detail?",
})
```

### Metadata and Instrumentation

Agents can emit metadata about their internal processes:

- Reasoning steps through custom events
- Performance metrics and timing information
- Source citations and reference tracking
- Confidence scores for different response options

This allows front-ends to provide transparency into the agent's decision-making
process and help users understand how conclusions were reached.

## Using Agents

Once you've implemented or instantiated an agent, you can use it like this:

```typescript
// Create an agent instance
const agent = new HttpAgent({
  url: "https://your-agent-endpoint.com/agent",
})

// Add initial messages if needed
agent.messages = [
  {
    id: "1",
    role: "user",
    content: "Hello, how can you help me today?",
  },
]

// Run the agent
agent
  .runAgent({
    runId: "run_123",
    tools: [], // Optional tools
    context: [], // Optional context
  })
  .subscribe({
    next: (event) => {
      // Handle different event types
      switch (event.type) {
        case EventType.TEXT_MESSAGE_CONTENT:
          console.log("Content:", event.delta)
          break
        // Handle other events
      }
    },
    error: (error) => console.error("Error:", error),
    complete: () => console.log("Run complete"),
  })
```

## Agent Configuration

Agents accept configuration through the constructor:

```typescript
interface AgentConfig {
  agentId?: string // Unique identifier for the agent
  description?: string // Human-readable description
  threadId?: string // Conversation thread identifier
  initialMessages?: Message[] // Initial messages
  initialState?: State // Initial state object
}

// Using the configuration
const agent = new HttpAgent({
  agentId: "my-agent-123",
  description: "A helpful assistant",
  threadId: "thread-456",
  initialMessages: [
    { id: "1", role: "system", content: "You are a helpful assistant." },
  ],
  initialState: { preferredLanguage: "English" },
})
```

## Agent State Management

AG-UI agents maintain state across interactions:

```typescript
// Access current state
console.log(agent.state)

// Access messages
console.log(agent.messages)

// Clone an agent with its state
const clonedAgent = agent.clone()
```

## Conclusion

Agents are the foundation of the AG-UI protocol, providing a standardized way to
connect front-end applications with AI services. By implementing the
`AbstractAgent` class, you can create custom integrations with any AI service
while maintaining a consistent interface for your applications.

The event-driven architecture enables real-time, streaming interactions that are
essential for modern AI applications, and the standardized protocol ensures
compatibility across different implementations.



================================================
FILE: docs/concepts/architecture.mdx
================================================
---
title: "Core architecture"
description: "Understand how AG-UI connects front-end applications to AI agents"
---

Agent User Interaction Protocol (AG-UI) is built on a flexible, event-driven
architecture that enables seamless, efficient communication between front-end
applications and AI agents. This document covers the core architectural
components and concepts.

## Design Principles

AG-UI is designed to be lightweight and minimally opinionated, making it easy to
integrate with a wide range of agent implementations. The protocol's flexibility
comes from its simple requirements:

1. **Event-Driven Communication**: Agents need to emit any of the 16
   standardized event types during execution, creating a stream of updates that
   clients can process.

2. **Bidirectional Interaction**: Agents accept input from users, enabling
   collaborative workflows where humans and AI work together seamlessly.

The protocol includes a built-in middleware layer that maximizes compatibility
in two key ways:

- **Flexible Event Structure**: Events don't need to match AG-UI's format
  exactly—they just need to be AG-UI-compatible. This allows existing agent
  frameworks to adapt their native event formats with minimal effort.

- **Transport Agnostic**: AG-UI doesn't mandate how events are delivered,
  supporting various transport mechanisms including Server-Sent Events (SSE),
  webhooks, WebSockets, and more. This flexibility lets developers choose the
  transport that best fits their architecture.

This pragmatic approach makes AG-UI easy to adopt without requiring major
changes to existing agent implementations or frontend applications.

## Architectural Overview

AG-UI follows a client-server architecture that standardizes communication
between agents and applications:

```mermaid
flowchart LR
    subgraph "Frontend"
        App["Application"]
        Client["AG-UI Client"]
    end

    subgraph "Backend"
        A1["AI Agent A"]
        P["Secure Proxy"]
        A2["AI Agent B"]
        A3["AI Agent C"]
    end

    App <--> Client
    Client <-->|"AG-UI Protocol"| A1
    Client <-->|"AG-UI Protocol"| P
    P <-->|"AG-UI Protocol"| A2
    P <-->|"AG-UI Protocol"| A3

    class P mintStyle;
    classDef mintStyle fill:#E0F7E9,stroke:#66BB6A,stroke-width:2px,color:#000000;

    style App rx:5, ry:5;
    style Client rx:5, ry:5;
    style A1 rx:5, ry:5;
    style P rx:5, ry:5;
    style A2 rx:5, ry:5;
    style A3 rx:5, ry:5;
```

- **Application**: User-facing apps (i.e. chat or any AI-enabled application).
- **AG-UI Client**: Generic communication clients like `HttpAgent` or
  specialized clients for connecting to existing protocols.
- **Agents**: Backend AI agents that process requests and generate streaming
  responses.
- **Secure Proxy**: Backend services that provide additional capabilities and
  act as a secure proxy.

## Core components

### Protocol layer

AG-UI's protocol layer provides a flexible foundation for agent communication.

- **Universal compatibility**: Connect to any protocol by implementing
  `run(input: RunAgentInput) -> Observable<BaseEvent>`

The protocol's primary abstraction enables applications to run agents and
receive a stream of events:

{/* prettier-ignore */}
```typescript
// Core agent execution interface
type RunAgent = () => Observable<BaseEvent>

class MyAgent extends AbstractAgent {
  run(input: RunAgentInput): RunAgent {
    const { threadId, runId } = input
    return () =>
      from([
        { type: EventType.RUN_STARTED, threadId, runId },
        {
          type: EventType.MESSAGES_SNAPSHOT,
          messages: [
            { id: "msg_1", role: "assistant", content: "Hello, world!" }
          ],
        },
        { type: EventType.RUN_FINISHED, threadId, runId },
      ])
  }
}
```

### Standard HTTP client

AG-UI offers a standard HTTP client `HttpAgent` that can be used to connect to
any endpoint that accepts POST requests with a body of type `RunAgentInput` and
sends a stream of `BaseEvent` objects.

`HttpAgent` supports the following transports:

- **HTTP SSE (Server-Sent Events)**

  - Text-based streaming for wide compatibility
  - Easy to read and debug

- **HTTP binary protocol**
  - Highly performant and space-efficient custom transport
  - Robust binary serialization for production environments

### Message types

AG-UI defines several event categories for different aspects of agent
communication:

- **Lifecycle events**

  - `RUN_STARTED`, `RUN_FINISHED`, `RUN_ERROR`
  - `STEP_STARTED`, `STEP_FINISHED`

- **Text message events**

  - `TEXT_MESSAGE_START`, `TEXT_MESSAGE_CONTENT`, `TEXT_MESSAGE_END`

- **Tool call events**

  - `TOOL_CALL_START`, `TOOL_CALL_ARGS`, `TOOL_CALL_END`

- **State management events**

  - `STATE_SNAPSHOT`, `STATE_DELTA`, `MESSAGES_SNAPSHOT`

- **Special events**
  - `RAW`, `CUSTOM`

## Running Agents

To run an agent, you create a client instance and execute it:

```typescript
// Create an HTTP agent client
const agent = new HttpAgent({
  url: "https://your-agent-endpoint.com/agent",
  agentId: "unique-agent-id",
  threadId: "conversation-thread"
});

// Start the agent and handle events
agent.runAgent({
  tools: [...],
  context: [...]
}).subscribe({
  next: (event) => {
    // Handle different event types
    switch(event.type) {
      case EventType.TEXT_MESSAGE_CONTENT:
        // Update UI with new content
        break;
      // Handle other event types
    }
  },
  error: (error) => console.error("Agent error:", error),
  complete: () => console.log("Agent run complete")
});
```

## State Management

AG-UI provides efficient state management through specialized events:

- `STATE_SNAPSHOT`: Complete state representation at a point in time
- `STATE_DELTA`: Incremental state changes using JSON Patch format (RFC 6902)
- `MESSAGES_SNAPSHOT`: Complete conversation history

These events enable efficient client-side state management with minimal data
transfer.

## Tools and Handoff

AG-UI supports agent-to-agent handoff and tool usage through standardized
events:

- Tool definitions are passed in the `runAgent` parameters
- Tool calls are streamed as sequences of `TOOL_CALL_START` → `TOOL_CALL_ARGS` →
  `TOOL_CALL_END` events
- Agents can hand off to other agents, maintaining context continuity

## Events

All communication in AG-UI is based on typed events. Every event inherits from
`BaseEvent`:

```typescript
interface BaseEvent {
  type: EventType
  timestamp?: number
  rawEvent?: any
}
```

Events are strictly typed and validated, ensuring reliable communication between
components.



================================================
FILE: docs/concepts/events.mdx
================================================
---
title: "Events"
description: "Understanding events in the Agent User Interaction Protocol"
---

# Events

The Agent User Interaction Protocol uses a streaming event-based architecture.
Events are the fundamental units of communication between agents and frontends,
enabling real-time, structured interaction.

## Event Types Overview

Events in the protocol are categorized by their purpose:

| Category                | Description                             |
| ----------------------- | --------------------------------------- |
| Lifecycle Events        | Monitor the progression of agent runs   |
| Text Message Events     | Handle streaming textual content        |
| Tool Call Events        | Manage tool executions by agents        |
| State Management Events | Synchronize state between agents and UI |
| Special Events          | Support custom functionality            |
| Draft Events            | Proposed events under development       |

## Base Event Properties

All events share a common set of base properties:

| Property    | Description                                                      |
| ----------- | ---------------------------------------------------------------- |
| `type`      | The specific event type identifier                               |
| `timestamp` | Optional timestamp indicating when the event was created         |
| `rawEvent`  | Optional field containing the original event data if transformed |

## Lifecycle Events

These events represent the lifecycle of an agent run. A typical agent run
follows a predictable pattern: it begins with a `RunStarted` event, may contain
multiple optional `StepStarted`/`StepFinished` pairs, and concludes with either
a `RunFinished` event (success) or a `RunError` event (failure).

Lifecycle events provide crucial structure to agent runs, enabling frontends to
track progress, manage UI states appropriately, and handle errors gracefully.
They create a consistent framework for understanding when operations begin and
end, making it possible to implement features like loading indicators, progress
tracking, and error recovery mechanisms.

```mermaid
sequenceDiagram
    participant Agent
    participant Client

    Note over Agent,Client: Run begins
    Agent->>Client: RunStarted

    opt Sending steps is optional
        Note over Agent,Client: Step execution
        Agent->>Client: StepStarted
        Agent->>Client: StepFinished
    end

    Note over Agent,Client: Run completes
    alt
        Agent->>Client: RunFinished
    else
        Agent->>Client: RunError
    end
```

The `RunStarted` and either `RunFinished` or `RunError` events are mandatory,
forming the boundaries of an agent run. Step events are optional and may occur
multiple times within a run, allowing for structured, observable progress
tracking.

### RunStarted

Signals the start of an agent run.

The `RunStarted` event is the first event emitted when an agent begins
processing a request. It establishes a new execution context identified by a
unique `runId`. This event serves as a marker for frontends to initialize UI
elements such as progress indicators or loading states. It also provides crucial
identifiers that can be used to associate subsequent events with this specific
run.

| Property   | Description                   |
| ---------- | ----------------------------- |
| `threadId` | ID of the conversation thread |
| `runId`    | ID of the agent run           |

### RunFinished

Signals the successful completion of an agent run.

The `RunFinished` event indicates that an agent has successfully completed all
its work for the current run. Upon receiving this event, frontends should
finalize any UI states that were waiting on the agent's completion. This event
marks a clean termination point and indicates that no further processing will
occur in this run unless explicitly requested. The optional `result` field can
contain any output data produced by the agent run.

| Property   | Description                   |
| ---------- | ----------------------------- |
| `threadId` | ID of the conversation thread |
| `runId`    | ID of the agent run           |
| `result`   | Optional result data from run |

### RunError

Signals an error during an agent run.

The `RunError` event indicates that the agent encountered an error it could not
recover from, causing the run to terminate prematurely. This event provides
information about what went wrong, allowing frontends to display appropriate
error messages and potentially offer recovery options. After a `RunError` event,
no further processing will occur in this run.

| Property  | Description         |
| --------- | ------------------- |
| `message` | Error message       |
| `code`    | Optional error code |

### StepStarted

Signals the start of a step within an agent run.

The `StepStarted` event indicates that the agent is beginning a specific subtask
or phase of its processing. Steps provide granular visibility into the agent's
progress, enabling more precise tracking and feedback in the UI. Steps are
optional but highly recommended for complex operations that benefit from being
broken down into observable stages. The `stepName` could be the name of a node
or function that is currently executing.

| Property   | Description      |
| ---------- | ---------------- |
| `stepName` | Name of the step |

### StepFinished

Signals the completion of a step within an agent run.

The `StepFinished` event indicates that the agent has completed a specific
subtask or phase. When paired with a corresponding `StepStarted` event, it
creates a bounded context for a discrete unit of work. Frontends can use these
events to update progress indicators, show completion animations, or reveal
results specific to that step. The `stepName` must match the corresponding
`StepStarted` event to properly pair the beginning and end of the step.

| Property   | Description      |
| ---------- | ---------------- |
| `stepName` | Name of the step |

## Text Message Events

These events represent the lifecycle of text messages in a conversation. Text
message events follow a streaming pattern, where content is delivered
incrementally. A message begins with a `TextMessageStart` event, followed by one
or more `TextMessageContent` events that deliver chunks of text as they become
available, and concludes with a `TextMessageEnd` event.

This streaming approach enables real-time display of message content as it's
generated, creating a more responsive user experience compared to waiting for
the entire message to be complete before showing anything.

```mermaid
sequenceDiagram
    participant Agent
    participant Client

    Note over Agent,Client: Message begins
    Agent->>Client: TextMessageStart

    loop Content streaming
        Agent->>Client: TextMessageContent
    end

    Note over Agent,Client: Message completes
    Agent->>Client: TextMessageEnd
```

The `TextMessageContent` events each contain a `delta` field with a chunk of
text. Frontends should concatenate these deltas in the order received to
construct the complete message. The `messageId` property links all related
events, allowing the frontend to associate content chunks with the correct
message.

### TextMessageStart

Signals the start of a text message.

The `TextMessageStart` event initializes a new text message in the conversation.
It establishes a unique `messageId` that will be referenced by subsequent
content chunks and the end event. This event allows frontends to prepare the UI
for an incoming message, such as creating a new message bubble with a loading
indicator. The `role` property identifies whether the message is coming from the
assistant or potentially another participant in the conversation.

| Property    | Description                                                                       |
| ----------- | --------------------------------------------------------------------------------- |
| `messageId` | Unique identifier for the message                                                 |
| `role`      | Role of the message sender ("developer", "system", "assistant", "user", "tool") |

### TextMessageContent

Represents a chunk of content in a streaming text message.

The `TextMessageContent` event delivers incremental parts of the message text as
they become available. Each event contains a small chunk of text in the `delta`
property that should be appended to previously received chunks. The streaming
nature of these events enables real-time display of content, creating a more
responsive and engaging user experience. Implementations should handle these
events efficiently to ensure smooth text rendering without visible delays or
flickering.

| Property    | Description                            |
| ----------- | -------------------------------------- |
| `messageId` | Matches the ID from `TextMessageStart` |
| `delta`     | Text content chunk (non-empty)         |

### TextMessageEnd

Signals the end of a text message.

The `TextMessageEnd` event marks the completion of a streaming text message.
After receiving this event, the frontend knows that the message is complete and
no further content will be added. This allows the UI to finalize rendering,
remove any loading indicators, and potentially trigger actions that should occur
after message completion, such as enabling reply controls or performing
automatic scrolling to ensure the full message is visible.

| Property    | Description                            |
| ----------- | -------------------------------------- |
| `messageId` | Matches the ID from `TextMessageStart` |

### TextMessageChunk

A self-contained text message event that combines start, content, and end.

The `TextMessageChunk` event provides a convenient way to send complete text messages 
in a single event instead of the three-event sequence (start, content, end). This is 
particularly useful for simple messages or when the entire content is available at once. 
The event includes both the message metadata and content, making it more efficient for 
non-streaming scenarios.

| Property    | Description                                                                           |
| ----------- | ------------------------------------------------------------------------------------- |
| `messageId` | Optional unique identifier for the message                                            |
| `role`      | Optional role of the sender ("developer", "system", "assistant", "user", "tool")    |
| `delta`     | Optional text content of the message                                                  |

## Tool Call Events

These events represent the lifecycle of tool calls made by agents. Tool calls
follow a streaming pattern similar to text messages. When an agent needs to use
a tool, it emits a `ToolCallStart` event, followed by one or more `ToolCallArgs`
events that stream the arguments being passed to the tool, and concludes with a
`ToolCallEnd` event.

This streaming approach allows frontends to show tool executions in real-time,
making the agent's actions transparent and providing immediate feedback about
what tools are being invoked and with what parameters.

```mermaid
sequenceDiagram
    participant Agent
    participant Client

    Note over Agent,Client: Tool call begins
    Agent->>Client: ToolCallStart

    loop Arguments streaming
        Agent->>Client: ToolCallArgs
    end

    Note over Agent,Client: Tool call completes
    Agent->>Client: ToolCallEnd

    Note over Agent,Client: Tool execution result
    Agent->>Client: ToolCallResult
```

The `ToolCallArgs` events each contain a `delta` field with a chunk of the
arguments. Frontends should concatenate these deltas in the order received to
construct the complete arguments object. The `toolCallId` property links all
related events, allowing the frontend to associate argument chunks with the
correct tool call.

### ToolCallStart

Signals the start of a tool call.

The `ToolCallStart` event indicates that the agent is invoking a tool to perform
a specific function. This event provides the name of the tool being called and
establishes a unique `toolCallId` that will be referenced by subsequent events
in this tool call. Frontends can use this event to display tool usage to users,
such as showing a notification that a specific operation is in progress. The
optional `parentMessageId` allows linking the tool call to a specific message in
the conversation, providing context for why the tool is being used.

| Property          | Description                         |
| ----------------- | ----------------------------------- |
| `toolCallId`      | Unique identifier for the tool call |
| `toolCallName`    | Name of the tool being called       |
| `parentMessageId` | Optional ID of the parent message   |

### ToolCallArgs

Represents a chunk of argument data for a tool call.

The `ToolCallArgs` event delivers incremental parts of the tool's arguments as
they become available. Each event contains a segment of the argument data in the
`delta` property. These deltas are often JSON fragments that, when combined,
form the complete arguments object for the tool. Streaming the arguments is
particularly valuable for complex tool calls where constructing the full
arguments may take time. Frontends can progressively reveal these arguments to
users, providing insight into exactly what parameters are being passed to tools.

| Property     | Description                         |
| ------------ | ----------------------------------- |
| `toolCallId` | Matches the ID from `ToolCallStart` |
| `delta`      | Argument data chunk                 |

### ToolCallEnd

Signals the end of a tool call.

The `ToolCallEnd` event marks the completion of a tool call. After receiving
this event, the frontend knows that all arguments have been transmitted and the
tool execution is underway or completed. This allows the UI to finalize the tool
call display and prepare for potential results. In systems where tool execution
results are returned separately, this event indicates that the agent has
finished specifying the tool and its arguments, and is now waiting for or has
received the results.

| Property     | Description                         |
| ------------ | ----------------------------------- |
| `toolCallId` | Matches the ID from `ToolCallStart` |

### ToolCallResult

Provides the result of a tool call execution.

The `ToolCallResult` event delivers the output or result from a tool that was
previously invoked by the agent. This event is sent after the tool has been
executed by the system and contains the actual output generated by the tool.
Unlike the streaming pattern of tool call specification (start, args, end), the
result is delivered as a complete unit since tool execution typically produces a
complete output. Frontends can use this event to display tool results to users,
append them to the conversation history, or trigger follow-up actions based on
the tool's output.

| Property     | Description                                                 |
| ------------ | ----------------------------------------------------------- |
| `messageId`  | ID of the conversation message this result belongs to       |
| `toolCallId` | Matches the ID from the corresponding `ToolCallStart` event |
| `content`    | The actual result/output content from the tool execution    |
| `role`       | Optional role identifier, typically "tool" for tool results |

## State Management Events

These events are used to manage and synchronize the agent's state with the
frontend. State management in the protocol follows an efficient snapshot-delta
pattern where complete state snapshots are sent initially or infrequently, while
incremental updates (deltas) are used for ongoing changes.

This approach optimizes for both completeness and efficiency: snapshots ensure
the frontend has the full state context, while deltas minimize data transfer for
frequent updates. Together, they enable frontends to maintain an accurate
representation of agent state without unnecessary data transmission.

```mermaid
sequenceDiagram
    participant Agent
    participant Client

    Note over Agent,Client: Initial state transfer
    Agent->>Client: StateSnapshot

    Note over Agent,Client: Incremental updates
    loop State changes over time
        Agent->>Client: StateDelta
        Agent->>Client: StateDelta
    end

    Note over Agent,Client: Occasional full refresh
    Agent->>Client: StateSnapshot

    loop More incremental updates
        Agent->>Client: StateDelta
    end

    Note over Agent,Client: Message history update
    Agent->>Client: MessagesSnapshot
```

The combination of snapshots and deltas allows frontends to efficiently track
changes to agent state while ensuring consistency. Snapshots serve as
synchronization points that reset the state to a known baseline, while deltas
provide lightweight updates between snapshots.

### StateSnapshot

Provides a complete snapshot of an agent's state.

The `StateSnapshot` event delivers a comprehensive representation of the agent's
current state. This event is typically sent at the beginning of an interaction
or when synchronization is needed. It contains all state variables relevant to
the frontend, allowing it to completely rebuild its internal representation.
Frontends should replace their existing state model with the contents of this
snapshot rather than trying to merge it with previous state.

| Property   | Description             |
| ---------- | ----------------------- |
| `snapshot` | Complete state snapshot |

### StateDelta

Provides a partial update to an agent's state using JSON Patch.

The `StateDelta` event contains incremental updates to the agent's state in the
form of JSON Patch operations (as defined in RFC 6902). Each delta represents
specific changes to apply to the current state model. This approach is
bandwidth-efficient, sending only what has changed rather than the entire state.
Frontends should apply these patches in sequence to maintain an accurate state
representation. If a frontend detects inconsistencies after applying patches, it
may request a fresh `StateSnapshot`.

| Property | Description                               |
| -------- | ----------------------------------------- |
| `delta`  | Array of JSON Patch operations (RFC 6902) |

### MessagesSnapshot

Provides a snapshot of all messages in a conversation.

The `MessagesSnapshot` event delivers a complete history of messages in the
current conversation. Unlike the general state snapshot, this focuses
specifically on the conversation transcript. This event is useful for
initializing the chat history, synchronizing after connection interruptions, or
providing a comprehensive view when a user joins an ongoing conversation.
Frontends should use this to establish or refresh the conversational context
displayed to users.

| Property   | Description              |
| ---------- | ------------------------ |
| `messages` | Array of message objects |

## Special Events

Special events provide flexibility in the protocol by allowing for
system-specific functionality and integration with external systems. These
events don't follow the standard lifecycle or streaming patterns of other event
types but instead serve specialized purposes.

### Raw

Used to pass through events from external systems.

The `Raw` event acts as a container for events originating from external systems
or sources that don't natively follow the Agent UI Protocol. This event type
enables interoperability with other event-based systems by wrapping their events
in a standardized format. The enclosed event data is preserved in its original
form inside the `event` property, while the optional `source` property
identifies the system it came from. Frontends can use this information to handle
external events appropriately, either by processing them directly or by
delegating them to system-specific handlers.

| Property | Description                |
| -------- | -------------------------- |
| `event`  | Original event data        |
| `source` | Optional source identifier |

### Custom

Used for application-specific custom events.

The `Custom` event provides an extension mechanism for implementing features not
covered by the standard event types. Unlike `Raw` events which act as
passthrough containers, `Custom` events are explicitly part of the protocol but
with application-defined semantics. The `name` property identifies the specific
custom event type, while the `value` property contains the associated data. This
mechanism allows for protocol extensions without requiring formal specification
changes. Teams should document their custom events to ensure consistent
implementation across frontends and agents.

| Property | Description                     |
| -------- | ------------------------------- |
| `name`   | Name of the custom event        |
| `value`  | Value associated with the event |

## Draft Events

These events are currently in draft status and may change before finalization. They represent proposed extensions to the protocol that are under active development and discussion.

### Activity Events

<span style={{backgroundColor: '#3b82f6', color: 'white', padding: '2px 6px', borderRadius: '4px', fontSize: '0.75em', fontWeight: 'bold'}}>DRAFT</span> [View Proposal](/drafts/activity-events)

Activity events represent ongoing agent progress between chat messages, allowing frameworks to surface fine-grained activity updates chronologically.

#### ActivitySnapshotEvent

Provides the complete activity state at a point in time.

| Property       | Description                                          |
| -------------- | ---------------------------------------------------- |
| `messageId`    | Unique identifier for the ActivityMessage           |
| `activityType` | Activity type (e.g., "PLAN", "SEARCH", "SCRAPE")    |
| `content`      | Complete activity state at this point               |

#### ActivityDeltaEvent

Provides incremental updates to the activity state using JSON Patch operations.

| Property       | Description                                          |
| -------------- | ---------------------------------------------------- |
| `messageId`    | Unique identifier for the ActivityMessage           |
| `activityType` | Activity type (e.g., "PLAN", "SEARCH", "SCRAPE")    |
| `patch`        | JSON Patch operations (RFC 6902) to apply           |

### Reasoning Events

<span style={{backgroundColor: '#3b82f6', color: 'white', padding: '2px 6px', borderRadius: '4px', fontSize: '0.75em', fontWeight: 'bold'}}>DRAFT</span> [View Proposal](/drafts/reasoning)

Reasoning events support LLM reasoning visibility and continuity, enabling chain-of-thought reasoning while maintaining privacy.

#### ReasoningStart

Marks the start of reasoning.

| Property           | Description                             |
| ------------------ | --------------------------------------- |
| `messageId`        | Unique identifier of this reasoning     |
| `encryptedContent` | Optional encrypted content              |

#### ReasoningMessageStart

Signals the start of a reasoning message.

| Property    | Description                         |
| ----------- | ----------------------------------- |
| `messageId` | Unique identifier of the message    |
| `role`      | Role of the reasoning message       |

#### ReasoningMessageContent

Represents a chunk of content in a streaming reasoning message.

| Property    | Description                          |
| ----------- | ------------------------------------ |
| `messageId` | Matches ID from ReasoningMessageStart |
| `delta`     | Reasoning content chunk (non-empty)   |

#### ReasoningMessageEnd

Signals the end of a reasoning message.

| Property    | Description                          |
| ----------- | ------------------------------------ |
| `messageId` | Matches ID from ReasoningMessageStart |

#### ReasoningMessageChunk

A convenience event to auto start/close reasoning messages.

| Property    | Description                               |
| ----------- | ----------------------------------------- |
| `messageId` | Optional message ID                       |
| `delta`     | Optional reasoning content chunk          |

#### ReasoningEnd

Marks the end of reasoning.

| Property    | Description                         |
| ----------- | ----------------------------------- |
| `messageId` | Unique identifier of this reasoning |

### Meta Events

<span style={{backgroundColor: '#3b82f6', color: 'white', padding: '2px 6px', borderRadius: '4px', fontSize: '0.75em', fontWeight: 'bold'}}>DRAFT</span> [View Proposal](/drafts/meta-events)

Meta events provide annotations and signals independent of agent runs, such as user feedback or external system events.

#### MetaEvent

A side-band annotation event that can occur anywhere in the stream.

| Property   | Description                                               |
| ---------- | --------------------------------------------------------- |
| `metaType` | Application-defined type (e.g., "thumbs_up", "tag")      |
| `payload`  | Application-defined payload                               |

### Modified Lifecycle Events

<span style={{backgroundColor: '#3b82f6', color: 'white', padding: '2px 6px', borderRadius: '4px', fontSize: '0.75em', fontWeight: 'bold'}}>DRAFT</span> [View Proposal](/drafts/interrupts)

Extensions to existing lifecycle events to support interrupts and branching.

#### RunFinished (Extended)

The `RunFinished` event gains new fields to support interrupt-aware workflows.

| Property    | Description                                          |
| ----------- | ---------------------------------------------------- |
| `outcome`   | Optional: "success" or "interrupt"                  |
| `interrupt` | Optional: Contains interrupt details when paused    |

<span style={{backgroundColor: '#3b82f6', color: 'white', padding: '2px 6px', borderRadius: '4px', fontSize: '0.75em', fontWeight: 'bold'}}>DRAFT</span> [View Proposal](/drafts/serialization)

#### RunStarted (Extended)

The `RunStarted` event gains new fields to support branching and input tracking.

| Property     | Description                                        |
| ------------ | -------------------------------------------------- |
| `parentRunId`| Optional: Parent run ID for branching/time travel |
| `input`      | Optional: The exact agent input for this run      |

## Event Flow Patterns

Events in the protocol typically follow specific patterns:

1. **Start-Content-End Pattern**: Used for streaming content (text messages,
   tool calls)

   - `Start` event initiates the stream
   - `Content` events deliver data chunks
   - `End` event signals completion

2. **Snapshot-Delta Pattern**: Used for state synchronization

   - `Snapshot` provides complete state
   - `Delta` events provide incremental updates

3. **Lifecycle Pattern**: Used for monitoring agent runs
   - `Started` events signal beginnings
   - `Finished`/`Error` events signal endings

## Implementation Considerations

When implementing event handlers:

- Events should be processed in the order they are received
- Events with the same ID (e.g., `messageId`, `toolCallId`) belong to the same
  logical stream
- Implementations should be resilient to out-of-order delivery
- Custom events should follow the established patterns for consistency



================================================
FILE: docs/concepts/messages.mdx
================================================
---
title: "Messages"
description: "Understanding message structure and communication in AG-UI"
---

# Messages

Messages form the backbone of communication in the AG-UI protocol. They
represent the conversation history between users and AI agents, and provide a
standardized way to exchange information regardless of the underlying AI service
being used.

## Message Structure

AG-UI messages follow a vendor-neutral format, ensuring compatibility across
different AI providers while maintaining a consistent structure. This allows
applications to switch between AI services (like OpenAI, Anthropic, or custom
models) without changing the client-side implementation.

The basic message structure includes:

```typescript
interface BaseMessage {
  id: string // Unique identifier for the message
  role: string // The role of the sender (user, assistant, system, tool)
  content?: string // Optional text content of the message
  name?: string // Optional name of the sender
}
```

## Message Types

AG-UI supports several message types to accommodate different participants in a
conversation:

### User Messages

Messages from the end user to the agent:

```typescript
interface UserMessage {
  id: string
  role: "user"
  content: string // Text input from the user
  name?: string // Optional user identifier
}
```

### Assistant Messages

Messages from the AI assistant to the user:

```typescript
interface AssistantMessage {
  id: string
  role: "assistant"
  content?: string // Text response from the assistant (optional if using tool calls)
  name?: string // Optional assistant identifier
  toolCalls?: ToolCall[] // Optional tool calls made by the assistant
}
```

### System Messages

Instructions or context provided to the agent:

```typescript
interface SystemMessage {
  id: string
  role: "system"
  content: string // Instructions or context for the agent
  name?: string // Optional identifier
}
```

### Tool Messages

Results from tool executions:

```typescript
interface ToolMessage {
  id: string
  role: "tool"
  content: string // Result from the tool execution
  toolCallId: string // ID of the tool call this message responds to
}
```

### Developer Messages

Internal messages used for development or debugging:

```typescript
interface DeveloperMessage {
  id: string
  role: "developer"
  content: string
  name?: string
}
```

## Vendor Neutrality

AG-UI messages are designed to be vendor-neutral, meaning they can be easily
mapped to and from proprietary formats used by various AI providers:

```typescript
// Example: Converting AG-UI messages to OpenAI format
const openaiMessages = agUiMessages
  .filter((msg) => ["user", "system", "assistant"].includes(msg.role))
  .map((msg) => ({
    role: msg.role as "user" | "system" | "assistant",
    content: msg.content || "",
    // Map tool calls if present
    ...(msg.role === "assistant" && msg.toolCalls
      ? {
          tool_calls: msg.toolCalls.map((tc) => ({
            id: tc.id,
            type: tc.type,
            function: {
              name: tc.function.name,
              arguments: tc.function.arguments,
            },
          })),
        }
      : {}),
  }))
```

This abstraction allows AG-UI to serve as a common interface regardless of the
underlying AI service.

## Message Synchronization

Messages can be synchronized between client and server through two primary
mechanisms:

### Complete Snapshots

The `MESSAGES_SNAPSHOT` event provides a complete view of all messages in a
conversation:

```typescript
interface MessagesSnapshotEvent {
  type: EventType.MESSAGES_SNAPSHOT
  messages: Message[] // Complete array of all messages
}
```

This is typically used:

- When initializing a conversation
- After connection interruptions
- When major state changes occur
- To ensure client-server synchronization

### Streaming Messages

For real-time interactions, new messages can be streamed as they're generated:

1. **Start a message**: Indicate a new message is being created

   ```typescript
   interface TextMessageStartEvent {
     type: EventType.TEXT_MESSAGE_START
     messageId: string
     role: string
   }
   ```

2. **Stream content**: Send content chunks as they become available

   ```typescript
   interface TextMessageContentEvent {
     type: EventType.TEXT_MESSAGE_CONTENT
     messageId: string
     delta: string // Text chunk to append
   }
   ```

3. **End a message**: Signal the message is complete
   ```typescript
   interface TextMessageEndEvent {
     type: EventType.TEXT_MESSAGE_END
     messageId: string
   }
   ```

This streaming approach provides a responsive user experience with immediate
feedback.

## Tool Integration in Messages

AG-UI messages elegantly integrate tool usage, allowing agents to perform
actions and process their results:

### Tool Calls

Tool calls are embedded within assistant messages:

```typescript
interface ToolCall {
  id: string // Unique ID for this tool call
  type: "function" // Type of tool call
  function: {
    name: string // Name of the function to call
    arguments: string // JSON-encoded string of arguments
  }
}
```

Example assistant message with tool calls:

```typescript
{
  id: "msg_123",
  role: "assistant",
  content: "I'll help you with that calculation.",
  toolCalls: [
    {
      id: "call_456",
      type: "function",
      function: {
        name: "calculate",
        arguments: '{"expression": "24 * 7"}'
      }
    }
  ]
}
```

### Tool Results

Results from tool executions are represented as tool messages:

```typescript
{
  id: "result_789",
  role: "tool",
  content: "168",
  toolCallId: "call_456" // References the original tool call
}
```

This creates a clear chain of tool usage:

1. Assistant requests a tool call
2. Tool executes and returns a result
3. Assistant can reference and respond to the result

## Streaming Tool Calls

Similar to text messages, tool calls can be streamed to provide real-time
visibility into the agent's actions:

1. **Start a tool call**:

   ```typescript
   interface ToolCallStartEvent {
     type: EventType.TOOL_CALL_START
     toolCallId: string
     toolCallName: string
     parentMessageId?: string // Optional link to parent message
   }
   ```

2. **Stream arguments**:

   ```typescript
   interface ToolCallArgsEvent {
     type: EventType.TOOL_CALL_ARGS
     toolCallId: string
     delta: string // JSON fragment to append to arguments
   }
   ```

3. **End a tool call**:
   ```typescript
   interface ToolCallEndEvent {
     type: EventType.TOOL_CALL_END
     toolCallId: string
   }
   ```

This allows frontends to show tools being invoked progressively as the agent
constructs its reasoning.

## Practical Example

Here's a complete example of a conversation with tool usage:

```typescript
// Conversation history
;[
  // User query
  {
    id: "msg_1",
    role: "user",
    content: "What's the weather in New York?",
  },

  // Assistant response with tool call
  {
    id: "msg_2",
    role: "assistant",
    content: "Let me check the weather for you.",
    toolCalls: [
      {
        id: "call_1",
        type: "function",
        function: {
          name: "get_weather",
          arguments: '{"location": "New York", "unit": "celsius"}',
        },
      },
    ],
  },

  // Tool result
  {
    id: "result_1",
    role: "tool",
    content:
      '{"temperature": 22, "condition": "Partly Cloudy", "humidity": 65}',
    toolCallId: "call_1",
  },

  // Assistant's final response using tool results
  {
    id: "msg_3",
    role: "assistant",
    content:
      "The weather in New York is partly cloudy with a temperature of 22°C and 65% humidity.",
  },
]
```

## Conclusion

The message structure in AG-UI enables sophisticated conversational AI
experiences while maintaining vendor neutrality. By standardizing how messages
are represented, synchronized, and streamed, AG-UI provides a consistent way to
implement interactive human-agent communication regardless of the underlying AI
service.

This system supports everything from simple text exchanges to complex tool-based
workflows, all while optimizing for both real-time responsiveness and efficient
data transfer.



================================================
FILE: docs/concepts/state.mdx
================================================
---
title: "State Management"
description:
  "Understanding state synchronization between agents and frontends in AG-UI"
---

# State Management

State management is a core feature of the AG-UI protocol that enables real-time
synchronization between agents and frontend applications. By providing efficient
mechanisms for sharing and updating state, AG-UI creates a foundation for
collaborative experiences where both AI agents and human users can work together
seamlessly.

## Shared State Architecture

In AG-UI, state is a structured data object that:

1. Persists across interactions with an agent
2. Can be accessed by both the agent and the frontend
3. Updates in real-time as the interaction progresses
4. Provides context for decision-making on both sides

This shared state architecture creates a bidirectional communication channel
where:

- Agents can access the application's current state to make informed decisions
- Frontends can observe and react to changes in the agent's internal state
- Both sides can modify the state, creating a collaborative workflow

## State Synchronization Methods

AG-UI provides two complementary methods for state synchronization:

### State Snapshots

The `STATE_SNAPSHOT` event delivers a complete representation of an agent's
current state:

```typescript
interface StateSnapshotEvent {
  type: EventType.STATE_SNAPSHOT
  snapshot: any // Complete state object
}
```

Snapshots are typically used:

- At the beginning of an interaction to establish the initial state
- After connection interruptions to ensure synchronization
- When major state changes occur that require a complete refresh
- To establish a new baseline for future delta updates

When a frontend receives a `STATE_SNAPSHOT` event, it should replace its
existing state model entirely with the contents of the snapshot.

### State Deltas

The `STATE_DELTA` event delivers incremental updates to the state using JSON
Patch format (RFC 6902):

```typescript
interface StateDeltaEvent {
  type: EventType.STATE_DELTA
  delta: JsonPatchOperation[] // Array of JSON Patch operations
}
```

Deltas are bandwidth-efficient, sending only what has changed rather than the
entire state. This approach is particularly valuable for:

- Frequent small updates during streaming interactions
- Large state objects where most properties remain unchanged
- High-frequency updates that would be inefficient to send as full snapshots

## JSON Patch Format

AG-UI uses the JSON Patch format (RFC 6902) for state deltas, which defines a
standardized way to express changes to a JSON document:

```typescript
interface JsonPatchOperation {
  op: "add" | "remove" | "replace" | "move" | "copy" | "test"
  path: string // JSON Pointer (RFC 6901) to the target location
  value?: any // The value to apply (for add, replace)
  from?: string // Source path (for move, copy)
}
```

Common operations include:

1. **add**: Adds a value to an object or array

   ```json
   { "op": "add", "path": "/user/preferences", "value": { "theme": "dark" } }
   ```

2. **replace**: Replaces a value

   ```json
   { "op": "replace", "path": "/conversation_state", "value": "paused" }
   ```

3. **remove**: Removes a value

   ```json
   { "op": "remove", "path": "/temporary_data" }
   ```

4. **move**: Moves a value from one location to another
   ```json
   { "op": "move", "path": "/completed_items", "from": "/pending_items/0" }
   ```

Frontends should apply these patches in sequence to maintain an accurate state
representation. If inconsistencies are detected after applying patches, the
frontend can request a fresh `STATE_SNAPSHOT`.

## State Processing in AG-UI

In the AG-UI implementation, state deltas are applied using the
`fast-json-patch` library:

```typescript
case EventType.STATE_DELTA: {
  const { delta } = event as StateDeltaEvent;

  try {
    // Apply the JSON Patch operations to the current state without mutating the original
    const result = applyPatch(state, delta, true, false);
    state = result.newDocument;
    return emitUpdate({ state });
  } catch (error: unknown) {
    console.warn(
      `Failed to apply state patch:\n` +
      `Current state: ${JSON.stringify(state, null, 2)}\n` +
      `Patch operations: ${JSON.stringify(delta, null, 2)}\n` +
      `Error: ${errorMessage}`
    );
    return emitNoUpdate();
  }
}
```

This implementation ensures that:

- Patches are applied atomically (all or none)
- The original state is not mutated during the application process
- Errors are caught and handled gracefully

## Human-in-the-Loop Collaboration

The shared state system is fundamental to human-in-the-loop workflows in AG-UI.
It enables:

1. **Real-time visibility**: Users can observe the agent's thought process and
   current status
2. **Contextual awareness**: The agent can access user actions, preferences, and
   application state
3. **Collaborative decision-making**: Both human and AI can contribute to the
   evolving state
4. **Feedback loops**: Humans can correct or guide the agent by modifying state
   properties

For example, an agent might update its state with a proposed action:

```json
{
  "proposal": {
    "action": "send_email",
    "recipient": "client@example.com",
    "content": "Draft email content..."
  }
}
```

The frontend can display this proposal to the user, who can then approve,
reject, or modify it before execution.

## CopilotKit Implementation

[CopilotKit](https://docs.copilotkit.ai), a popular framework for building AI
assistants, leverages AG-UI's state management system through its "shared state"
feature. This implementation enables bidirectional state synchronization between
agents (particularly LangGraph agents) and frontend applications.

CopilotKit's shared state system is implemented through:

```jsx
// In the frontend React application
const { state: agentState, setState: setAgentState } = useCoAgent({
  name: "agent",
  initialState: { someProperty: "initialValue" },
})
```

This hook creates a real-time connection to the agent's state, allowing:

1. Reading the agent's current state in the frontend
2. Updating the agent's state from the frontend
3. Rendering UI components based on the agent's state

On the backend, LangGraph agents can emit state updates using:

```python
# In the LangGraph agent
async def tool_node(self, state: ResearchState, config: RunnableConfig):
    # Update state with new information
    tool_state = {
        "title": new_state.get("title", ""),
        "outline": new_state.get("outline", {}),
        "sections": new_state.get("sections", []),
        # Other state properties...
    }

    # Emit updated state to frontend
    await copilotkit_emit_state(config, tool_state)

    return tool_state
```

These state updates are transmitted using AG-UI's state snapshot and delta
mechanisms, creating a seamless shared context between agent and frontend.

## Best Practices

When implementing state management in AG-UI:

1. **Use snapshots judiciously**: Full snapshots should be sent only when
   necessary to establish a baseline.
2. **Prefer deltas for incremental changes**: Small state updates should use
   deltas to minimize data transfer.
3. **Structure state thoughtfully**: Design state objects to support partial
   updates and minimize patch complexity.
4. **Handle state conflicts**: Implement strategies for resolving conflicting
   updates from agent and frontend.
5. **Include error recovery**: Provide mechanisms to resynchronize state if
   inconsistencies are detected.
6. **Consider security implications**: Avoid storing sensitive information in
   shared state.

## Conclusion

AG-UI's state management system provides a powerful foundation for building
collaborative applications where humans and AI agents work together. By
efficiently synchronizing state between frontend and backend through snapshots
and JSON Patch deltas, AG-UI enables sophisticated human-in-the-loop workflows
that combine the strengths of both human intuition and AI capabilities.

The implementation in frameworks like CopilotKit demonstrates how this shared
state approach can create collaborative experiences that are more effective than
either fully autonomous systems or traditional user interfaces.



================================================
FILE: docs/concepts/tools.mdx
================================================
---
title: "Tools"
description:
  "Understanding tools and how they enable human-in-the-loop AI workflows"
---

# Tools

Tools are a fundamental concept in the AG-UI protocol that enable AI agents to
interact with external systems and incorporate human judgment into their
workflows. By defining tools in the frontend and passing them to agents,
developers can create sophisticated human-in-the-loop experiences that combine
AI capabilities with human expertise.

## What Are Tools?

In AG-UI, tools are functions that agents can call to:

1. Request specific information
2. Perform actions in external systems
3. Ask for human input or confirmation
4. Access specialized capabilities

Tools bridge the gap between AI reasoning and real-world actions, allowing
agents to accomplish tasks that would be impossible through conversation alone.

## Tool Structure

Tools follow a consistent structure that defines their name, purpose, and
expected parameters:

```typescript
interface Tool {
  name: string // Unique identifier for the tool
  description: string // Human-readable explanation of what the tool does
  parameters: {
    // JSON Schema defining the tool's parameters
    type: "object"
    properties: {
      // Tool-specific parameters
    }
    required: string[] // Array of required parameter names
  }
}
```

The `parameters` field uses [JSON Schema](https://json-schema.org/) to define
the structure of arguments that the tool accepts. This schema is used by both
the agent (to generate valid tool calls) and the frontend (to validate and parse
tool arguments).

## Frontend-Defined Tools

A key aspect of AG-UI's tool system is that tools are defined in the frontend
and passed to the agent during execution:

```typescript
// Define tools in the frontend
const userConfirmationTool = {
  name: "confirmAction",
  description: "Ask the user to confirm a specific action before proceeding",
  parameters: {
    type: "object",
    properties: {
      action: {
        type: "string",
        description: "The action that needs user confirmation",
      },
      importance: {
        type: "string",
        enum: ["low", "medium", "high", "critical"],
        description: "The importance level of the action",
      },
    },
    required: ["action"],
  },
}

// Pass tools to the agent during execution
agent.runAgent({
  tools: [userConfirmationTool],
  // Other parameters...
})
```

This approach has several advantages:

1. **Frontend control**: The frontend determines what capabilities are available
   to the agent
2. **Dynamic capabilities**: Tools can be added or removed based on user
   permissions, context, or application state
3. **Separation of concerns**: Agents focus on reasoning while frontends handle
   tool implementation
4. **Security**: Sensitive operations are controlled by the application, not the
   agent

## Tool Call Lifecycle

When an agent needs to use a tool, it follows a standardized sequence of events:

1. **ToolCallStart**: Indicates the beginning of a tool call with a unique ID
   and tool name

   ```typescript
   {
     type: EventType.TOOL_CALL_START,
     toolCallId: "tool-123",
     toolCallName: "confirmAction",
     parentMessageId: "msg-456" // Optional reference to a message
   }
   ```

2. **ToolCallArgs**: Streams the tool arguments as they're generated

   ```typescript
   {
     type: EventType.TOOL_CALL_ARGS,
     toolCallId: "tool-123",
     delta: '{"act' // Partial JSON being streamed
   }
   ```

   ```typescript
   {
     type: EventType.TOOL_CALL_ARGS,
     toolCallId: "tool-123",
     delta: 'ion":"Depl' // More JSON being streamed
   }
   ```

   ```typescript
   {
     type: EventType.TOOL_CALL_ARGS,
     toolCallId: "tool-123",
     delta: 'oy the application to production"}' // Final JSON fragment
   }
   ```

3. **ToolCallEnd**: Marks the completion of the tool call
   ```typescript
   {
     type: EventType.TOOL_CALL_END,
     toolCallId: "tool-123"
   }
   ```

The frontend accumulates these deltas to construct the complete tool call
arguments. Once the tool call is complete, the frontend can execute the tool and
provide results back to the agent.

## Tool Results

After a tool has been executed, the result is sent back to the agent as a "tool
message":

```typescript
{
  id: "result-789",
  role: "tool",
  content: "true", // Tool result as a string
  toolCallId: "tool-123" // References the original tool call
}
```

This message becomes part of the conversation history, allowing the agent to
reference and incorporate the tool's result in subsequent responses.

## Human-in-the-Loop Workflows

The AG-UI tool system is especially powerful for implementing human-in-the-loop
workflows. By defining tools that request human input or confirmation,
developers can create AI experiences that seamlessly blend autonomous operation
with human judgment.

For example:

1. Agent needs to make an important decision
2. Agent calls the `confirmAction` tool with details about the decision
3. Frontend displays a confirmation dialog to the user
4. User provides their input
5. Frontend sends the user's decision back to the agent
6. Agent continues processing with awareness of the user's choice

This pattern enables use cases like:

- **Approval workflows**: AI suggests actions that require human approval
- **Data verification**: Humans verify or correct AI-generated data
- **Collaborative decision-making**: AI and humans jointly solve complex
  problems
- **Supervised learning**: Human feedback improves future AI decisions

## CopilotKit Integration

[CopilotKit](https://docs.copilotkit.ai/) provides a simplified way to work with
AG-UI tools in React applications through its
[`useCopilotAction`](https://docs.copilotkit.ai/guides/frontend-actions) hook:

```tsx
import { useCopilotAction } from "@copilotkit/react-core"

// Define a tool for user confirmation
useCopilotAction({
  name: "confirmAction",
  description: "Ask the user to confirm an action",
  parameters: {
    type: "object",
    properties: {
      action: {
        type: "string",
        description: "The action to confirm",
      },
    },
    required: ["action"],
  },
  handler: async ({ action }) => {
    // Show a confirmation dialog
    const confirmed = await showConfirmDialog(action)
    return confirmed ? "approved" : "rejected"
  },
})
```

This approach makes it easy to define tools that integrate with your React
components and handle the tool execution logic in a clean, declarative way.

## Tool Examples

Here are some common types of tools used in AG-UI applications:

### User Confirmation

```typescript
{
  name: "confirmAction",
  description: "Ask the user to confirm an action",
  parameters: {
    type: "object",
    properties: {
      action: {
        type: "string",
        description: "The action to confirm"
      },
      importance: {
        type: "string",
        enum: ["low", "medium", "high", "critical"],
        description: "The importance level"
      }
    },
    required: ["action"]
  }
}
```

### Data Retrieval

```typescript
{
  name: "fetchUserData",
  description: "Retrieve data about a specific user",
  parameters: {
    type: "object",
    properties: {
      userId: {
        type: "string",
        description: "ID of the user"
      },
      fields: {
        type: "array",
        items: {
          type: "string"
        },
        description: "Fields to retrieve"
      }
    },
    required: ["userId"]
  }
}
```

### User Interface Control

```typescript
{
  name: "navigateTo",
  description: "Navigate to a different page or view",
  parameters: {
    type: "object",
    properties: {
      destination: {
        type: "string",
        description: "Destination page or view"
      },
      params: {
        type: "object",
        description: "Optional parameters for the navigation"
      }
    },
    required: ["destination"]
  }
}
```

### Content Generation

```typescript
{
  name: "generateImage",
  description: "Generate an image based on a description",
  parameters: {
    type: "object",
    properties: {
      prompt: {
        type: "string",
        description: "Description of the image to generate"
      },
      style: {
        type: "string",
        description: "Visual style for the image"
      },
      dimensions: {
        type: "object",
        properties: {
          width: { type: "number" },
          height: { type: "number" }
        },
        description: "Dimensions of the image"
      }
    },
    required: ["prompt"]
  }
}
```

## Best Practices

When designing tools for AG-UI:

1. **Clear naming**: Use descriptive, action-oriented names
2. **Detailed descriptions**: Include thorough descriptions to help the agent
   understand when and how to use the tool
3. **Structured parameters**: Define precise parameter schemas with descriptive
   field names and constraints
4. **Required fields**: Only mark parameters as required if they're truly
   necessary
5. **Error handling**: Implement robust error handling in tool execution code
6. **User experience**: Design tool UIs that provide appropriate context for
   human decision-making

## Conclusion

Tools in AG-UI bridge the gap between AI reasoning and real-world actions,
enabling sophisticated workflows that combine the strengths of AI and human
intelligence. By defining tools in the frontend and passing them to agents,
developers can create interactive experiences where AI and humans collaborate
efficiently.

The tool system is particularly powerful for implementing human-in-the-loop
workflows, where AI can suggest actions but defer critical decisions to humans.
This balances automation with human judgment, creating AI experiences that are
both powerful and trustworthy.



================================================
FILE: docs/development/contributing.mdx
================================================
---
title: Contributing
description: How to participate in Agent User Interaction Protocol development
---

# Naming conventions

Add your package under `typescript-sdk/integrations/` with docs and tests.

If your integration is work in progress, you can still add it to main branch.
You can prefix it with `wip-`, i.e.
(`typescript-sdk/integrations/wip-your-integration`) or if you're a third party
contributor use the `community` prefix, i.e.
(`typescript-sdk/integrations/community-your-integration`).

For questions and discussions, please use
[GitHub Discussions](https://github.com/orgs/ag-ui-protocol/discussions).



================================================
FILE: docs/development/roadmap.mdx
================================================
---
title: Roadmap
description: Our plans for evolving Agent User Interaction Protocol
---

You can follow the progress of the AG-UI Protocol on our
[public roadmap](https://github.com/orgs/ag-ui-protocol/projects/1).

## Get Involved

If you’d like to contribute ideas, feature requests, or bug reports to 
the roadmap, please see the [Contributing Guide](https://github.com/ag-ui-protocol/ag-ui/blob/main/CONTRIBUTING.md)
for details on how to get involved.



================================================
FILE: docs/development/updates.mdx
================================================
---
title: "What's New"
description: "The latest updates and improvements to AG-UI"
---

<Update label="2025-04-09" description="AG-UI repositories are now public">
  - Initial release of the Agent User Interaction Protocol
</Update>



================================================
FILE: docs/drafts/activity-events.mdx
================================================
---
title: Activity Events
description:
  Proposal for representing ongoing agent progress between chat messages
---

# Activity Events Proposal

## Summary

### Problem Statement

Users want to render "activity" updates inline with chat, not just at run start
or end. Currently, there's no standardized way to represent ongoing agent
progress between chat messages.

### Motivation

AG-UI is extended with **ActivityEvents** and **ActivityMessages** to represent
ongoing agent progress in between chat messages. This allows frameworks to
surface fine-grained activity updates chronologically, giving users immediate
visibility into what an agent is doing without waiting for the next message or
run boundary.

## Status

- **Status**: Draft
- **Author(s)**: Markus Ecker (mail@mme.xyz)

## Background

Users want real-time visibility into agent activities as they happen. Consider
this example UI:

```
+------------------------------------------------------------+
| I will search the internet for relevant information        | <- TextMessage
+------------------------------------------------------------+
+------------------------------------------------------------+
| ✓ checking reddit                                          | <- ActivityMessage
| searching X.com...                                         |
+------------------------------------------------------------+
```

### Use Cases

- **Workflows**: Step-by-step progress through workflow execution
- **Planning**: Intermediate planning or tool use visibility
- **Custom frameworks**: Signals representing ongoing work in any agent system

## Challenges

- **Flexibility**: Must handle arbitrary activity data from different frameworks
- **Serializability**: Events must be replayable and rehydrated for session
  recovery
- **Extensibility**: Developers should define custom renderers per activity
  type, with a generic fallback
- **Chronology**: Activities must interleave naturally with chat and run events

## Detailed Specification

### Overview

This proposal introduces new concepts to the AG-UI protocol:

1. **ActivitySnapshotEvent** and **ActivityDeltaEvent**: Two new event types following the established state management pattern
2. **ActivityMessage**: A new message type alongside TextMessage, ToolMessage,
   etc.

Frameworks may emit ActivityEvents, and frontends can render them inline with
chat.

### New Events: ActivitySnapshotEvent and ActivityDeltaEvent

Following the established pattern in AG-UI (similar to `StateSnapshotEvent` and `StateDeltaEvent`), activities are represented using two complementary events:

#### ActivitySnapshotEvent

Provides the complete activity state at a point in time.

```typescript
type ActivitySnapshotEvent = BaseEvent & {
  type: EventType.ACTIVITY_SNAPSHOT
  /**
   * Unique identifier for the ActivityMessage this event belongs to.
   */
  messageId: string
  /**
   * Activity type, e.g. "PLAN", "SEARCH", "SCRAPE"
   */
  activityType: string
  /**
   * Complete activity state at this point in time.
   */
  content: Record<string, any>
}
```

#### ActivityDeltaEvent

Provides incremental updates to the activity state.

```typescript
type ActivityDeltaEvent = BaseEvent & {
  type: EventType.ACTIVITY_DELTA
  /**
   * Unique identifier for the ActivityMessage this event belongs to.
   */
  messageId: string
  /**
   * Activity type, e.g. "PLAN", "SEARCH", "SCRAPE"
   */
  activityType: string
  /**
   * JSON Patch operations to apply to the current activity state.
   * Follows RFC 6902 semantics.
   */
  patch: JSONPatchOperation[]
}
```

#### Example Events

Initial activity snapshot:

```json
{
  "id": "evt_001",
  "ts": 1714064100000,
  "type": "ACTIVITY_SNAPSHOT",
  "messageId": "msg_789",
  "activityType": "PLAN",
  "content": {
    "tasks": ["check reddit", "search X.com"]
  }
}
```

Incremental update via patch:

```json
{
  "id": "evt_002",
  "ts": 1714064120000,
  "type": "ACTIVITY_DELTA",
  "messageId": "msg_789",
  "activityType": "PLAN",
  "patch": [
    {
      "op": "replace",
      "path": "/tasks/0",
      "value": "✓ check reddit"
    }
  ]
}
```

### New Message: ActivityMessage

```typescript
type ActivityMessage = {
  id: string
  role: "activity"
  activityType: string
  /**
   * Finalized activity content as of compaction.
   */
  content: Record<string, any>
}
```

### Rendering Strategy

- **Generic renderer**: Displays raw snapshot/patch as JSON or formatted text
- **Custom renderer**: Developers can register a renderer per `activityType`:
  - `"PLAN"` → Interactive checklist component
  - `"SEARCH"` → Live status with progress indicators
  - `"WORKFLOW"` → Step-by-step workflow visualization

## Implementation Considerations

### Client SDK Changes

TypeScript SDK additions:

- New `ActivitySnapshotEvent` and `ActivityDeltaEvent` types in `@ag-ui/core`
- New `ActivityMessage` type in message unions
- Activity renderer registry in `@ag-ui/client`
- JSON Patch utilities for activity updates

Python SDK additions:

- New `ActivitySnapshotEvent` and `ActivityDeltaEvent` classes in `ag_ui.core.events`
- New `ActivityMessage` class in message types
- Activity serialization/deserialization support
- JSON Patch utilities for activity updates

### Integration Impact

- **Planning Frameworks**: Can emit ActivitySnapshotEvent/ActivityDeltaEvent during planning or tool
  execution phases
- **Workflow Systems**: Can surface step-by-step workflow progress as
  ActivitySnapshotEvent/ActivityDeltaEvent
- **Other frameworks**: May emit ActivitySnapshotEvent/ActivityDeltaEvent freely; AG-UI will serialize
  them like other events

## Examples and Use Cases

### Example 1: Web Search Activity

```typescript
// Agent emits initial search activity snapshot
agent.emitActivitySnapshot({
  messageId: "msg_123",
  activityType: "SEARCH",
  content: {
    sources: [
      { name: "Reddit", status: "pending" },
      { name: "X.com", status: "pending" },
      { name: "Google", status: "pending" },
    ],
  },
})

// Update as search progresses
agent.emitActivityDelta({
  messageId: "msg_123",
  activityType: "SEARCH",
  patch: [
    {
      op: "replace",
      path: "/sources/0/status",
      value: "complete",
    }
  ],
})
```

### Use Case: Multi-Step Workflow Visibility

A data analysis agent performing multiple steps:

1. Loading dataset → ActivitySnapshotEvent/ActivityDeltaEvent shows progress bar
2. Cleaning data → ActivitySnapshotEvent/ActivityDeltaEvent shows rows processed
3. Running analysis → ActivitySnapshotEvent/ActivityDeltaEvent shows current computation
4. Generating report → ActivitySnapshotEvent/ActivityDeltaEvent shows sections completed

Each step appears inline with chat, giving users real-time feedback.

## Testing Strategy

- Unit tests for ActivitySnapshotEvent/ActivityDeltaEvent serialization/deserialization
- Integration tests with mock frameworks emitting ActivitySnapshotEvent/ActivityDeltaEvent
- E2E tests in AG-UI Dojo demonstrating activity rendering
- Performance benchmarks for high-frequency activity updates
- JSON Patch application correctness tests

## References

- [JSON Patch RFC 6902](https://tools.ietf.org/html/rfc6902)
- [AG-UI Events Documentation](/concepts/events)
- [AG-UI Messages Documentation](/concepts/messages)



================================================
FILE: docs/drafts/generative-ui.mdx
================================================
---
title: Generative User Interfaces
description: AI-generated interfaces without custom tool renderers
---

# Generative User Interfaces

## Summary

### Problem Statement

Currently, creating custom user interfaces for agent interactions requires
programmers to define specific tool renderers. This limits the flexibility and
adaptability of agent-driven applications.

### Motivation

This draft describes an AG-UI extension that addresses **generative user
interfaces**—interfaces produced directly by artificial intelligence without
requiring a programmer to define custom tool renderers. The key idea is to
leverage our ability to send client-side tools to the agent, thereby enabling
this capability across all agent frameworks supported by AG-UI.

## Status

- **Status**: Draft
- **Author(s)**: Markus Ecker (mail@mme.xyz)

## Challenges and Limitations

### Tool Description Length

OpenAI enforces a limit of 1024 characters for tool descriptions. Gemini and
Anthropic impose no such limit.

### Arguments JSON Schema Constraints

Classes, nesting, `$ref`, and `oneOf` are not reliably supported across LLM
providers.

### Context Window Considerations

Injecting a large UI description language into an agent may reduce its
performance. Agents dedicated solely to UI generation perform better than agents
combining UI generation with other tasks.

## Detailed Specification

### Two-Step Generation Process

```mermaid
flowchart TD
    A[Agent needs UI] --> B["Step 1: <b>What?</b> <br/> Agent calls generateUserInterface <br/>(description, data, output)"]
    B --> C["Step 2: <b>How?</b> <br/> Secondary generator builds actual UI <br/>(JSON Schema, React, etc.)"]
    C --> D[Rendered UI shown to user]
    D --> E[Validated user input returned to Agent]
```

### Step 1: What to Generate?

Inject a lightweight tool into the agent:

**Tool Definition:**

- **Name:** `generateUserInterface`
- **Arguments:**
  - **description**: A high-level description of the UI (e.g., _"A form for
    entering the user's address"_)
  - **data**: Arbitrary pre-populated data for the generated UI
  - **output**: A description or schema of the data the agent expects the user
    to submit back (fields, required/optional, types, constraints)

**Example Tool Call:**

```json
{
  "tool": "generateUserInterface",
  "arguments": {
    "description": "A form that collects a user's shipping address.",
    "data": {
      "firstName": "Ada",
      "lastName": "Lovelace",
      "city": "London"
    },
    "output": {
      "type": "object",
      "required": [
        "firstName",
        "lastName",
        "street",
        "city",
        "postalCode",
        "country"
      ],
      "properties": {
        "firstName": { "type": "string", "title": "First Name" },
        "lastName": { "type": "string", "title": "Last Name" },
        "street": { "type": "string", "title": "Street Address" },
        "city": { "type": "string", "title": "City" },
        "postalCode": { "type": "string", "title": "Postal Code" },
        "country": {
          "type": "string",
          "title": "Country",
          "enum": ["GB", "US", "DE", "AT"]
        }
      }
    }
  }
}
```

### Step 2: How to Generate?

Delegate UI generation to a secondary LLM or agent:

- The CopilotKit user stays in control: Can make their own generators, add
  custom libraries, include additional prompts etc.
- On tool invocation, the secondary model consumes `description`, `data`, and
  `output` to generate the user interface
- This model is focused solely on UI generation, ensuring maximum fidelity and
  consistency
- The generation method can be swapped as needed (e.g., JSON, HTML, or other
  renderable formats)
- The UI format description is not subject to structural or length constraints,
  allowing arbitrarily complex specifications

## Implementation Examples

### Example Output: UISchemaGenerator

```json
{
  "jsonSchema": {
    "title": "Shipping Address",
    "type": "object",
    "required": [
      "firstName",
      "lastName",
      "street",
      "city",
      "postalCode",
      "country"
    ],
    "properties": {
      "firstName": { "type": "string", "title": "First name" },
      "lastName": { "type": "string", "title": "Last name" },
      "street": { "type": "string", "title": "Street address" },
      "city": { "type": "string", "title": "City" },
      "postalCode": { "type": "string", "title": "Postal code" },
      "country": {
        "type": "string",
        "title": "Country",
        "enum": ["GB", "US", "DE", "AT"]
      }
    }
  },
  "uiSchema": {
    "type": "VerticalLayout",
    "elements": [
      {
        "type": "Group",
        "label": "Personal Information",
        "elements": [
          { "type": "Control", "scope": "#/properties/firstName" },
          { "type": "Control", "scope": "#/properties/lastName" }
        ]
      },
      {
        "type": "Group",
        "label": "Address",
        "elements": [
          { "type": "Control", "scope": "#/properties/street" },
          { "type": "Control", "scope": "#/properties/city" },
          { "type": "Control", "scope": "#/properties/postalCode" },
          { "type": "Control", "scope": "#/properties/country" }
        ]
      }
    ]
  },
  "initialData": {
    "firstName": "Ada",
    "lastName": "Lovelace",
    "city": "London",
    "country": "GB"
  }
}
```

### Example Output: ReactFormHookGenerator

```tsx
import React from "react"
import { useForm } from "react-hook-form"
import { z } from "zod"
import { zodResolver } from "@hookform/resolvers/zod"

// ----- Schema (contract) -----
const AddressSchema = z.object({
  firstName: z.string().min(1, "Required"),
  lastName: z.string().min(1, "Required"),
  street: z.string().min(1, "Required"),
  city: z.string().min(1, "Required"),
  postalCode: z.string().regex(/^[A-Za-z0-9\\-\\s]{3,10}$/, "3–10 chars"),
  country: z.enum(["GB", "US", "DE", "AT", "FR", "IT", "ES"]),
})
export type Address = z.infer<typeof AddressSchema>

type Props = {
  initialData?: Partial<Address>
  meta?: { title?: string; submitLabel?: string }
  respond: (data: Address) => void // <-- called on successful submit
}

const COUNTRIES: Address["country"][] = [
  "GB",
  "US",
  "DE",
  "AT",
  "FR",
  "IT",
  "ES",
]

export default function AddressForm({ initialData, meta, respond }: Props) {
  const {
    register,
    handleSubmit,
    formState: { errors },
  } = useForm<Address>({
    resolver: zodResolver(AddressSchema),
    defaultValues: {
      firstName: "",
      lastName: "",
      street: "",
      city: "",
      postalCode: "",
      country: "GB",
      ...initialData,
    },
  })

  const onSubmit = (data: Address) => {
    // Guaranteed to match AddressSchema
    respond(data)
  }

  return (
    <form onSubmit={handleSubmit(onSubmit)}>
      {meta?.title && <h2>{meta.title}</h2>}

      {/* Section: Personal Information */}
      <fieldset>
        <legend>Personal Information</legend>

        <div>
          <label>First name</label>
          <input {...register("firstName")} placeholder="Ada" autoFocus />
          {errors.firstName && <small>{errors.firstName.message}</small>}
        </div>

        <div>
          <label>Last name</label>
          <input {...register("lastName")} placeholder="Lovelace" />
          {errors.lastName && <small>{errors.lastName.message}</small>}
        </div>
      </fieldset>

      {/* Section: Address */}
      <fieldset>
        <legend>Address</legend>

        <div>
          <label>Street address</label>
          <input {...register("street")} />
          {errors.street && <small>{errors.street.message}</small>}
        </div>

        <div>
          <label>City</label>
          <input {...register("city")} />
          {errors.city && <small>{errors.city.message}</small>}
        </div>

        <div>
          <label>Postal code</label>
          <input {...register("postalCode")} />
          {errors.postalCode && <small>{errors.postalCode.message}</small>}
        </div>

        <div>
          <label>Country</label>
          <select {...register("country")}>
            {COUNTRIES.map((c) => (
              <option key={c} value={c}>
                {c}
              </option>
            ))}
          </select>
          {errors.country && <small>{errors.country.message}</small>}
        </div>
      </fieldset>

      <div>
        <button type="submit">{meta?.submitLabel ?? "Submit"}</button>
      </div>
    </form>
  )
}
```

## Implementation Considerations

### Client SDK Changes

TypeScript SDK additions:

- New `generateUserInterface` tool type
- UI generator registry for pluggable generators
- Validation layer for generated UI schemas
- Response handler for user-submitted data

Python SDK additions:

- Support for UI generation tool invocation
- Schema validation utilities
- Serialization for UI definitions

### Integration Impact

- All AG-UI integrations can leverage this capability without modification
- Frameworks emit standard tool calls; client handles UI generation
- Backward compatible with existing tool-based UI approaches

## Use Cases

### Dynamic Forms

Agents can generate forms on-the-fly based on conversation context without
pre-defined schemas.

### Data Visualization

Generate charts, graphs, or tables appropriate to the data being discussed.

### Interactive Workflows

Create multi-step wizards or guided processes tailored to user needs.

### Adaptive Interfaces

Generate different UI layouts based on user preferences or device capabilities.

## Testing Strategy

- Unit tests for tool injection and invocation
- Integration tests with multiple UI generators
- E2E tests demonstrating various UI types
- Performance benchmarks comparing single vs. two-step generation
- Cross-provider compatibility testing

## References

- [AG-UI Tools Documentation](/concepts/tools)
- [JSON Schema](https://json-schema.org/)
- [React Hook Form](https://react-hook-form.com/)
- [JSON Forms](https://jsonforms.io/)



================================================
FILE: docs/drafts/interrupts.mdx
================================================
---
title: Interrupt-Aware Run Lifecycle
description: Native support for human-in-the-loop pauses and interrupts
---

# Interrupt-Aware Run Lifecycle Proposal

## Summary

### Problem Statement

Agents often need to pause execution to request human approval, gather
additional input, or confirm potentially risky actions. Currently, there's no
standardized way to handle these interruptions across different agent
frameworks.

### Motivation

Support **human-in-the-loop pauses** (and related mechanisms) natively in AG-UI
and CopilotKit. This enables compatibility with various framework interrupts,
workflow suspend/resume, and other framework-specific pause mechanisms.

## Status

- **Status**: Draft
- **Author(s)**: Markus Ecker (mail@mme.xyz)

## Overview

This proposal introduces a standardized interrupt/resume pattern:

```mermaid
sequenceDiagram
  participant Agent
  participant Client as Client App

  Agent-->>Client: RUN_FINISHED { outcome: "interrupt", interrupt:{ id, reason, payload }}
  Client-->>Agent: RunAgentInput.resume { threadId, interruptId, payload }
  Agent-->>Client: RUN_FINISHED { outcome: "success", result }
```

## Detailed Specification

### Updates to RUN_FINISHED Event

```typescript
type RunFinishedOutcome = "success" | "interrupt"

type RunFinished = {
  type: "RUN_FINISHED"

  // ... existing fields

  outcome?: RunFinishedOutcome // optional for back-compat (see rules below)

  // Present when outcome === "success" (or when outcome omitted and interrupt is absent)
  result?: any

  // Present when outcome === "interrupt" (or when outcome omitted and interrupt is present)
  interrupt?: {
    id?: string // id can be set when needed
    reason?: string // e.g. "human_approval" | "upload_required" | "policy_hold"
    payload?: any // arbitrary JSON for UI (forms, proposals, diffs, etc.)
  }
}
```

When a run finishes with `outcome == "interrupt"`, the agent indicates that on
the next run, a value needs to be provided to continue.

### Updates to RunAgentInput

```typescript
type RunAgentInput = {
  // ... existing fields

  // NEW: resume channel for continuing a suspension
  resume?: {
    interruptId?: string // echo back if one was provided
    payload?: any // arbitrary JSON: approvals, edits, files-as-refs, etc.
  }
}
```

### Contract Rules

- Resume requests **must** use the same `threadId`
- When given in the `interrupt`, the `interruptId` must be provided via
  `RunAgentInput`
- Agents should handle missing or invalid resume payloads gracefully

## Implementation Examples

### Minimal Interrupt/Resume

**Agent sends interrupt:**

```json
{
  "type": "RUN_FINISHED",
  "threadId": "t1",
  "runId": "r1",
  "outcome": "interrupt",
  "interrupt": {
    "id": "int-abc123",
    "reason": "human_approval",
    "payload": {
      "proposal": {
        "tool": "sendEmail",
        "args": { "to": "a@b.com", "subject": "Hi", "body": "…" }
      }
    }
  }
}
```

**User responds:**

```json
{
  "threadId": "t1",
  "runId": "r2",
  "resume": {
    "interruptId": "int-abc123",
    "payload": { "approved": true }
  }
}
```

### Complex Approval Flow

**Agent requests approval with context:**

```json
{
  "type": "RUN_FINISHED",
  "threadId": "thread-456",
  "runId": "run-789",
  "outcome": "interrupt",
  "interrupt": {
    "id": "approval-001",
    "reason": "database_modification",
    "payload": {
      "action": "DELETE",
      "table": "users",
      "affectedRows": 42,
      "query": "DELETE FROM users WHERE last_login < '2023-01-01'",
      "rollbackPlan": "Restore from backup snapshot-2025-01-23",
      "riskLevel": "high"
    }
  }
}
```

**User approves with modifications:**

```json
{
  "threadId": "thread-456",
  "runId": "run-790",
  "resume": {
    "interruptId": "approval-001",
    "payload": {
      "approved": true,
      "modifications": {
        "batchSize": 10,
        "dryRun": true
      }
    }
  }
}
```

## Use Cases

### Human Approval

Agents pause before executing sensitive operations (sending emails, making
purchases, deleting data).

### Information Gathering

Agent requests additional context or files from the user mid-execution.

### Policy Enforcement

Automatic pauses triggered by organizational policies or compliance
requirements.

### Multi-Step Wizards

Complex workflows where each step requires user confirmation or input.

### Error Recovery

Agent pauses when encountering an error, allowing user to provide guidance.

## Implementation Considerations

### Client SDK Changes

TypeScript SDK:

- Extended `RunFinishedEvent` type with outcome and interrupt fields
- Updated `RunAgentInput` with resume field
- Helper methods for interrupt handling

Python SDK:

- Extended `RunFinishedEvent` class
- Updated `RunAgentInput` with resume support
- Interrupt state management utilities

### Framework Integration

**Planning Frameworks:**

- Map framework interrupts to AG-UI interrupt events
- Handle resume payloads in execution continuation

**Workflow Systems:**

- Convert workflow suspensions to AG-UI interrupts
- Resume workflow execution with provided payload

**Custom Frameworks:**

- Provide interrupt/resume adapter interface
- Documentation for integration patterns

### UI Considerations

- Standard components for common interrupt reasons
- Customizable interrupt UI based on payload
- Clear indication of pending interrupts
- History of interrupt/resume actions

## Testing Strategy

- Unit tests for interrupt/resume serialization
- Integration tests with multiple frameworks
- E2E tests demonstrating various interrupt scenarios
- State consistency tests across interrupt boundaries
- Performance tests for rapid interrupt/resume cycles

## References

- [AG-UI Events Documentation](/concepts/events)
- [AG-UI State Management](/concepts/state)



================================================
FILE: docs/drafts/meta-events.mdx
================================================
---
title: Meta Events
description: Annotations and signals independent of agent runs
---

# Meta Events Proposal

## Summary

### Problem Statement

Currently, AG-UI events are tightly coupled to agent runs. There's no
standardized way to attach user feedback, annotations, or external signals to
the event stream that are independent of the agent's execution lifecycle.

### Motivation

AG-UI is extended with **MetaEvents**, a new class of events that can occur at
any point in the event stream, independent of agent runs. MetaEvents provide a
way to attach annotations, signals, or feedback to a serialized stream. They may
originate from users, clients, or external systems rather than from agents.
Examples include reactions such as thumbs up/down on a message.

## Status

- **Status**: Draft
- **Author(s)**: Markus Ecker (mail@mme.xyz)

## Detailed Specification

### Overview

This proposal introduces:

- A new **MetaEvent** type for side-band annotations
- Events that can appear anywhere in the stream
- Support for user feedback, tags, and external annotations
- Extensible payload structure for application-specific data

## New Type: MetaEvent

```typescript
type MetaEvent = BaseEvent & {
  type: EventType.META
  /**
   * Application-defined type of the meta event.
   * Examples: "thumbs_up", "thumbs_down", "tag", "note"
   */
  metaType: string

  /**
   * Application-defined payload.
   * May reference other entities (e.g., messageId) or contain freeform data.
   */
  payload: Record<string, unknown>
}
```

### Key Characteristics

- **Run-independent**: MetaEvents are not tied to any specific run lifecycle
- **Position-flexible**: Can appear before, between, or after runs
- **Origin-diverse**: May come from users, clients, or external systems
- **Extensible**: Applications define their own metaType values and payload
  schemas

## Implementation Examples

### User Feedback

**Thumbs Up:**

```json
{
  "id": "evt_123",
  "ts": 1714063982000,
  "type": "META",
  "metaType": "thumbs_up",
  "payload": {
    "messageId": "msg_456",
    "userId": "user_789"
  }
}
```

**Thumbs Down with Reason:**

```json
{
  "id": "evt_124",
  "ts": 1714063985000,
  "type": "META",
  "metaType": "thumbs_down",
  "payload": {
    "messageId": "msg_456",
    "userId": "user_789",
    "reason": "inaccurate",
    "comment": "The calculation seems incorrect"
  }
}
```

### Annotations

**User Note:**

```json
{
  "id": "evt_789",
  "ts": 1714064001000,
  "type": "META",
  "metaType": "note",
  "payload": {
    "text": "Important question to revisit",
    "relatedRunId": "run_001",
    "author": "user_123"
  }
}
```

**Tag Assignment:**

```json
{
  "id": "evt_890",
  "ts": 1714064100000,
  "type": "META",
  "metaType": "tag",
  "payload": {
    "tags": ["important", "follow-up"],
    "threadId": "thread_001"
  }
}
```

### External System Events

**Analytics Event:**

```json
{
  "id": "evt_901",
  "ts": 1714064200000,
  "type": "META",
  "metaType": "analytics",
  "payload": {
    "event": "conversation_shared",
    "properties": {
      "shareMethod": "link",
      "recipientCount": 3
    }
  }
}
```

**Moderation Flag:**

```json
{
  "id": "evt_902",
  "ts": 1714064300000,
  "type": "META",
  "metaType": "moderation",
  "payload": {
    "action": "flag",
    "messageId": "msg_999",
    "category": "inappropriate_content",
    "confidence": 0.95
  }
}
```

## Common Meta Event Types

While applications can define their own types, these are commonly used:

| MetaType      | Description       | Typical Payload                    |
| ------------- | ----------------- | ---------------------------------- |
| `thumbs_up`   | Positive feedback | `{ messageId, userId }`            |
| `thumbs_down` | Negative feedback | `{ messageId, userId, reason? }`   |
| `note`        | User annotation   | `{ text, relatedId?, author }`     |
| `tag`         | Categorization    | `{ tags[], targetId }`             |
| `bookmark`    | Save for later    | `{ messageId, userId }`            |
| `copy`        | Content copied    | `{ messageId, content }`           |
| `share`       | Content shared    | `{ messageId, method }`            |
| `rating`      | Numeric rating    | `{ messageId, rating, maxRating }` |

## Use Cases

### User Feedback Collection

Capture user reactions to agent responses for quality improvement.

### Conversation Annotation

Allow users to add notes, tags, or bookmarks to important parts of
conversations.

### Analytics and Tracking

Record user interactions and behaviors without affecting agent execution.

### Content Moderation

Flag or mark content for review by external moderation systems.

### Collaborative Features

Enable multiple users to annotate or comment on shared conversations.

### Audit Trail

Create a complete record of all interactions, not just agent responses.

## Implementation Considerations

### Client SDK Changes

TypeScript SDK:

- New `MetaEvent` type in `@ag-ui/core`
- Helper functions for common meta event types
- MetaEvent filtering and querying utilities

Python SDK:

- `MetaEvent` class implementation
- Meta event builders for common types
- Event stream filtering capabilities

## Testing Strategy

- Unit tests for MetaEvent creation and validation
- Integration tests with mixed event streams
- Performance tests with high-volume meta events
- Security tests for payload validation

## References

- [AG-UI Events Documentation](/concepts/events)
- [Event Sourcing](https://martinfowler.com/eaaDev/EventSourcing.html)
- [CQRS Pattern](https://martinfowler.com/bliki/CQRS.html)



================================================
FILE: docs/drafts/multimodal-messages.mdx
================================================
---
title: Multi-modal Messages
description:
  Support for multimodal input messages including text, images, audio, and files
---

# Multi-modal Messages Proposal

## Summary

### Problem Statement

Current AG-UI protocol only supports text-based user messages. As LLMs
increasingly support multimodal inputs (images, audio, files), the protocol
needs to evolve to handle these richer input types.

### Motivation

Evolve AG-UI to support **multimodal input messages** without breaking existing
apps. Inputs may include text, images, audio, and files.

## Status

- **Status**: Draft
- **Author(s)**: Markus Ecker (mail@mme.xyz)

## Detailed Specification

### Overview

Extend the `UserMessage` `content` property to be either a string or an array of
`InputContent`:

```typescript
interface TextInputContent {
  type: "text"
  text: string
}

interface BinaryInputContent {
  type: "binary"
  mimeType: string
  id?: string
  url?: string
  data?: string
  filename?: string
}

type InputContent = TextInputContent | BinaryInputContent

type UserMessage = {
  id: string
  role: "user"
  content: string | InputContent[]
  name?: string
}
```

### InputContent Types

#### TextInputContent

Represents text content within a multimodal message.

```typescript
interface TextInputContent {
  type: "text"
  text: string
}
```

| Property | Type     | Description                     |
| -------- | -------- | ------------------------------- |
| `type`   | `"text"` | Identifies this as text content |
| `text`   | `string` | The text content                |

#### BinaryInputContent

Represents binary content such as images, audio, or files.

```typescript
interface BinaryInputContent {
  type: "binary"
  mimeType: string
  id?: string
  url?: string
  data?: string
  filename?: string
}
```

| Property   | Type       | Description                                                |
| ---------- | ---------- | ---------------------------------------------------------- |
| `type`     | `"binary"` | Identifies this as binary content                          |
| `mimeType` | `string`   | MIME type of the content (e.g., "image/jpeg", "audio/wav") |
| `id`       | `string?`  | Optional identifier for content reference                  |
| `url`      | `string?`  | Optional URL to fetch the content                          |
| `data`     | `string?`  | Optional base64-encoded content                            |
| `filename` | `string?`  | Optional filename for the content                          |

### Content Delivery Methods

Binary content can be provided through multiple methods:

1. **Inline Data**: Base64-encoded in the `data` field
2. **URL Reference**: External URL in the `url` field
3. **ID Reference**: Reference to pre-uploaded content via `id` field

At least one of `data`, `url`, or `id` must be provided for binary content.

## Implementation Examples

### Simple Text Message (Backward Compatible)

```json
{
  "id": "msg-001",
  "role": "user",
  "content": "What's in this image?"
}
```

### Image with Text

```json
{
  "id": "msg-002",
  "role": "user",
  "content": [
    {
      "type": "text",
      "text": "What's in this image?"
    },
    {
      "type": "binary",
      "mimeType": "image/jpeg",
      "data": "base64-encoded-image-data..."
    }
  ]
}
```

### Multiple Images with Question

```json
{
  "id": "msg-003",
  "role": "user",
  "content": [
    {
      "type": "text",
      "text": "What are the differences between these images?"
    },
    {
      "type": "binary",
      "mimeType": "image/png",
      "url": "https://example.com/image1.png"
    },
    {
      "type": "binary",
      "mimeType": "image/png",
      "url": "https://example.com/image2.png"
    }
  ]
}
```

### Audio Transcription Request

```json
{
  "id": "msg-004",
  "role": "user",
  "content": [
    {
      "type": "text",
      "text": "Please transcribe this audio recording"
    },
    {
      "type": "binary",
      "mimeType": "audio/wav",
      "filename": "meeting-recording.wav",
      "id": "audio-upload-123"
    }
  ]
}
```

### Document Analysis

```json
{
  "id": "msg-005",
  "role": "user",
  "content": [
    {
      "type": "text",
      "text": "Summarize the key points from this PDF"
    },
    {
      "type": "binary",
      "mimeType": "application/pdf",
      "filename": "quarterly-report.pdf",
      "url": "https://example.com/reports/q4-2024.pdf"
    }
  ]
}
```

## Implementation Considerations

### Client SDK Changes

TypeScript SDK:

- Extended `UserMessage` type in `@ag-ui/core`
- Content validation utilities
- Helper methods for constructing multimodal messages
- Binary content encoding/decoding utilities

Python SDK:

- Extended `UserMessage` class
- Content type validation
- Multimodal message builders
- Binary content handling utilities

### Framework Integration

Frameworks need to:

- Parse multimodal user messages
- Forward content to LLM providers that support multimodal inputs
- Handle fallbacks for models that don't support certain content types
- Manage content upload/storage for binary data

## Use Cases

### Visual Question Answering

Users can upload images and ask questions about them.

### Document Processing

Upload PDFs, Word documents, or spreadsheets for analysis.

### Audio Transcription and Analysis

Process voice recordings, podcasts, or meeting audio.

### Multi-document Comparison

Compare multiple images, documents, or mixed media.

### Screenshot Analysis

Share screenshots for UI/UX feedback or debugging assistance.

## Testing Strategy

- Unit tests for content type validation
- Integration tests with multimodal LLMs
- Backward compatibility tests with string content
- Performance tests for large binary payloads
- Security tests for content validation and sanitization

## References

- [OpenAI Vision API](https://platform.openai.com/docs/guides/vision)
- [Anthropic Vision](https://docs.anthropic.com/en/docs/vision)
- [MIME Types](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types)
- [Data URLs](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/Data_URLs)



================================================
FILE: docs/drafts/overview.mdx
================================================
---
title: Overview
description: Draft changes being considered for the AG-UI protocol
---

# Overview

This section contains draft changes being considered for the AG-UI protocol. These proposals are under internal review and may be modified or withdrawn before implementation.

## Current Drafts

<CardGroup cols={2}>
  <Card
    title="Activity Events"
    href="/drafts/activity-events"
    icon="file-lines"
    color="#3B82F6"
    iconType="light"
  >
    Represent ongoing agent progress between chat messages with fine-grained activity updates
  </Card>
  <Card
    title="Reasoning"
    href="/drafts/reasoning"
    icon="file-lines"
    color="#3B82F6"
    iconType="light"
  >
    Support for LLM reasoning visibility and continuity with encrypted content
  </Card>
  <Card
    title="Serialization"
    href="/drafts/serialization"
    icon="file-lines"
    color="#3B82F6"
    iconType="light"
  >
    Stream serialization for chat history restoration and event compaction
  </Card>
  <Card
    title="Multi-modal Messages"
    href="/drafts/multimodal-messages"
    icon="file-lines"
    color="#3B82F6"
    iconType="light"
  >
    Support for multimodal input messages including images, audio, and files
  </Card>
  <Card
    title="Interrupt-Aware Run Lifecycle"
    href="/drafts/interrupts"
    icon="file-lines"
    color="#3B82F6"
    iconType="light"
  >
    Native support for agent pauses requiring human approval or input
  </Card>
  <Card
    title="Generative User Interfaces"
    href="/drafts/generative-ui"
    icon="file-lines"
    color="#3B82F6"
    iconType="light"
  >
    AI-generated interfaces without requiring custom tool renderers
  </Card>
  <Card
    title="Meta Events"
    href="/drafts/meta-events"
    icon="file-lines"
    color="#3B82F6"
    iconType="light"
  >
    Annotations and signals independent of agent runs
  </Card>
</CardGroup>

## Status Definitions

- **Draft** - Initial proposal under consideration
- **Under Review** - Active development and testing
- **Accepted** - Approved for implementation
- **Implemented** - Merged into the main protocol specification
- **Withdrawn** - Proposal has been withdrawn or superseded


================================================
FILE: docs/drafts/reasoning.mdx
================================================
---
title: Reasoning
description: Support for LLM reasoning visibility and continuity
---

# Reasoning Proposal

## Summary

### Problem Statement

LLMs increasingly use chain-of-thought reasoning to improve response quality,
but there's no standardized way to surface reasoning signals while maintaining
privacy and state continuity across turns.

### Motivation

AG-UI should support **LLM reasoning** without breaking existing apps.

- **Reasoning visibility & continuity**: We must surface reasoning signals
  (e.g., **reasoning summaries**) and support **encrypted reasoning items** for
  state carry-over across turns—especially under `store:false`/ZDR—_without
  exposing raw chain-of-thought_.
- **Backwards compatibility**: Existing AG-UI clients must keep working
  unchanged. New capabilities should be non-breaking.

## Status

- **Status**: Draft
- **Author(s)**: Markus Ecker (mail@mme.xyz)

## Detailed Specification

### Overview

This proposal introduces:

- New events for reasoning lifecycle management
- A new `ReasoningMessage` type for message history
- Support for encrypted reasoning content

## New Reasoning Events

These events represent the lifecycle of reasoning messages in a conversation.

### ReasoningStartEvent

Marks the start of reasoning.

```typescript
type ReasoningStartEvent = BaseEvent & {
  type: EventType.REASONING_START
  messageId: string
  encryptedContent?: string
}
```

| Property           | Type      | Description                         |
| ------------------ | --------- | ----------------------------------- |
| `messageId`        | `string`  | Unique identifier of this reasoning |
| `encryptedContent` | `string?` | Optionally the encrypted content    |

### ReasoningMessageStartEvent

Signals the start of a reasoning message.

```typescript
type ReasoningMessageStartEvent = BaseEvent & {
  type: EventType.REASONING_MESSAGE_START
  messageId: string
  role: "assistant"
}
```

| Property    | Type          | Description                      |
| ----------- | ------------- | -------------------------------- |
| `messageId` | `string`      | Unique identifier of the message |
| `role`      | `"assistant"` | Role of the reasoning message    |

### ReasoningMessageContentEvent

Represents a chunk of content in a streaming reasoning message.

```typescript
type ReasoningMessageContentEvent = BaseEvent & {
  type: EventType.REASONING_MESSAGE_CONTENT
  messageId: string
  delta: string // Non-empty string
}
```

| Property    | Type     | Description                                    |
| ----------- | -------- | ---------------------------------------------- |
| `messageId` | `string` | Matches the ID from ReasoningMessageStartEvent |
| `delta`     | `string` | Reasoning content chunk (non-empty)            |

### ReasoningMessageEndEvent

Signals the end of a reasoning message.

```typescript
type ReasoningMessageEndEvent = BaseEvent & {
  type: EventType.REASONING_MESSAGE_END
  messageId: string
}
```

| Property    | Type     | Description                                    |
| ----------- | -------- | ---------------------------------------------- |
| `messageId` | `string` | Matches the ID from ReasoningMessageStartEvent |

### ReasoningMessageChunkEvent

A convenience event to auto start/close reasoning messages.

```typescript
type ReasoningMessageChunkEvent = BaseEvent & {
  type: EventType.REASONING_MESSAGE_CHUNK
  messageId?: string
  delta?: string
}
```

| Property    | Type      | Description                                |
| ----------- | --------- | ------------------------------------------ |
| `messageId` | `string?` | Message ID (first event must be non-empty) |
| `delta`     | `string?` | Reasoning content chunk                    |

### ReasoningEndEvent

Marks the end of reasoning.

```typescript
type ReasoningEndEvent = BaseEvent & {
  type: EventType.REASONING_END
  messageId: string
}
```

| Property    | Type     | Description                         |
| ----------- | -------- | ----------------------------------- |
| `messageId` | `string` | Unique identifier of this reasoning |

## New ReasoningMessage Type

```typescript
type ReasoningMessage = {
  id: string
  role: "reasoning"
  content: string[]
  encryptedContent?: string
}
```

## Removed Events

These events have never been publicly documented and will be removed:

- `THINKING_TEXT_MESSAGE_START`
- `THINKING_TEXT_MESSAGE_CONTENT`
- `THINKING_TEXT_MESSAGE_END`

## Implementation Considerations

### Client SDK Changes

TypeScript SDK:

- New event types in `@ag-ui/core`
- ReasoningMessage type in message unions
- Reasoning event handlers in subscriber
- Support for encrypted content handling

Python SDK:

- New event classes in `ag_ui.core.events`
- ReasoningMessage class
- Encryption/decryption utilities

### Privacy and Security

- **Encrypted reasoning**: Support for encrypted reasoning content that clients
  cannot decrypt
- **State continuity**: Encrypted reasoning items can be passed across turns
  without exposing content
- **ZDR compliance**: Works with `store:false` and zero data retention policies

### Backward Compatibility

- Clients not handling reasoning events continue to work
- Reasoning messages are optional in message history
- No changes required to existing integrations

## Use Cases

### Chain-of-Thought Visibility

Show users that the model is "thinking" without exposing internal reasoning.

### Reasoning Summaries

Provide high-level summaries of reasoning process for transparency.

### State Continuity

Maintain reasoning context across conversation turns without storing raw
content.

### Compliance and Privacy

Meet data retention requirements while preserving reasoning capabilities.

## Examples

### Basic Reasoning Flow

```typescript
// Agent emits reasoning start
{
  "type": "REASONING_START",
  "messageId": "reasoning-001",
  "encryptedContent": "encrypted-blob-xyz"
}

// Stream reasoning content (visible to client)
{
  "type": "REASONING_MESSAGE_START",
  "messageId": "msg-123",
  "role": "assistant"
}

{
  "type": "REASONING_MESSAGE_CONTENT",
  "messageId": "msg-123",
  "delta": "Let me think through this step by step..."
}

{
  "type": "REASONING_MESSAGE_END",
  "messageId": "msg-123"
}

// End reasoning
{
  "type": "REASONING_END",
  "messageId": "reasoning-001"
}
```

### Convenience Event Usage

```typescript
// Using chunk event for simpler implementation
{
  "type": "REASONING_MESSAGE_CHUNK",
  "messageId": "msg-456",
  "delta": "Analyzing the requirements..."
}

// Auto-closes on next non-reasoning event or empty chunk
{
  "type": "REASONING_MESSAGE_CHUNK",
  "messageId": "msg-456",
  "delta": ""
}
```

## Testing Strategy

- Unit tests for new event types
- Integration tests with reasoning-capable models
- Backward compatibility tests with existing clients
- Encryption/decryption roundtrip tests
- Performance tests for reasoning event streaming

## References

- [AG-UI Events Documentation](/concepts/events)
- [AG-UI Messages Documentation](/concepts/messages)



================================================
FILE: docs/drafts/serialization.mdx
================================================
---
title: Serialization
description:
  Stream serialization for chat history restoration and event compaction
---

# Serialization Proposal

## Summary

### Problem Statement

Currently, there's no standardized way to serialize and restore AG-UI event
streams, making it difficult to reload chat history, attach to running agents,
or implement branching/time travel features.

### Motivation

AG-UI adds **stream serialization** to reload chat history and attach to active
agents, enabling restoration and interaction with live state. A standardized
`compactEvents(events: BaseEvent[]): BaseEvent[]` reduces already-streamed
events and normalizes inputs. Additionally, `RunStartedEvent` gains
`parentRunId` for branching/time travel and an `input` field carrying the exact
`AgentInput` sent to the agent (which may omit messages already present in
history).

## Status

- **Status**: Draft
- **Author(s)**: Markus Ecker (mail@mme.xyz)

## Detailed Specification

### Overview

This proposal introduces three key capabilities:

1. **Stream serialization** - Serialize/deserialize event streams for
   persistence and restoration
2. **Event compaction** - Reduce event volume while preserving semantic meaning
3. **Run lineage tracking** - Enable branching and time travel with parent run
   references

### Git-like Branching Model

The `parentRunId` field enables a git-like branching structure where the entire
conversation history can be stored as a continuous serialized stream, with each
run forming nodes in a directed acyclic graph:

```mermaid
gitGraph
    commit id: "run1"
    commit id: "run2"
    branch alternative-path
    checkout alternative-path
    commit id: "run3 (parent run2)"
    commit id: "run4"
    checkout main
    commit id: "run5 (parent run2)"
    commit id: "run6"
    checkout alternative-path
```

**Key Benefits of This Append-Only Architecture:**

- **Immutable History**: Events are never modified or deleted, only appended
- **Serializable Stream**: The entire DAG can be stored as a single continuous
  stream of events
- **Multiple Branches**: Different conversation paths coexist in the same
  serialized log
- **Time Travel**: Navigate to any point in any branch without data loss
- **Efficient Storage**: Compaction reduces redundancy while preserving the full
  graph structure

## Proposed Changes

### Stream Serialization

Support serializing/deserializing the event stream so chat history can be
reloaded and sessions can attach to running agents/live state.

### Event Compaction

Introduce `compactEvents(events: BaseEvent[]): BaseEvent[]` to:

- Reduce the number of already-streamed events
- **Normalize** `RunStartedEvent.input` so it contains only the messages that
  were not already sent/recorded earlier in the thread

```typescript
// Event compaction API
declare function compactEvents(events: BaseEvent[]): BaseEvent[]
```

### Run Lineage and Input Capture

Extend `RunStartedEvent` with:

- `parentRunId?: string` to enable branching/time travel - similar to git
  commits, this creates an append-only log where each run points to its parent,
  forming a directed acyclic graph of conversation branches
- `input?: AgentInput` containing the agent input exactly as sent
  - `input.messages` **may omit** messages already present in history
  - `compactEvents` **normalizes** this field to a minimal form

## Updated Types

```typescript
type RunStartedEvent = BaseEvent & {
  type: EventType.RUN_STARTED
  threadId: string
  runId: string
  /**
   * Optional lineage pointer for branching/time travel.
   * If present, refers to a prior run within the same thread.
   * Creates a git-like append-only log where runs form a DAG (directed acyclic graph),
   * enabling conversation branching without mutating existing history.
   */
  parentRunId?: string
  /**
   * The exact AgentInput payload that was sent to the agent for this run.
   * May omit messages already present in history; compactEvents() will normalize.
   */
  input?: AgentInput
}
```

## Event Compaction Rules

The `compactEvents` function applies these transformations:

### Message Events

- Consecutive `TEXT_MESSAGE_CONTENT` events with same `messageId` → single event
  with concatenated content
- Complete message sequences (START + CONTENT + END) → single snapshot event
- Tool call sequences → compacted tool invocation records

### State Events

- Multiple `STATE_DELTA` events → single `STATE_SNAPSHOT` with final state
- Redundant state updates → removed if superseded by later snapshots

### Run Input Normalization

- Messages in `RunStartedEvent.input` that exist in prior events → removed
- Only new/incremental messages retained in normalized form

## Implementation Examples

### Basic Serialization

```typescript
// Serialize event stream
const events: BaseEvent[] = [...]; // Full event history
const serialized = JSON.stringify(events);

// Store to database, file, etc.
await storage.save(threadId, serialized);

// Later: deserialize and restore
const restored = JSON.parse(await storage.load(threadId));
const compacted = compactEvents(restored);
```

### Event Compaction Example

**Before compaction:**

```typescript
;[
  { type: "TEXT_MESSAGE_START", messageId: "msg1", role: "user" },
  { type: "TEXT_MESSAGE_CONTENT", messageId: "msg1", delta: "Hello " },
  { type: "TEXT_MESSAGE_CONTENT", messageId: "msg1", delta: "world" },
  { type: "TEXT_MESSAGE_END", messageId: "msg1" },
  { type: "STATE_DELTA", patch: { op: "add", path: "/foo", value: 1 } },
  { type: "STATE_DELTA", patch: { op: "replace", path: "/foo", value: 2 } },
]
```

**After compaction:**

```typescript
;[
  {
    type: "MESSAGES_SNAPSHOT",
    messages: [{ id: "msg1", role: "user", content: "Hello world" }],
  },
  {
    type: "STATE_SNAPSHOT",
    state: { foo: 2 },
  },
]
```

### Branching with Parent Run ID

The `parentRunId` field creates a git-like branching model where the event
stream becomes an immutable, append-only log. Each run can branch from any
previous run, creating alternative conversation paths without modifying the
original history.

```typescript
// Original run (like a git commit)
{
  type: "RUN_STARTED",
  threadId: "thread1",
  runId: "run1",
  input: { messages: ["Tell me about Paris"] }
}

// Branch from run1 (like creating a git branch from a specific commit)
{
  type: "RUN_STARTED",
  threadId: "thread1",
  runId: "run2",
  parentRunId: "run1",  // Points to parent, creating a new branch
  input: { messages: ["Actually, tell me about London instead"] }
}
```

This append-only structure ensures that:

- No existing events are ever modified or deleted
- Multiple branches can coexist in the same event stream
- You can always trace back the full lineage of any conversation branch
- Time travel and undo operations are possible without data loss

### Normalized Input Example

```typescript
// First run includes full message
{
  type: "RUN_STARTED",
  runId: "run1",
  input: {
    messages: [
      { id: "msg1", role: "user", content: "Hello" }
    ]
  }
}

// Second run omits already-present message
{
  type: "RUN_STARTED",
  runId: "run2",
  input: {
    messages: [
      { id: "msg2", role: "user", content: "How are you?" }
    ]
    // msg1 omitted as it's already in history
  }
}
```

## Use Cases

### Session Restoration

Reload a previous chat session with full history and state.

### Live Agent Attachment

Connect to an already-running agent and receive ongoing events.

### Branching Conversations

Create alternative conversation branches from any point in history.

### Time Travel Debugging

Navigate to any point in conversation history for debugging.

### Efficient Storage

Compact events before long-term storage to reduce size.

## Implementation Considerations

### Client SDK Changes

TypeScript SDK:

- `compactEvents` function implementation
- Serialization/deserialization utilities
- Branch management helpers
- Storage adapter interfaces

Python SDK:

- Event compaction algorithm
- Serialization utilities
- Parent run tracking
- Storage abstractions

### Storage Considerations

- Support for various storage backends (memory, database, file)
- Incremental storage for append-only events
- Compression support for serialized streams
- Indexing strategies for quick access

## Testing Strategy

- Unit tests for compaction algorithm
- Round-trip serialization tests
- Branch/merge scenario tests
- Performance benchmarks for large event streams
- Storage adapter integration tests

## References

- [Event Sourcing](https://martinfowler.com/eaaDev/EventSourcing.html)
- [AG-UI Events Documentation](/concepts/events)
- [AG-UI State Management](/concepts/state)
- [JSON Patch RFC 6902](https://tools.ietf.org/html/rfc6902)



================================================
FILE: docs/icons/custom-icons.tsx
================================================
import { FaReact } from "react-icons/fa";
import { HiOutlineServerStack } from "react-icons/hi2";
import { LuBrush, LuZap, LuGlobe } from "react-icons/lu";
import { SiLangchain } from "react-icons/si";
import { TbBrandTypescript } from "react-icons/tb";
import { FaPython } from "react-icons/fa";
import { SiCrewai } from "@icons-pack/react-simple-icons";
import { LuLayoutTemplate } from "react-icons/lu";
import { IconBaseProps } from "react-icons";
import { RocketIcon } from "lucide-react";

export const DirectToLLMIcon = (props: IconBaseProps) => (
  <svg
    width="24"
    height="24"
    viewBox="0 0 24 24"
    fill="none"
    xmlns="http://www.w3.org/2000/svg"
    {...props}
  >
    <path
      d="M12 2L13.09 8.26L20 9L15 14L16.18 21L12 17.77L7.82 21L9 14L2 9L8.91 8.26L12 2Z"
      fill="currentColor"
    />
    <path
      d="M12 16L10.5 22L12 20.5L13.5 22L12 16Z"
      fill="currentColor"
      opacity="0.6"
    />
  </svg>
);

export const ADKIcon = ({ className = "", ...props }: IconBaseProps) => (
  <svg
    version="1.1"
    xmlns="http://www.w3.org/2000/svg"
    viewBox="0 0 512 512"
    width="512"
    height="512"
    className={className}
    {...props}>
    <g transform="scale(0.9)" transformOrigin="center">
      <rect width="512" height="512" rx="128" ry="128" fill="#000000" />
    </g>
    <g transform="scale(0.85)" transformOrigin="center">
      <path d="M0 0 C1.31304611 -0.0071553 2.62609222 -0.01431061 3.97892761 -0.02168274 C5.41728187 -0.02452244 6.85563638 -0.02723424 8.29399109 -0.02983093 C9.82245027 -0.03610391 11.35090795 -0.04275144 12.87936401 -0.04974365 C17.88434302 -0.07069886 22.88932201 -0.08114259 27.89433289 -0.09111023 C29.62590742 -0.0951619 31.35748179 -0.09927894 33.08905602 -0.10346031 C41.23265771 -0.12249846 49.37625046 -0.13672865 57.5198701 -0.14507228 C66.87604093 -0.1548198 76.23197362 -0.18107987 85.58806068 -0.22157317 C92.8470237 -0.25189579 100.10591912 -0.2665566 107.36494416 -0.26985329 C111.68704116 -0.27218669 116.00888341 -0.28092007 120.33091164 -0.30631447 C156.48887641 -0.50776358 186.91460747 5.52279404 213.96855164 31.39717102 C235.95430616 54.21378255 243.8744487 81.85805342 243.56230164 113.01435852 C242.82592359 140.93045009 229.0853173 166.40142904 209.24198914 185.52217102 C191.67223194 201.09852568 168.35185318 212.20479416 144.56210327 212.29434204 C143.24905716 212.30149734 141.93601105 212.30865265 140.58317566 212.31602478 C139.14482141 212.31886448 137.7064669 212.32157628 136.26811218 212.32417297 C134.739653 212.33044595 133.21119532 212.33709348 131.68273926 212.34408569 C126.67776025 212.3650409 121.67278127 212.37548463 116.66777039 212.38545227 C114.93619586 212.38950394 113.20462148 212.39362098 111.47304726 212.39780235 C103.32944556 212.4168405 95.18585281 212.43107069 87.04223317 212.43941432 C77.68606234 212.44916184 68.33012965 212.47542191 58.97404259 212.51591522 C51.71507957 212.54623783 44.45618416 212.56089864 37.19715911 212.56419533 C32.87506212 212.56652873 28.55321986 212.57526211 24.23119164 212.60065651 C-11.92677313 212.80210562 -42.3525042 206.771548 -69.40644836 180.89717102 C-91.39220289 158.08055949 -99.31234543 130.43628862 -99.00019836 99.27998352 C-98.26382032 71.36389195 -84.52321403 45.892913 -64.67988586 26.77217102 C-47.11012867 11.19581636 -23.78974991 0.08954788 0 0 Z M-46.71894836 56.14717102 C-47.38668274 56.79299133 -48.05441711 57.43881165 -48.74238586 58.10420227 C-61.67195913 71.53106682 -66.89202154 90.95525774 -66.71894836 109.14717102 C-65.29530616 129.51620557 -57.05214152 145.96044845 -42.71894836 160.14717102 C-42.07312805 160.8149054 -41.42730774 161.48263977 -40.76191711 162.17060852 C-24.98727636 177.36100332 -3.35172872 180.30022142 17.56498718 180.29243469 C19.52295479 180.29747261 19.52295479 180.29747261 21.52047729 180.3026123 C25.07862687 180.31139119 28.63675235 180.31378787 32.19491172 180.31442809 C34.42546379 180.31514223 36.65600987 180.31728344 38.88656044 180.31992531 C46.68985503 180.32915951 54.49313262 180.33326274 62.2964325 180.33247375 C69.53487138 180.33187394 76.77323552 180.34241182 84.01165551 180.35821742 C90.25489513 180.37135716 96.49811051 180.37665495 102.74136382 180.37601835 C106.45736026 180.37576518 110.17329021 180.37851461 113.88927269 180.3891964 C118.04294698 180.40081177 122.19645719 180.39609147 126.35014343 180.39009094 C127.55737061 180.39571045 128.76459778 180.40132996 130.00840759 180.40711975 C152.39194774 180.32551845 172.41914732 174.77307242 188.63261414 158.82295227 C189.50659851 157.93994446 190.38058289 157.05693665 191.28105164 156.14717102 C192.2826532 155.17844055 192.2826532 155.17844055 193.30448914 154.19013977 C206.23406241 140.76327522 211.45412481 121.3390843 211.28105164 103.14717102 C209.85740944 82.77813647 201.61424479 66.33389359 187.28105164 52.14717102 C186.63523132 51.47943665 185.98941101 50.81170227 185.32402039 50.12373352 C169.54937963 34.93333872 147.91383199 31.99412062 126.99711609 32.00190735 C125.69180435 31.99854874 124.38649261 31.99519012 123.04162598 31.99172974 C119.4834764 31.98295085 115.92535092 31.98055418 112.36719155 31.97991395 C110.13663948 31.97919981 107.9060934 31.9770586 105.67554283 31.97441673 C97.87224824 31.96518253 90.06897065 31.9610793 82.26567078 31.96186829 C75.02723189 31.9624681 67.78886776 31.95193022 60.55044776 31.93612462 C54.30720814 31.92298488 48.06399276 31.91768709 41.82073945 31.9183237 C38.10474301 31.91857686 34.38881306 31.91582744 30.67283058 31.90514565 C26.51915629 31.89353027 22.36564608 31.89825057 18.21195984 31.9042511 C17.00473267 31.89863159 15.79750549 31.89301208 14.55369568 31.88722229 C-9.74912493 31.97582051 -29.57176548 38.82295232 -46.71894836 56.14717102 Z " fill="#4285F3" transform="translate(183.7189483642578,64.85282897949219)"/>
      <path d="M0 0 C1.0116774 -0.0071553 2.0233548 -0.01431061 3.06568909 -0.02168274 C4.18036057 -0.02437164 5.29503204 -0.02706055 6.44348145 -0.02983093 C7.61454941 -0.03640213 8.78561737 -0.04297333 9.99217224 -0.04974365 C13.88804468 -0.06953217 17.78390359 -0.08116344 21.67980957 -0.09111023 C23.02597575 -0.09515966 24.37214173 -0.09927664 25.7183075 -0.10346031 C32.05722186 -0.12253533 38.39612456 -0.13674947 44.73506212 -0.14507228 C52.0141461 -0.15478951 59.29292149 -0.18095284 66.57189703 -0.22157317 C72.21718494 -0.25197492 77.86238582 -0.2665645 83.50775385 -0.26985329 C86.86869574 -0.27218089 90.22905077 -0.28287636 93.58995056 -0.30631447 C127.52447927 -0.52804368 155.90682785 7.49509286 180.89855957 31.39717102 C202.8843141 54.21378255 210.80445664 81.85805342 210.49230957 113.01435852 C209.75593153 140.93045009 196.01532523 166.40142904 176.17199707 185.52217102 C157.21503455 202.3283436 133.22869857 212.44791206 107.77355957 212.20967102 C106.85123535 212.2051593 105.92891113 212.20064758 104.9786377 212.19599915 C102.72270909 212.1843104 100.4669199 212.16792557 98.21105957 212.14717102 C98.80660645 211.8661554 99.40215332 211.58513977 100.01574707 211.29560852 C104.55080571 208.92317571 108.31448565 205.9588493 110.21105957 201.14717102 C110.91963049 196.1542178 110.76258648 192.23052744 108.46105957 187.70967102 C105.35448295 184.17162543 102.39148647 182.20254758 98.21105957 180.14717102 C98.84495605 180.07723938 99.47885254 180.00730774 100.13195801 179.93525696 C117.08170288 178.25720486 117.08170288 178.25720486 133.21105957 173.14717102 C134.26035645 172.67666321 135.30965332 172.2061554 136.39074707 171.72138977 C155.03767156 162.52955288 167.5962346 146.47746783 174.27746582 127.04560852 C180.02943998 106.42742096 176.68231485 85.87299508 166.32043457 67.38545227 C155.2959017 50.46075274 138.75429348 39.2147683 119.18762207 34.65498352 C111.44830405 33.33462743 103.84588907 33.00120148 96.0032959 33.00532532 C94.91921951 33.00185089 93.83514313 32.99837646 92.71821594 32.99479675 C90.37912819 32.98748064 88.04003368 32.98211334 85.70093727 32.97850609 C81.99776791 32.9713016 78.29472806 32.95560844 74.59159851 32.93716431 C64.06803408 32.88485903 53.54455821 32.83667226 43.02087402 32.82124329 C36.57184779 32.81117073 30.12315547 32.78187562 23.67426109 32.73992348 C21.22029836 32.72761024 18.76628853 32.7224928 16.31229591 32.72472191 C12.88471273 32.72707104 9.45809275 32.70494028 6.03063965 32.67720032 C5.01906799 32.68380173 4.00749634 32.69040314 2.965271 32.69720459 C-2.80021041 32.62054081 -6.24711946 32.06102164 -10.78894043 28.14717102 C-15.09121633 23.37978421 -16.15176546 19.70969807 -16.07019043 13.42842102 C-15.48074654 8.64737616 -12.64073246 5.89755924 -9.16394043 2.83467102 C-5.79446793 0.44057214 -4.08456692 0.01979461 0 0 Z " fill="#34A753" transform="translate(216.7889404296875,64.85282897949219)"/>
      <path d="M0 0 C0.969944 -0.0099852 1.939888 -0.0199704 2.93922424 -0.03025818 C4.98230751 -0.04538665 7.02546429 -0.05248561 9.06860352 -0.05200195 C12.17914707 -0.05856787 15.28650395 -0.11311746 18.39648438 -0.16992188 C20.38931209 -0.17872467 22.38215484 -0.1846784 24.375 -0.1875 C25.2960704 -0.20905655 26.21714081 -0.2306131 27.16612244 -0.25282288 C33.11450504 -0.19027226 36.5357215 1.08734462 41.17347717 4.8250885 C45.27116401 9.1778744 45.29139256 13.69858971 45.20703125 19.44921875 C44.50380333 24.18994493 42.28995027 27.3409311 38.75390625 30.50390625 C28.00422925 35.87874475 14.7724105 32.50390625 2.75390625 32.50390625 C2.75390625 60.22390625 2.75390625 87.94390625 2.75390625 116.50390625 C13.31390625 116.50390625 23.87390625 116.50390625 34.75390625 116.50390625 C42.51252694 121.67632004 42.51252694 121.67632004 44.75390625 126.50390625 C45.76706246 132.70512099 45.75733985 137.90839454 42.00390625 143.12890625 C39.37436439 146.27638817 36.77704938 148.24253595 32.63180542 148.77806091 C31.87142975 148.78870071 31.11105408 148.79934052 30.32763672 148.81030273 C29.02510452 148.83428383 29.02510452 148.83428383 27.69625854 148.85874939 C26.76395416 148.86411209 25.83164978 148.86947479 24.87109375 148.875 C23.41692551 148.88854271 23.41692551 148.88854271 21.93338013 148.90235901 C19.88074833 148.91686062 17.82807032 148.92574802 15.77539062 148.92944336 C12.66064566 148.94128558 9.54791756 148.99042637 6.43359375 149.04101562 C4.43360335 149.05048689 2.43360086 149.05771856 0.43359375 149.0625 C-0.94664474 149.09179848 -0.94664474 149.09179848 -2.35476685 149.12168884 C-9.92572472 149.06982826 -16.49482381 146.93193879 -22.16726685 141.74827576 C-28.65531841 134.81932604 -29.69318322 127.92658301 -29.66601562 118.77026367 C-29.67448517 117.71241165 -29.68295471 116.65455963 -29.69168091 115.56465149 C-29.7150144 112.07943585 -29.7160738 108.59466273 -29.71484375 105.109375 C-29.72194814 102.67710698 -29.72964755 100.24484064 -29.73791504 97.81257629 C-29.75135198 92.71769552 -29.75293235 87.62296969 -29.74707031 82.52807617 C-29.74117899 76.01481781 -29.77174719 69.50230607 -29.81218719 62.98919201 C-29.83810035 57.96613087 -29.8409382 52.94324854 -29.83729553 47.92012978 C-29.83898377 45.51966795 -29.84859751 43.11919761 -29.86643982 40.7188015 C-29.88869701 37.3539445 -29.87889134 33.99058647 -29.86132812 30.62573242 C-29.87459137 29.64410126 -29.88785461 28.66247009 -29.90151978 27.65109253 C-29.80889397 19.9725961 -27.76071089 13.34695187 -22.49068451 7.58446026 C-15.75554207 1.27156305 -8.92472752 0.03227723 0 0 Z " fill="#FBBC04" transform="translate(114.24609375,298.49609375)"/>
      <path d="M0 0 C1.00804687 -0.02835937 2.01609375 -0.05671875 3.0546875 -0.0859375 C9.43320633 0.87165184 14.10205612 5.93302187 18.8125 10.0625 C20.05383751 11.13900176 21.29603371 12.21451409 22.5390625 13.2890625 C23.14782227 13.81532227 23.75658203 14.34158203 24.38378906 14.88378906 C27.65387491 17.68436174 31.00567465 20.38382978 34.34765625 23.09765625 C38.02975925 26.11687714 41.65562893 29.20001945 45.28271484 32.28491211 C48.75762479 35.23471308 52.26962023 38.13171652 55.8125 41 C59.4625647 43.99176857 63.10593378 46.98892096 66.6875 50.0625 C67.49316406 50.74699219 68.29882812 51.43148437 69.12890625 52.13671875 C72.25531681 55.64245897 72.99009286 59.16083572 73.5 63.75 C72.23112403 76.01580108 61.6567399 82.50975572 52.75 89.8125 C51.84507812 90.54339844 50.94015625 91.27429688 50.0078125 92.02734375 C46.84483953 94.60315236 43.73182328 97.23205222 40.625 99.875 C35.7854038 103.98801755 30.89546343 108.03129034 25.96484375 112.03515625 C23.85439681 113.77402519 21.76654861 115.52850079 19.69140625 117.30859375 C18.81742187 118.05689453 18.81742187 118.05689453 17.92578125 118.8203125 C16.79381663 119.79202146 15.66474961 120.76711942 14.5390625 121.74609375 C8.54117063 126.89141745 4.89439365 127.84040182 -2.9609375 127.6875 C-8.26240101 127.11897844 -10.71472495 124.59634216 -14.25 120.875 C-16.66962394 115.66350228 -16.82157547 111.01718605 -15.875 105.375 C-13.14263444 99.03635439 -7.2128306 94.95475519 -1.93359375 90.79296875 C1.31680523 88.22336854 4.47447973 85.56496979 7.625 82.875 C12.58417214 78.65340994 17.60608304 74.51974134 22.6875 70.4453125 C25.31218374 68.21601381 27.72108729 65.83901053 30.125 63.375 C22.79394251 57.04995624 15.43855789 50.77249376 7.921875 44.66796875 C3.62741415 41.14723059 -0.60036933 37.54886866 -4.83203125 33.953125 C-6.38015925 32.64148896 -7.93057745 31.33124859 -9.52490234 30.07617188 C-13.59763742 26.84364661 -15.51766108 24.78016584 -16.28125 19.640625 C-16.48744609 13.00798425 -15.87605463 9.04292903 -11.578125 3.8984375 C-7.91857027 0.62498258 -4.86609762 0.06190964 0 0 Z " fill="#EA4335" transform="translate(353.875,308.625)"/>
      <path d="M0 0 C4.8143755 4.57365673 6.75156236 7.6837462 7 14.375 C7.05442907 19.60019083 5.74920258 22.29348937 2.5 26.375 C0.41477163 28.44352984 -1.79612751 30.32113007 -4.0625 32.1875 C-4.68310303 32.70884521 -5.30370605 33.23019043 -5.94311523 33.76733398 C-10.4931635 37.5786204 -15.09008477 41.33381521 -19.6953125 45.078125 C-22.96474094 47.75559564 -26.20638031 50.46400467 -29.4375 53.1875 C-29.98156494 53.64390869 -30.52562988 54.10031738 -31.08618164 54.57055664 C-34.02478096 57.0547381 -36.82445279 59.60919337 -39.5 62.375 C-32.16894251 68.70004376 -24.81355789 74.97750624 -17.296875 81.08203125 C-13.00241415 84.60276941 -8.77463067 88.20113134 -4.54296875 91.796875 C-2.99484075 93.10851104 -1.44442255 94.41875141 0.14990234 95.67382812 C4.25387546 98.93114699 6.21122997 100.96410599 6.8359375 106.1796875 C6.96940663 116.67036144 6.96940663 116.67036144 2.5 121.375 C-1.14862143 124.84119036 -4.2835837 125.75704768 -9.1875 126.125 C-16.77761867 125.81729249 -20.9482353 122.2355227 -26.5 117.375 C-27.7114746 116.33764761 -28.92375896 115.30124039 -30.13671875 114.265625 C-32.72788532 112.04071271 -35.3021929 109.79798436 -37.8671875 107.54296875 C-41.75272303 104.13629156 -45.74276272 100.86973525 -49.7578125 97.6171875 C-53.42820794 94.61603796 -57.03852636 91.54658493 -60.64697266 88.47143555 C-64.21555198 85.43755671 -67.84162128 82.48821954 -71.5 79.5625 C-80.20927177 72.45289039 -80.20927177 72.45289039 -82.3125 67.4375 C-82.374375 66.756875 -82.43625 66.07625 -82.5 65.375 C-82.6546875 63.76625 -82.6546875 63.76625 -82.8125 62.125 C-82.06556266 53.16175197 -77.08290815 48.60490099 -70.48828125 42.875 C-66.808321 39.79634044 -63.11536392 36.73740932 -59.3828125 33.72265625 C-56.21983953 31.14684764 -53.10682328 28.51794778 -50 25.875 C-45.1604038 21.76198245 -40.27046343 17.71870966 -35.33984375 13.71484375 C-33.22939681 11.97597481 -31.14154861 10.22149921 -29.06640625 8.44140625 C-28.48375 7.94253906 -27.90109375 7.44367188 -27.30078125 6.9296875 C-26.16881663 5.95797854 -25.03974961 4.98288058 -23.9140625 4.00390625 C-16.77309541 -2.12201066 -8.93231938 -4.14714828 0 0 Z " fill="#EA4335" transform="translate(263.5,310.625)"/>
      <path d="M0 0 C4.58944252 2.59067597 8.07973488 5.95843049 9.58984375 11.1171875 C10.43895989 16.98821911 10.4991943 22.35248537 7.52734375 27.6171875 C4.02449419 32.28765359 0.50237103 34.69595422 -5.24609375 35.52734375 C-11.27485226 35.82346665 -15.88471772 35.3237664 -20.78515625 31.6484375 C-25.50423874 27.20160977 -26.66198643 23.50460631 -26.97265625 17.1796875 C-26.75311095 11.33246435 -25.4377323 7.39134984 -21.41015625 3.1171875 C-15.27205905 -1.89350409 -7.4975257 -2.74625739 0 0 Z " fill="#4285F4" transform="translate(327.41015625,153.8828125)"/>
      <path d="M0 0 C4.58944252 2.59067597 8.07973488 5.95843049 9.58984375 11.1171875 C10.43895989 16.98821911 10.4991943 22.35248537 7.52734375 27.6171875 C4.02449419 32.28765359 0.50237103 34.69595422 -5.24609375 35.52734375 C-11.27485226 35.82346665 -15.88471772 35.3237664 -20.78515625 31.6484375 C-25.50423874 27.20160977 -26.66198643 23.50460631 -26.97265625 17.1796875 C-26.75311095 11.33246435 -25.4377323 7.39134984 -21.41015625 3.1171875 C-15.27205905 -1.89350409 -7.4975257 -2.74625739 0 0 Z " fill="#4285F4" transform="translate(203.41015625,153.8828125)"/>
    </g>
  </svg>
);

export const AG2Icon = ({ className = "", ...props }: IconBaseProps) => (
  <svg
    xmlns="http://www.w3.org/2000/svg"
    fill="none"
    viewBox="0 0 90 50"
    className={className}
    {...props}
  >
    <path
      fill="currentColor"
      d="M69.285 0h-3.232v3.232h3.232V0Zm-3.232 16.095h-3.21v6.442h3.21v-6.442Zm0-12.863h-3.21v3.21h3.21v-3.21Zm-3.21 9.652h-3.21v3.21h3.21v-3.21Zm0-6.442h-3.21v3.232h3.21V6.442Zm-3.211 3.232H53.19v3.21h6.442v-3.21ZM53.19 6.442H37.095v3.232H53.19V6.442Zm6.442 19.305v-6.42h-3.231v-3.232H33.885v3.232h-3.232v6.42h28.98Zm-9.652-6.42h3.21v3.21h-3.21v-3.21Zm-12.885 0h3.21v3.21h-3.21v-3.21Zm0-9.653h-6.442v3.21h6.442v-3.21Zm-6.442 3.21h-3.21v3.21h3.21v-3.21Zm0-6.442h-3.21v3.232h3.21V6.442Zm-3.211 9.653h-3.21v6.442h3.21v-6.442Zm0-12.863h-3.21v3.21h3.21v-3.21ZM24.232 0H21v3.232h3.232V0Z"
    />
    <path
      fill="currentColor"
      d="M65.867 37.748V34.33H55.615v-3.418h10.252v3.418h3.418v3.417h-3.418Zm-6.834 3.417v-3.417h6.834v3.417h-6.834ZM55.615 48v-6.835h3.418v3.418h10.252V48h-13.67Zm-13.89-13.67v-3.417h10.252v3.418H41.725Zm-3.417 10.253V34.33h3.417v10.252h-3.417Zm10.252 0v-3.418h-3.417v-3.417h6.834v6.835H48.56ZM41.725 48v-3.417h6.835V48h-6.835ZM21 48V34.33h3.417v-3.417h6.835v3.418h3.418V48h-3.418v-6.835h-6.835V48H21Zm3.417-10.252h6.835v-3.28h-6.835v3.28Z"
    />
  </svg>
);

export const MastraIcon = ({ className = "", ...props }: IconBaseProps) => (
  <svg
    viewBox="0 0 34 34"
    fill="none"
    xmlns="http://www.w3.org/2000/svg"
    className={className}
    {...props}
  >
    <circle
      cx="16.6532"
      cy="16.9999"
      r="14.0966"
      stroke="currentColor"
      strokeWidth="1.16026"
    />
    <ellipse
      cx="16.6533"
      cy="17"
      rx="14.0966"
      ry="9.45478"
      transform="rotate(45 16.6533 17)"
      stroke="currentColor"
      strokeWidth="1.16026"
    />
    <path
      d="M10.8984 17.0508H22.483"
      stroke="currentColor"
      strokeWidth="1.16026"
    />
    <path
      d="M13.748 19.9932L19.6339 14.1074"
      stroke="currentColor"
      strokeWidth="1.16026"
    />
  </svg>
);

export const AgnoIcon = ({ className = "", ...props }: IconBaseProps) => (
  <svg
    viewBox="0 0 195 75"
    fill="none"
    xmlns="http://www.w3.org/2000/svg"
    className={className}
    {...props}
  >
    <path
      d="M44.9802 0.200195H16.5703V10.1702H38.0442L56.3442 58.8102H68.1658L44.9802 0.200195Z"
      fill="currentColor"
    />
    <path d="M29.59 48.8403H0.5V58.8103H29.59V48.8403Z" fill="currentColor" />
    <path
      fillRule="evenodd"
      clipRule="evenodd"
      d="M106.781 57.3271L106.748 57.2946C106.538 62.7046 104.862 66.8535 101.651 69.6754C99.5756 71.5114 96.5827 72.8283 93.2745 73.5576C89.9605 74.288 86.2923 74.4374 82.8361 73.9058C75.9469 72.8462 69.7197 69.0281 69.1124 61.5207L69.0687 60.9803H78.6597L78.725 61.4043C79.2382 64.7375 82.2366 66.4244 85.7803 66.7766C87.5364 66.9511 89.3766 66.7868 91.0038 66.3457C92.6378 65.9028 94.0152 65.1926 94.8845 64.3096L94.8981 64.2958L94.9127 64.2831C95.4723 63.7965 95.9123 63.0704 96.2494 62.1504C96.5856 61.2328 96.8086 60.152 96.9526 58.9907C97.2411 56.6652 97.2058 54.0891 97.1761 51.9777L97.176 51.9732L97.1725 51.7158C94.7704 55.4822 91.6364 57.5831 88.3259 58.402C84.6598 59.3088 80.8264 58.6317 77.5991 56.9885C73.8994 55.2095 71.4248 51.9689 69.8863 48.249C68.3481 44.5298 67.7308 40.2996 67.7608 36.4697C67.7213 29.0048 71.2243 20.2031 78.2871 16.8883C81.4568 15.376 85.3623 14.7771 88.9867 15.6312C92.1761 16.3827 95.1358 18.258 97.1608 21.5917V16.3104H106.781V57.3271ZM94.6816 46.608C92.9795 49.2165 90.4738 50.8999 87.1583 50.8504L87.1487 50.8504C83.8825 50.8645 81.4259 49.1844 79.7641 46.5957C78.0929 43.9925 77.2408 40.4841 77.2408 36.9466C77.2408 33.4091 78.0929 29.9014 79.764 27.2995C81.4258 24.7121 83.8822 23.0339 87.1483 23.0504L87.1567 23.0504L87.1652 23.0502C90.5349 22.9535 93.0495 24.6029 94.7414 27.1812C96.4443 29.7765 97.3024 33.306 97.2896 36.8736C97.2767 40.4407 96.3932 43.9852 94.6816 46.608Z"
      fill="currentColor"
    />
    <path
      d="M122.38 21.817V16.29H113.24V58.84H122.83V32.77C122.83 31.4259 123.051 30.1528 123.481 28.9482C123.912 27.743 124.51 26.7201 125.264 25.8723L125.272 25.8624C126.034 24.9601 126.941 24.2776 128.028 23.8091L128.036 23.8056L128.044 23.8018C129.13 23.2869 130.356 23.02 131.75 23.02C134.224 23.02 135.948 23.6948 137.019 24.9541L137.027 24.9623C138.12 26.1911 138.761 28.2555 138.86 31.2684V58.84H148.45V29C148.45 24.4884 147.192 21.012 144.594 18.678C142.019 16.364 138.517 15.23 134.16 15.23C131.517 15.23 129.089 15.8709 126.898 17.1784C125.084 18.2196 123.584 20.0156 122.38 21.817Z"
      fill="currentColor"
    />
    <path
      fillRule="evenodd"
      clipRule="evenodd"
      d="M189.43 21.9877C186.048 17.7695 180.97 15.0322 174.236 15.2401C167.503 15.0322 162.425 17.7695 159.043 21.9877C155.669 26.1945 153.999 31.8497 154 37.4791C154.001 43.1085 155.672 48.7631 159.045 52.9686C162.428 57.1855 167.505 59.9209 174.236 59.7106C180.967 59.9209 186.044 57.1855 189.427 52.9686C192.801 48.7631 194.472 43.1085 194.473 37.4791C194.473 31.8497 192.803 26.1945 189.43 21.9877ZM182.196 47.4833C180.409 50.1702 177.762 51.9245 174.238 51.9103H174.234C170.71 51.9245 168.064 50.1702 166.277 47.4833C164.481 44.7816 163.564 41.1431 163.561 37.4762C163.559 33.8093 164.471 30.1718 166.266 27.4716C168.051 24.7867 170.699 23.0337 174.234 23.0503L174.239 23.0503C177.773 23.0337 180.422 24.7867 182.207 27.4716C184.002 30.1718 184.914 33.8093 184.911 37.4762C184.909 41.1431 183.992 44.7816 182.196 47.4833Z"
      fill="currentColor"
    />
  </svg>
);

export const AgnoIconBlack = (props: IconBaseProps, className?: string) => (
  <svg
    width="195"
    height="75"
    viewBox="0 0 195 75"
    fill="none"
    xmlns="http://www.w3.org/2000/svg"
    {...props}
  >
    <path
      d="M44.9802 0.200195H16.5703V10.1702H38.0442L56.3442 58.8102H68.1658L44.9802 0.200195Z"
      fill="#18181B"
    />
    <path d="M29.59 48.8403H0.5V58.8103H29.59V48.8403Z" fill="#18181B" />
    <path
      fillRule="evenodd"
      clipRule="evenodd"
      d="M106.781 57.3271L106.748 57.2946C106.538 62.7046 104.862 66.8535 101.651 69.6754C99.5756 71.5114 96.5827 72.8283 93.2745 73.5576C89.9605 74.288 86.2923 74.4374 82.8361 73.9058C75.9469 72.8462 69.7197 69.0281 69.1124 61.5207L69.0687 60.9803H78.6597L78.725 61.4043C79.2382 64.7375 82.2366 66.4244 85.7803 66.7766C87.5364 66.9511 89.3766 66.7868 91.0038 66.3457C92.6378 65.9028 94.0152 65.1926 94.8845 64.3096L94.8981 64.2958L94.9127 64.2831C95.4723 63.7965 95.9123 63.0704 96.2494 62.1504C96.5856 61.2328 96.8086 60.152 96.9526 58.9907C97.2411 56.6652 97.2058 54.0891 97.1761 51.9777L97.176 51.9732L97.1725 51.7158C94.7704 55.4822 91.6364 57.5831 88.3259 58.402C84.6598 59.3088 80.8264 58.6317 77.5991 56.9885C73.8994 55.2095 71.4248 51.9689 69.8863 48.249C68.3481 44.5298 67.7308 40.2996 67.7608 36.4697C67.7213 29.0048 71.2243 20.2031 78.2871 16.8883C81.4568 15.376 85.3623 14.7771 88.9867 15.6312C92.1761 16.3827 95.1358 18.258 97.1608 21.5917V16.3104H106.781V57.3271ZM94.6816 46.608C92.9795 49.2165 90.4738 50.8999 87.1583 50.8504L87.1487 50.8504C83.8825 50.8645 81.4259 49.1844 79.7641 46.5957C78.0929 43.9925 77.2408 40.4841 77.2408 36.9466C77.2408 33.4091 78.0929 29.9014 79.764 27.2995C81.4258 24.7121 83.8822 23.0339 87.1483 23.0504L87.1567 23.0504L87.1652 23.0502C90.5349 22.9535 93.0495 24.6029 94.7414 27.1812C96.4443 29.7765 97.3024 33.306 97.2896 36.8736C97.2767 40.4407 96.3932 43.9852 94.6816 46.608Z"
      fill="#18181B"
    />
    <path
      d="M122.38 21.817V16.29H113.24V58.84H122.83V32.77C122.83 31.4259 123.051 30.1528 123.481 28.9482C123.912 27.743 124.51 26.7201 125.264 25.8723L125.272 25.8624C126.034 24.9601 126.941 24.2776 128.028 23.8091L128.036 23.8056L128.044 23.8018C129.13 23.2869 130.356 23.02 131.75 23.02C134.224 23.02 135.948 23.6948 137.019 24.9541L137.027 24.9623C138.12 26.1911 138.761 28.2555 138.86 31.2684V58.84H148.45V29C148.45 24.4884 147.192 21.012 144.594 18.678C142.019 16.364 138.517 15.23 134.16 15.23C131.517 15.23 129.089 15.8709 126.898 17.1784C125.084 18.2196 123.584 20.0156 122.38 21.817Z"
      fill="#18181B"
    />
    <path
      fillRule="evenodd"
      clipRule="evenodd"
      d="M189.43 21.9877C186.048 17.7695 180.97 15.0322 174.236 15.2401C167.503 15.0322 162.425 17.7695 159.043 21.9877C155.669 26.1945 153.999 31.8497 154 37.4791C154.001 43.1085 155.672 48.7631 159.045 52.9686C162.428 57.1855 167.505 59.9209 174.236 59.7106C180.967 59.9209 186.044 57.1855 189.427 52.9686C192.801 48.7631 194.472 43.1085 194.473 37.4791C194.473 31.8497 192.803 26.1945 189.43 21.9877ZM182.196 47.4833C180.409 50.1702 177.762 51.9245 174.238 51.9103H174.234C170.71 51.9245 168.064 50.1702 166.277 47.4833C164.481 44.7816 163.564 41.1431 163.561 37.4762C163.559 33.8093 164.471 30.1718 166.266 27.4716C168.051 24.7867 170.699 23.0337 174.234 23.0503L174.239 23.0503C177.773 23.0337 180.422 24.7867 182.207 27.4716C184.002 30.1718 184.914 33.8093 184.911 37.4762C184.909 41.1431 183.992 44.7816 182.196 47.4833Z"
      fill="#18181B"
    />
  </svg>
);

export const LlamaIndexIcon = ({ className = "", ...props }: IconBaseProps) => (
  <svg
    viewBox="0 0 81 80"
    version="1.1"
    xmlns="http://www.w3.org/2000/svg"
    className={className}
    {...props}
  >
    <title>llamaindex</title>
    <defs>
      <linearGradient
        x1="23.4558085%"
        y1="8.41682113%"
        x2="91.6436502%"
        y2="80.3192605%"
        id="linearGradient-1"
      >
        <stop stopColor="#F6DCD9" offset="6.19804%"></stop>
        <stop stopColor="#FFA5EA" offset="32.5677%"></stop>
        <stop stopColor="#45DFF8" offset="58.9257%"></stop>
        <stop stopColor="#BC8DEB" offset="100%"></stop>
      </linearGradient>
    </defs>
    <g id="Page-1" stroke="none" strokeWidth="1" fill="none" fillRule="evenodd">
      <g id="llamaindex" transform="translate(0, 0)" fillRule="nonzero">
        <path
          d="M0,16 C0,7.16344 7.16925,0 16.013,0 L64.0518,0 C72.8955,0 80.0648,7.16344 80.0648,16 L80.0648,64 C80.0648,72.8366 72.8955,80 64.0518,80 L16.013,80 C7.16924,80 0,72.8366 0,64 L0,16 Z"
          id="Path"
          fill="#000000"
        ></path>
        <path
          d="M50.3091,52.6201 C45.1552,54.8952 39.5718,53.963 37.4243,53.2126 C37.4243,53.726 37.4009,55.3218 37.3072,57.597 C37.2135,59.8721 36.4873,61.3099 36.1359,61.7444 C36.1749,63.1664 36.2062,66.271 36.0188,67.3138 C35.8313,68.3566 35.1598,69.2493 34.8474,69.5652 L31.6848,69.5652 C31.9659,68.1433 33.0513,67.2348 33.5589,66.9583 C33.84,64.0195 33.2856,61.4679 32.9733,60.5594 C32.6609,61.6654 31.8956,64.2328 31.3334,65.6548 C30.7711,67.0768 29.9278,68.3803 29.5763,68.8543 L27.2337,68.8543 C27.1165,67.4323 27.8974,66.9583 28.405,66.9583 C28.6393,66.5238 29.2015,65.1571 29.5763,63.1664 C29.9512,61.1756 29.4202,57.439 29.1078,55.8195 L29.1078,50.7241 C25.3595,48.7096 23.9539,46.6952 23.0168,44.4437 C22.2672,42.6425 22.4702,39.9013 22.6654,38.7558 C22.4311,38.3213 21.7481,37.217 21.4941,35.6749 C21.1427,33.5419 21.3379,32.0014 21.4941,31.1719 C21.2598,30.9349 20.7913,29.7263 20.7913,26.7875 C20.7913,23.8488 21.6502,22.3241 22.0797,21.9291 L22.0797,20.6256 C20.4398,20.5071 18.7999,19.7961 17.8629,18.8482 C16.9258,17.9002 17.6286,16.4782 18.2143,16.0042 C18.7999,15.5302 19.3856,15.8857 20.2056,15.6487 C21.0255,15.4117 21.7283,15.1747 22.0797,14.4637 C22.3608,13.895 21.8064,11.5408 21.494,10.4348 C22.8997,10.6244 23.7977,11.8568 24.071,12.4493L24.071,10.4348 C25.828,11.2643 28.9907,13.2788 30.0449,17.6632 C30.8882,21.1707 31.4895,28.5255 31.6847,31.7645 C36.1749,31.804 41.8755,31.1211 47.0294,32.2384 C51.7148,33.2542 53.8232,35.3194 56.283,35.3194 C58.7428,35.3194 60.1484,33.8974 61.9055,35.0824 C63.6625,36.2674 64.5996,39.5853 64.3653,42.0738 C64.1779,44.0645 62.6473,44.7202 61.9055,44.7992 C60.9684,47.9276 61.9055,50.9216 62.4911,52.0276 L62.4911,56.5305 C62.7645,56.9255 63.3111,58.1421 63.3111,59.8484 C63.3111,61.5548 62.7645,62.6924 62.4911,63.0479 C62.9597,65.7022 62.2959,68.4198 61.9055,69.4468 L58.7428,69.4468 C59.1177,68.4988 59.758,68.2618 60.0313,68.2618 C60.5936,65.3231 60.1875,62.6134 59.9142,61.6259 C58.1337,60.5831 56.9858,58.7425 56.6344,57.9525 C56.6735,58.624 56.5641,60.4883 55.8145,62.5739 C55.0648,64.6595 53.9403,65.8918 53.4718,66.2473 L53.4718,68.7358 L50.3091,68.7358 C50.3091,67.219 51.1681,66.9188 51.5976,66.9583 C52.1443,65.9708 53.4718,64.4699 53.4718,61.5074 C53.4718,59.0077 51.7148,57.834 50.4263,55.5825 C49.8141,54.5128 50.1139,53.1731 50.3091,52.6201 Z"
          id="Path"
          fill="url(#linearGradient-1)"
        ></path>
      </g>
    </g>
  </svg>
);

export const PydanticAIIcon = (props: IconBaseProps) => (
  <svg
    width="139px"
    height="120px"
    viewBox="0 0 139 120"
    version="1.1"
    xmlns="http://www.w3.org/2000/svg"
    {...props}
  >
    <g id="Page-1" stroke="none" strokeWidth="1" fill="none" fillRule="evenodd">
      <g
        id="pydantic-logo"
        transform="translate(0, 0.1733)"
        fill="currentColor"
        fillRule="nonzero"
      >
        <path
          d="M137.124,90.38975 L73.371,2.06775 C71.364,-0.68925 66.738,-0.68925 64.751,2.06775 L0.998,90.38975 C0.349072482,91.2935362 0,92.3781241 0,93.49075 C0.00318943775,95.7819584 1.469778,97.814973 3.643,98.54075 L67.397,119.39175 L67.407,119.39175 C68.4772724,119.740719 69.6307276,119.740719 70.701,119.39175 L70.711,119.39175 L134.464,98.54175 C136.077884,98.0193374 137.341287,96.7514677 137.858,95.13575 C138.390392,93.5257019 138.111354,91.7575889 137.109,90.38975 L137.124,90.38975 Z M69.064,14.23875 L94.617,49.64175 L70.721,41.82875 C70.536,41.76875 70.341,41.77875 70.157,41.73475 C69.976359,41.6901364 69.7924394,41.6600405 69.607,41.64475 C69.423,41.61975 69.248,41.54975 69.064,41.54975 C68.879,41.54975 68.709,41.61975 68.524,41.64475 C68.34,41.66475 68.155,41.69475 67.976,41.73475 C67.786,41.76975 67.591,41.76975 67.422,41.82875 L43.67,49.59675 L43.52,49.64675 L69.074,14.23675 L69.064,14.23675 L69.064,14.23875 Z M32.96,64.26475 L60.779,55.16075 L63.749,54.19375 L63.749,107.03175 L13.869,90.71375 L32.959,64.26475 L32.96,64.26475 Z M74.384,107.02175 L74.384,54.19375 L105.172,64.26475 L124.263,90.69875 L74.379,107.02175 L74.384,107.02175 Z"
          id="Shape"
        ></path>
      </g>
    </g>
  </svg>
);

export const customIcons = {
  adk: ADKIcon,
  react: FaReact,
  server: HiOutlineServerStack,
  zap: LuZap,
  brush: LuBrush,
  globe: LuGlobe,
  langchain: SiLangchain,
  typescript: TbBrandTypescript,
  python: FaPython,
  crewai: SiCrewai,
  component: LuLayoutTemplate,
  ag2: AG2Icon,
  mastra: MastraIcon,
  agno: AgnoIcon,
  agnoBlack: AgnoIconBlack,
  llamaindex: LlamaIndexIcon,
  pydantic: PydanticAIIcon,
  llm: RocketIcon,
  "direct-to-llm": RocketIcon,
};



================================================
FILE: docs/icons/index.tsx
================================================
import { icons as lucideIcons } from "lucide-react";
import { createElement } from "react";
import { customIcons } from "./custom-icons";

export function icon(icon: any) {
  if (!icon) {
    return;
  }

  let iconElement: React.ReactNode = null;

  if (icon.startsWith("lucide/")) {
    const iconName = icon.split("lucide/")[1];
    if (iconName in lucideIcons)
      iconElement = createElement(
        lucideIcons[iconName as keyof typeof lucideIcons]
      );
  }

  if (icon.startsWith("custom/")) {
    const iconName = icon.split("custom/")[1];
    if (iconName in customIcons)
      iconElement = createElement(
        customIcons[iconName as keyof typeof customIcons]
      );
  }

  return (
    <div key={icon} className="text-primary">
      {iconElement}
    </div>
  );
}



================================================
FILE: docs/images/left-illustration.avif
================================================
[Binary file]


================================================
FILE: docs/quickstart/applications.mdx
================================================
---
title: "Build applications"
description:
  "Build agentic applications utilizing compatible event AG-UI event streams"
---

# Introduction

AG-UI provides a concise, event-driven protocol that lets any agent stream rich,
structured output to any client. It can be used to connect any agentic system to
any client.

A client is defined as any system that can receieve, display, and respond to 
AG-UI events. For more information on existing clients and integrations, see
the [integrations](/integrations) page.

# Automatic Setup
AG-UI provides a CLI tool to automatically create or scaffold a new application with any client and server.

```sh
npx create-ag-ui-app@latest
```

<img
  className="w-full rounded-3xl mx-auto"
  src="https://copilotkit-public-assets.s3.us-east-1.amazonaws.com/docs/ag-ui/quickstart.gif"
/>

Once the setup is done, start the server with

```sh
npm run dev
```

For the copilotkit example you can head to http://localhost:3000/copilotkit to see the app in action.



================================================
FILE: docs/quickstart/clients.mdx
================================================
---
title: "Build clients"
description:
  "Showcase: build a conversational CLI agent from scratch using AG-UI and Mastra"
---

# Introduction

A client implementation allows you to **build conversational applications that
leverage AG-UI's event-driven protocol**. This approach creates a direct
interface between your users and AI agents, demonstrating direct access to the
AG-UI protocol.

## When to use a client implementation

Building your own client is useful if you want to explore/hack on the AG-UI
protocol. For production use, use a full-featured client like
[CopilotKit](https://copilotkit.ai).

## What you'll build

In this guide, we'll create a CLI client that:

1. Uses the `MastraAgent` from `@ag-ui/mastra`
2. Connects to OpenAI's GPT-4o model
3. Implements a weather tool for real-world functionality
4. Provides an interactive chat interface in the terminal

Let's get started!

## Prerequisites

Before we begin, make sure you have:

- [Node.js](https://nodejs.org/) **v18 or later**
- An **OpenAI API key**
- [pnpm](https://pnpm.io/) package manager

### 1. Provide your OpenAI API key

First, let's set up your API key:

```bash
# Set your OpenAI API key
export OPENAI_API_KEY=your-api-key-here
```

### 2. Install pnpm

If you don't have pnpm installed:

```bash
# Install pnpm
npm install -g pnpm
```

## Step 1 – Initialize your project

Create a new directory for your AG-UI client:

```bash
mkdir my-ag-ui-client
cd my-ag-ui-client
```

Initialize a new Node.js project:

```bash
pnpm init
```

### Set up TypeScript and basic configuration

Install TypeScript and essential development dependencies:

```bash
pnpm add -D typescript @types/node tsx
```

Create a `tsconfig.json` file:

```json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "commonjs",
    "lib": ["ES2022"],
    "outDir": "./dist",
    "rootDir": "./src",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "resolveJsonModule": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}
```

Update your `package.json` scripts:

```json
{
  // ...

  "scripts": {
    "start": "tsx src/index.ts",
    "dev": "tsx --watch src/index.ts",
    "build": "tsc",
    "clean": "rm -rf dist"
  }

  // ...
}
```

## Step 2 – Install AG-UI and dependencies

Install the core AG-UI packages and dependencies:

```bash
# Core AG-UI packages
pnpm add @ag-ui/client @ag-ui/core @ag-ui/mastra

# Mastra ecosystem packages
pnpm add @mastra/core @mastra/memory @mastra/libsql

# AI SDK and utilities
pnpm add @ai-sdk/openai zod@^3.25
```

## Step 3 – Create your agent

Let's create a basic conversational agent. Create `src/agent.ts`:

```typescript
import { openai } from "@ai-sdk/openai"
import { Agent } from "@mastra/core/agent"
import { MastraAgent } from "@ag-ui/mastra"
import { Memory } from "@mastra/memory"
import { LibSQLStore } from "@mastra/libsql"

export const agent = new MastraAgent({
  agent: new Agent({
    name: "AG-UI Assistant",
    instructions: `
      You are a helpful AI assistant. Be friendly, conversational, and helpful. 
      Answer questions to the best of your ability and engage in natural conversation.
    `,
    model: openai("gpt-4o"),
    memory: new Memory({
      storage: new LibSQLStore({
        url: "file:./assistant.db",
      }),
    }),
  }),
  threadId: "main-conversation",
})
```

### What's happening in the agent?

1. **MastraAgent** – We wrap a Mastra Agent with the AG-UI protocol adapter
2. **Model Configuration** – We use OpenAI's GPT-4o for high-quality responses
3. **Memory Setup** – We configure persistent memory using LibSQL for
   conversation context
4. **Instructions** – We give the agent basic guidelines for helpful
   conversation

## Step 4 – Create the CLI interface

Now let's create the interactive chat interface. Create `src/index.ts`:

```typescript
import * as readline from "readline"
import { agent } from "./agent"
import { randomUUID } from "node:crypto"

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout,
})

async function chatLoop() {
  console.log("🤖 AG-UI Assistant started!")
  console.log("Type your messages and press Enter. Press Ctrl+D to quit.\n")

  return new Promise<void>((resolve) => {
    const promptUser = () => {
      rl.question("> ", async (input) => {
        if (input.trim() === "") {
          promptUser()
          return
        }
        console.log("")

        // Pause input while processing
        rl.pause()

        // Add user message to conversation
        agent.messages.push({
          id: randomUUID(),
          role: "user",
          content: input.trim(),
        })

        try {
          // Run the agent with event handlers
          await agent.runAgent(
            {}, // No additional configuration needed
            {
              onTextMessageStartEvent() {
                process.stdout.write("🤖 Assistant: ")
              },
              onTextMessageContentEvent({ event }) {
                process.stdout.write(event.delta)
              },
              onTextMessageEndEvent() {
                console.log("\n")
              },
            }
          )
        } catch (error) {
          console.error("❌ Error:", error)
        }

        // Resume input
        rl.resume()
        promptUser()
      })
    }

    // Handle Ctrl+D to quit
    rl.on("close", () => {
      console.log("\n👋 Thanks for using AG-UI Assistant!")
      resolve()
    })

    promptUser()
  })
}

async function main() {
  await chatLoop()
}

main().catch(console.error)
```

### What's happening in the CLI interface?

1. **Readline Interface** – We create an interactive prompt for user input
2. **Message Management** – We add each user input to the agent's conversation
   history
3. **Event Handling** – We listen to AG-UI events to provide real-time feedback
4. **Streaming Display** – We show the agent's response as it's being generated

## Step 5 – Test your assistant

Let's run your new AG-UI client:

```bash
pnpm dev
```

You should see:

```
🤖 AG-UI Assistant started!
Type your messages and press Enter. Press Ctrl+D to quit.

>
```

Try asking questions like:

- "Hello! How are you?"
- "What can you help me with?"
- "Tell me a joke"
- "Explain quantum computing in simple terms"

You'll see the agent respond with streaming text in real-time!

## Step 6 – Understanding the AG-UI event flow

Let's break down what happens when you send a message:

1. **User Input** – You type a question and press Enter
2. **Message Added** – Your input is added to the conversation history
3. **Agent Processing** – The agent analyzes your request and formulates a
   response
4. **Response Generation** – The agent streams its response back
5. **Streaming Output** – You see the response appear word by word

### Event types you're handling:

- `onTextMessageStartEvent` – Agent starts responding
- `onTextMessageContentEvent` – Each chunk of the response
- `onTextMessageEndEvent` – Response is complete

## Step 7 – Add tool functionality

Now that you have a working chat interface, let's add some real-world
capabilities by creating tools. We'll start with a weather tool.

### Create your first tool

Let's create a weather tool that your agent can use. Create the directory
structure:

```bash
mkdir -p src/tools
```

Create `src/tools/weather.tool.ts`:

```typescript
import { createTool } from "@mastra/core/tools"
import { z } from "zod"

interface GeocodingResponse {
  results: {
    latitude: number
    longitude: number
    name: string
  }[]
}

interface WeatherResponse {
  current: {
    time: string
    temperature_2m: number
    apparent_temperature: number
    relative_humidity_2m: number
    wind_speed_10m: number
    wind_gusts_10m: number
    weather_code: number
  }
}

export const weatherTool = createTool({
  id: "get-weather",
  description: "Get current weather for a location",
  inputSchema: z.object({
    location: z.string().describe("City name"),
  }),
  outputSchema: z.object({
    temperature: z.number(),
    feelsLike: z.number(),
    humidity: z.number(),
    windSpeed: z.number(),
    windGust: z.number(),
    conditions: z.string(),
    location: z.string(),
  }),
  execute: async ({ context }) => {
    return await getWeather(context.location)
  },
})

const getWeather = async (location: string) => {
  const geocodingUrl = `https://geocoding-api.open-meteo.com/v1/search?name=${encodeURIComponent(
    location
  )}&count=1`
  const geocodingResponse = await fetch(geocodingUrl)
  const geocodingData = (await geocodingResponse.json()) as GeocodingResponse

  if (!geocodingData.results?.[0]) {
    throw new Error(`Location '${location}' not found`)
  }

  const { latitude, longitude, name } = geocodingData.results[0]

  const weatherUrl = `https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&current=temperature_2m,apparent_temperature,relative_humidity_2m,wind_speed_10m,wind_gusts_10m,weather_code`

  const response = await fetch(weatherUrl)
  const data = (await response.json()) as WeatherResponse

  return {
    temperature: data.current.temperature_2m,
    feelsLike: data.current.apparent_temperature,
    humidity: data.current.relative_humidity_2m,
    windSpeed: data.current.wind_speed_10m,
    windGust: data.current.wind_gusts_10m,
    conditions: getWeatherCondition(data.current.weather_code),
    location: name,
  }
}

function getWeatherCondition(code: number): string {
  const conditions: Record<number, string> = {
    0: "Clear sky",
    1: "Mainly clear",
    2: "Partly cloudy",
    3: "Overcast",
    45: "Foggy",
    48: "Depositing rime fog",
    51: "Light drizzle",
    53: "Moderate drizzle",
    55: "Dense drizzle",
    56: "Light freezing drizzle",
    57: "Dense freezing drizzle",
    61: "Slight rain",
    63: "Moderate rain",
    65: "Heavy rain",
    66: "Light freezing rain",
    67: "Heavy freezing rain",
    71: "Slight snow fall",
    73: "Moderate snow fall",
    75: "Heavy snow fall",
    77: "Snow grains",
    80: "Slight rain showers",
    81: "Moderate rain showers",
    82: "Violent rain showers",
    85: "Slight snow showers",
    86: "Heavy snow showers",
    95: "Thunderstorm",
    96: "Thunderstorm with slight hail",
    99: "Thunderstorm with heavy hail",
  }
  return conditions[code] || "Unknown"
}
```

### What's happening in the weather tool?

1. **Tool Definition** – We use `createTool` from Mastra to define the tool's
   interface
2. **Input Schema** – We specify that the tool accepts a location string
3. **Output Schema** – We define the structure of the weather data returned
4. **API Integration** – We fetch data from Open-Meteo's free weather API
5. **Data Processing** – We convert weather codes to human-readable conditions

### Update your agent

Now let's update our agent to use the weather tool. Update `src/agent.ts`:

```typescript
import { weatherTool } from "./tools/weather.tool" // <--- Import the tool

export const agent = new MastraAgent({
  agent: new Agent({
    // ...

    tools: { weatherTool }, // <--- Add the tool to the agent

    // ...
  }),
  threadId: "main-conversation",
})
```

### Update your CLI to handle tools

Update your CLI interface in `src/index.ts` to handle tool events:

```typescript
// Add these new event handlers to your agent.runAgent call:
await agent.runAgent(
  {}, // No additional configuration needed
  {
    // ... existing event handlers ...

    onToolCallStartEvent({ event }) {
      console.log("🔧 Tool call:", event.toolCallName)
    },
    onToolCallArgsEvent({ event }) {
      process.stdout.write(event.delta)
    },
    onToolCallEndEvent() {
      console.log("")
    },
    onToolCallResultEvent({ event }) {
      if (event.content) {
        console.log("🔍 Tool call result:", event.content)
      }
    },
  }
)
```

### Test your weather tool

Now restart your application and try asking about weather:

```bash
pnpm dev
```

Try questions like:

- "What's the weather like in London?"
- "How's the weather in Tokyo today?"
- "Is it raining in Seattle?"

You'll see the agent use the weather tool to fetch real data and provide
detailed responses!

## Step 8 – Add more functionality

### Create a browser tool

Let's add a web browsing capability. First install the `open` package:

```bash
pnpm add open
```

Create `src/tools/browser.tool.ts`:

```typescript
import { createTool } from "@mastra/core/tools"
import { z } from "zod"
import { open } from "open"

export const browserTool = createTool({
  id: "open-browser",
  description: "Open a URL in the default web browser",
  inputSchema: z.object({
    url: z.string().url().describe("The URL to open"),
  }),
  outputSchema: z.object({
    success: z.boolean(),
    message: z.string(),
  }),
  execute: async ({ context }) => {
    try {
      await open(context.url)
      return {
        success: true,
        message: `Opened ${context.url} in your default browser`,
      }
    } catch (error) {
      return {
        success: false,
        message: `Failed to open browser: ${error}`,
      }
    }
  },
})
```

### Update your agent with both tools

Update `src/agent.ts` to include both tools:

```typescript
import { openai } from "@ai-sdk/openai"
import { Agent } from "@mastra/core/agent"
import { MastraAgent } from "@ag-ui/mastra"
import { Memory } from "@mastra/memory"
import { LibSQLStore } from "@mastra/libsql"
import { weatherTool } from "./tools/weather.tool"
import { browserTool } from "./tools/browser.tool"

export const agent = new MastraAgent({
  agent: new Agent({
    name: "AG-UI Assistant",
    instructions: `
      You are a helpful assistant with weather and web browsing capabilities.

      For weather queries:
      - Always ask for a location if none is provided
      - Use the weatherTool to fetch current weather data

      For web browsing:
      - Always use full URLs (e.g., "https://www.google.com")
      - Use the browserTool to open web pages

      Be friendly and helpful in all interactions!
    `,
    model: openai("gpt-4o"),
    tools: { weatherTool, browserTool }, // Add both tools
    memory: new Memory({
      storage: new LibSQLStore({
        url: "file:./assistant.db",
      }),
    }),
  }),
  threadId: "main-conversation",
})
```

Now you can ask your assistant to open websites: "Open Google for me" or "Show
me the weather website".

## Step 9 – Deploy your client

### Building your client

Create a production build:

```bash
pnpm build
```

### Create a startup script

Add to your `package.json`:

```json
{
  "bin": {
    "weather-assistant": "./dist/index.js"
  }
}
```

Add a shebang to your built `dist/index.js`:

```javascript
#!/usr/bin/env node
// ... rest of your compiled code
```

Make it executable:

```bash
chmod +x dist/index.js
```

### Link globally

Install your CLI globally:

```bash
pnpm link --global
```

Now you can run `weather-assistant` from anywhere!

## Extending your client

Your AG-UI client is now a solid foundation. Here are some ideas for
enhancement:

### Add more tools

- **Calculator tool** – For mathematical operations
- **File system tool** – For reading/writing files
- **API tools** – For connecting to other services
- **Database tools** – For querying data

### Improve the interface

- **Rich formatting** – Use libraries like `chalk` for colored output
- **Progress indicators** – Show loading states for long operations
- **Configuration files** – Allow users to customize settings
- **Command-line arguments** – Support different modes and options

### Add persistence

- **Conversation history** – Save and restore chat sessions
- **User preferences** – Remember user settings
- **Tool results caching** – Cache expensive API calls

## Share your client

Built something useful? Consider sharing it with the community:

1. **Open source it** – Publish your code on GitHub
2. **Publish to npm** – Make it installable via `npm install`
3. **Create documentation** – Help others understand and extend your work
4. **Join discussions** – Share your experience in the
   [AG-UI GitHub Discussions](https://github.com/orgs/ag-ui-protocol/discussions)

## Conclusion

You've built a complete AG-UI client from scratch! Your weather assistant
demonstrates the core concepts:

- **Event-driven architecture** with real-time streaming
- **Tool integration** for real-world functionality
- **Conversation memory** for context retention
- **Interactive CLI interface** for user engagement

From here, you can extend your client to support any use case – from simple CLI
tools to complex conversational applications. The AG-UI protocol provides the
foundation, and your creativity provides the possibilities.

Happy building! 🚀



================================================
FILE: docs/quickstart/introduction.mdx
================================================
---
title: "Introduction"
description: "Learn how to get started building an AG-UI integration"
---

<video
  src="https://copilotkit-public-assets.s3.us-east-1.amazonaws.com/docs/ag-ui/ag-ui-animation-simple.mp4"
  autoPlay
  playsInline
  muted
  className="w-full h-[390px] rounded-lg object-cover mx-auto block"
/>

# What is an Integration?

An AG-UI integration makes your agent speak the AG-UI protocol. This means your agent can work with any AG-UI compatible client application - like chat interfaces, copilots, or custom AI tools.

Think of it like adding a universal translator to your agent. Instead of building custom APIs for each client, you implement AG-UI once and instantly work with any compatible application.

Agents integrating with AG-UI can:
- **Stream responses** - Real-time text that appears as it's generated
- **Call client-side tools** - Your agent can use functions and services defined by clients
- **Share state** - Your agent's state is bidirectional shared state
- **Execute universally** - Integrate with any AG-UI compatible client application
- **And much more!** - Check out the full specification [here](/concepts/events).

### When should I make any integration?
If the integration you're looking for is not listed on our [integrations page](/integrations), you'll need to make an integration. We've got a few guides on this below!

However, if you're looking to utilize an existing integration (like LangGraph, CrewAI, Mastra, etc.), you can skip this step and go straight to [building an application](/quickstart/applications).

# Types of Integrations
So you've decided you need an integration! Great, there are **two main ways** to implement an AG-UI integration:

<CardGroup cols={2}>
  <Card
    icon="server"
    title="Server Implementation"
    href="/quickstart/server"
  >
    Emit AG-UI events **directly from your agent** or server.
  </Card>
  <Card
    icon="code"
    title="Middleware Implementation"
    href="/quickstart/middleware"
  >
    **Translate existing protocols** and applications to AG-UI events.
  </Card>
</CardGroup>

### When to use a server implementation
Server implementations allow you to directly emit AG-UI events from your agent or server. If you are not using an
agent framework or haven't created a protocol for your agent framework yet, this is the best way to get started.

Server implementations are also great for:
- Building a **new agent frameworks** from scratch
- **Maximum control** over how and what events are emitted
- Exposing your agent as a **standalone API**

### When to use a middleware implementation
Middleware is the flexible option. It allows you to translate existing protocols and applications to AG-UI events
creating a bridge between your existing system and AG-UI.

Middleware is great for:
- Taking your **existing protocol or API** and **translating it universally**
- Working within the confines of **an existing system or framework**
- **When you don't have direct control** over the agent framework or system



================================================
FILE: docs/quickstart/middleware.mdx
================================================
---
title: "Middleware"
description:
  "Connect to existing protocols, in process agents or custom solutions via
  AG-UI"
---

# Introduction

A middleware implementation allows you to **translate existing protocols and
applications to AG-UI events**. This approach creates a bridge between your
existing system and AG-UI, making it perfect for adding agent capabilities to
current applications.

## When to use a middleware implementation

Middleware is the flexible option. It allows you to translate existing protocols
and applications to AG-UI events creating a bridge between your existing system
and AG-UI.

Middleware is great for:

- Taking your **existing protocol or API** and **translating it universally**
- Working within the confines of **an existing system or framework**
- **When you don't have direct control** over the agent framework or system

## What you'll build

In this guide, we'll create a middleware agent that:

1. Extends the `AbstractAgent` class
2. Connects to OpenAI's GPT-4o model
3. Translates OpenAI responses to AG-UI events
4. Runs in-process with your application

This approach gives you maximum flexibility to integrate with existing codebases
while maintaining the full power of the AG-UI protocol.

Let's get started!

## Prerequisites

Before we begin, make sure you have:

- [Node.js](https://nodejs.org/) **v16 or later**
- An **OpenAI API key**

### 1. Provide your OpenAI API key

First, let's set up your API key:

```bash
# Set your OpenAI API key
export OPENAI_API_KEY=your-api-key-here
```

### 2. Install build utilities

Install the following tools:

```bash
brew install protobuf
```

```bash
npm i turbo
```

```bash
curl -fsSL https://get.pnpm.io/install.sh | sh -
```

## Step 1 – Scaffold your integration

Start by cloning the repo and navigating to the TypeScript SDK:

```bash
git clone git@github.com:ag-ui-protocol/ag-ui.git
cd ag-ui/typescript-sdk
```

Copy the middleware-starter template to create your OpenAI integration:

```bash
cp -r integrations/middleware-starter integrations/openai
```

### Update metadata

Open `integrations/openai/package.json` and update the fields to match your new
folder:

```json
{
  "name": "@ag-ui/openai",
  "author": "Your Name <your-email@example.com>",
  "version": "0.0.1",

  ... rest of package.json
}
```

Next, update the class name inside `integrations/openai/src/index.ts`:

```ts
// change the name to OpenAIAgent
export class OpenAIAgent extends AbstractAgent {}
```

Finally, introduce your integration to the dojo by adding it to
`apps/dojo/src/menu.ts`:

```ts
// ...
export const menuIntegrations: MenuIntegrationConfig[] = [
  // ...

  {
    id: "openai",
    name: "OpenAI",
    features: ["agentic_chat"],
  },
]
```

And `apps/dojo/src/agents.ts`:

```ts
// ...
import { OpenAIAgent } from "@ag-ui/openai"

export const agentsIntegrations: AgentIntegrationConfig[] = [
  // ...

  {
    id: "openai",
    agents: async () => {
      return {
        agentic_chat: new OpenAIAgent(),
      }
    },
  },
]
```

## Step 2 – Add package to dojo dependencies

Open `apps/dojo/package.json` and add the package `@ag-ui/openai`:

```json
{
  "name": "demo-viewer",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "@ag-ui/agno": "workspace:*",
    "@ag-ui/langgraph": "workspace:*",
    "@ag-ui/mastra": "workspace:*",
    "@ag-ui/middleware-starter": "workspace:*",
    "@ag-ui/server-starter": "workspace:*",
    "@ag-ui/server-starter-all-features": "workspace:*",
    "@ag-ui/vercel-ai-sdk": "workspace:*",
    "@ag-ui/openai": "workspace:*", <- Add this line

  ... rest of package.json
}
```

## Step 3 – Start the dojo

Now let's see your work in action:

```bash
# Install dependencies
pnpm install

# Compile the project and run the dojo
turbo run dev
```

Head over to [http://localhost:3000](http://localhost:3000) and choose
**OpenAI** from the drop-down. You'll see the stub agent replies with **Hello
world!** for now.

Here's what's happening with that stub agent:

```ts
// integrations/openai/src/index.ts
import {
  AbstractAgent,
  BaseEvent,
  EventType,
  RunAgentInput,
} from "@ag-ui/client"
import { Observable } from "rxjs"

export class OpenAIAgent extends AbstractAgent {
  protected run(input: RunAgentInput): Observable<BaseEvent> {
    const messageId = Date.now().toString()
    return new Observable<BaseEvent>((observer) => {
      observer.next({
        type: EventType.RUN_STARTED,
        threadId: input.threadId,
        runId: input.runId,
      } as any)

      observer.next({
        type: EventType.TEXT_MESSAGE_START,
        messageId,
      } as any)

      observer.next({
        type: EventType.TEXT_MESSAGE_CONTENT,
        messageId,
        delta: "Hello world!",
      } as any)

      observer.next({
        type: EventType.TEXT_MESSAGE_END,
        messageId,
      } as any)

      observer.next({
        type: EventType.RUN_FINISHED,
        threadId: input.threadId,
        runId: input.runId,
      } as any)

      observer.complete()
    })
  }
}
```

## Step 4 – Bridge OpenAI with AG-UI

Let's transform our stub into a real agent that streams completions from OpenAI.

### Install the OpenAI SDK

First, we need the OpenAI SDK:

```bash
cd integrations/openai
pnpm install openai
```

### AG-UI recap

An AG-UI agent extends `AbstractAgent` and emits a sequence of events to signal:

- lifecycle events (`RUN_STARTED`, `RUN_FINISHED`, `RUN_ERROR`)
- content events (`TEXT_MESSAGE_*`, `TOOL_CALL_*`, and more)

### Implement the streaming agent

Now we'll transform our stub agent into a real OpenAI integration. The key
difference is that instead of sending a hardcoded "Hello world!" message, we'll
connect to OpenAI's API and stream the response back through AG-UI events.

The implementation follows the same event flow as our stub, but we'll add the
OpenAI client initialization in the constructor and replace our mock response
with actual API calls. We'll also handle tool calls if they're present in the
response, making our agent fully capable of using functions when needed.

```typescript
// integrations/openai/src/index.ts
import {
  AbstractAgent,
  RunAgentInput,
  EventType,
  BaseEvent,
} from "@ag-ui/client"
import { Observable } from "rxjs"

import { OpenAI } from "openai"

export class OpenAIAgent extends AbstractAgent {
  private openai: OpenAI

  constructor(openai?: OpenAI) {
    super()
    // Initialize OpenAI client - uses OPENAI_API_KEY from environment if not provided
    this.openai = openai ?? new OpenAI()
  }

  protected run(input: RunAgentInput): Observable<BaseEvent> {
    return new Observable<BaseEvent>((observer) => {
      // Same as before - emit RUN_STARTED to begin
      observer.next({
        type: EventType.RUN_STARTED,
        threadId: input.threadId,
        runId: input.runId,
      } as any)

      // NEW: Instead of hardcoded response, call OpenAI's API
      this.openai.chat.completions
        .create({
          model: "gpt-4o",
          stream: true, // Enable streaming for real-time responses
          // Convert AG-UI tools format to OpenAI's expected format
          tools: input.tools.map((tool) => ({
            type: "function",
            function: {
              name: tool.name,
              description: tool.description,
              parameters: tool.parameters,
            },
          })),
          // Transform AG-UI messages to OpenAI's message format
          messages: input.messages.map((message) => ({
            role: message.role as any,
            content: message.content ?? "",
            // Include tool calls if this is an assistant message with tools
            ...(message.role === "assistant" && message.toolCalls
              ? {
                  tool_calls: message.toolCalls,
                }
              : {}),
            // Include tool call ID if this is a tool result message
            ...(message.role === "tool"
              ? { tool_call_id: message.toolCallId }
              : {}),
          })),
        })
        .then(async (response) => {
          const messageId = Date.now().toString()

          // NEW: Stream each chunk from OpenAI's response
          for await (const chunk of response) {
            // Handle text content chunks
            if (chunk.choices[0].delta.content) {
              observer.next({
                type: EventType.TEXT_MESSAGE_CHUNK, // Chunk events open and close messages automatically
                messageId,
                delta: chunk.choices[0].delta.content,
              } as any)
            }
            // Handle tool call chunks (when the model wants to use a function)
            else if (chunk.choices[0].delta.tool_calls) {
              let toolCall = chunk.choices[0].delta.tool_calls[0]

              observer.next({
                type: EventType.TOOL_CALL_CHUNK,
                toolCallId: toolCall.id,
                toolCallName: toolCall.function?.name,
                parentMessageId: messageId,
                delta: toolCall.function?.arguments,
              } as any)
            }
          }

          // Same as before - emit RUN_FINISHED when complete
          observer.next({
            type: EventType.RUN_FINISHED,
            threadId: input.threadId,
            runId: input.runId,
          } as any)

          observer.complete()
        })
        // NEW: Handle errors from the API
        .catch((error) => {
          observer.next({
            type: EventType.RUN_ERROR,
            message: error.message,
          } as any)

          observer.error(error)
        })
    })
  }
}
```

### What happens under the hood?

Let's break down what your agent is doing:

1. **Setup** – We create an OpenAI client and emit `RUN_STARTED`
2. **Request** – We send the user's messages to `chat.completions` with
   `stream: true`
3. **Streaming** – We forward each chunk as either `TEXT_MESSAGE_CHUNK` or
   `TOOL_CALL_CHUNK`
4. **Finish** – We emit `RUN_FINISHED` (or `RUN_ERROR` if something goes wrong)
   and complete the observable

## Step 4 – Chat with your agent

Reload the dojo page and start typing. You'll see GPT-4o streaming its answer in
real-time, word by word.

## Bridging AG-UI to any protocol

The pattern you just implemented—translate inputs, forward streaming chunks,
emit AG-UI events—works for virtually any backend:

- REST or GraphQL APIs
- WebSockets
- IoT protocols such as MQTT

## Connect your agent to a frontend

Tools like [CopilotKit](https://docs.copilotkit.ai) already understand AG-UI and
provide plug-and-play React components. Point them at your agent endpoint and
you get a full-featured chat UI out of the box.

## Share your integration

Did you build a custom adapter that others could reuse? We welcome community
contributions!

1. Fork the [AG-UI repository](https://github.com/ag-ui-protocol/ag-ui)
2. Add your package under `typescript-sdk/integrations/`. See
   [Contributing](../development/contributing) for more details and naming
   conventions.
3. Open a pull request describing your use-case and design decisions

If you have questions, need feedback, or want to validate an idea first, start a
thread in the GitHub Discussions board:
[AG-UI GitHub Discussions board](https://github.com/orgs/ag-ui-protocol/discussions).

Your integration might ship in the next release and help the entire AG-UI
ecosystem grow.

## Conclusion

You now have a fully-functional AG-UI adapter for OpenAI and a local playground
to test it. From here you can:

- Add tool calls to enhance your agent
- Publish your integration to npm
- Bridge AG-UI to any other model or service

Happy building!



================================================
FILE: docs/quickstart/server.mdx
================================================
---
title: "Server"
description: "Implement AG-UI compatible servers"
---

# Introduction

A server implementation allows you to **emit AG-UI events directly from your
agent or server**. This approach is ideal when you're building a new agent from
scratch or want a dedicated service for your agent capabilities.

## When to use a server implementation

Server implementations allow you to directly emit AG-UI events from your agent
or server. If you are not using an agent framework or haven't created a protocol
for your agent framework yet, this is the best way to get started.

Server implementations are also great for:

- Building a **new agent frameworks** from scratch
- **Maximum control** over how and what events are emitted
- Exposing your agent as a **standalone API**

## What you'll build

In this guide, we'll create a standalone HTTP server that:

1. Accepts AG-UI protocol requests
2. Connects to OpenAI's GPT-4o model
3. Streams responses back as AG-UI events
4. Handles tool calls and state management

Let's get started!

## Prerequisites

Before we begin, make sure you have:

- [Python](https://www.python.org/downloads/) **3.12 or later**
- [Poetry](https://python-poetry.org/docs/#installation) for dependency
  management
- An **OpenAI API key**

### 1. Provide your OpenAI API key

First, let's set up your API key:

```bash
# Set your OpenAI API key
export OPENAI_API_KEY=your-api-key-here
```

### 2. Install build utilities

Install the following tools:

```bash
brew install protobuf
```

```bash
npm i turbo
```

```bash
curl -fsSL https://get.pnpm.io/install.sh | sh -
```

## Step 1 – Scaffold your server

Start by cloning the repo and navigating to the TypeScript SDK:

```bash
git clone git@github.com:ag-ui-protocol/ag-ui.git
cd ag-ui/typescript-sdk
```

Copy the server-starter template to create your OpenAI server:

```bash
cp -r integrations/server-starter integrations/openai-server
```

### Update metadata

Open `integrations/openai-server/package.json` and update the fields to match
your new folder:

```json
{
  "name": "@ag-ui/openai-server",
  "author": "Your Name <your-email@example.com>",
  "version": "0.0.1",

  ... rest of package.json
}
```

Next, update the class name inside `integrations/openai-server/src/index.ts`:

```ts
// Change the name to OpenAIServerAgent to add a minimal middleware for your integration.
// You can use this later on to add configuration etc.
export class OpenAIServerAgent extends HttpAgent {}
```

Finally, introduce your integration to the dojo by adding it to
`apps/dojo/src/menu.ts`:

```ts
// ...
export const menuIntegrations: MenuIntegrationConfig[] = [
  // ...

  {
    id: "openai-server",
    name: "OpenAI Server",
    features: ["agentic_chat"],
  },
]
```

And `apps/dojo/src/agents.ts`:

```ts
// ...
import { OpenAIServerAgent } from "@ag-ui/openai-server"

export const agentsIntegrations: AgentIntegrationConfig[] = [
  // ...

  {
    id: "openai-server",
    agents: async () => {
      return {
        agentic_chat: new OpenAIServerAgent(),
      }
    },
  },
]
```

## Step 2 – Add package to dojo dependencies

Open `apps/dojo/package.json` and add the package `@ag-ui/openai-server`:

```json
{
  "name": "demo-viewer",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "@ag-ui/agno": "workspace:*",
    "@ag-ui/langgraph": "workspace:*",
    "@ag-ui/mastra": "workspace:*",
    "@ag-ui/middleware-starter": "workspace:*",
    "@ag-ui/server-starter": "workspace:*",
    "@ag-ui/server-starter-all-features": "workspace:*",
    "@ag-ui/vercel-ai-sdk": "workspace:*",
    "@ag-ui/openai-server": "workspace:*", <- Add this line

  ... rest of package.json
}
```

## Step 3 – Start the dojo and server

Now let's see your work in action. First, start your Python server:

```bash
cd integrations/openai-server/server/python
poetry install && poetry run dev
```

In another terminal, start the dojo:

```bash
cd typescript-sdk

# Install dependencies
pnpm install

# Compile the project and run the dojo
turbo run dev
```

Head over to [http://localhost:3000](http://localhost:3000) and choose
**OpenAI** from the drop-down. You'll see the stub server replies with **Hello
world!** for now.

Here's what's happening with that stub server:

```python
# integrations/openai-server/server/python/example_server/__init__.py
@app.post("/")
async def agentic_chat_endpoint(input_data: RunAgentInput, request: Request):
    """Agentic chat endpoint"""
    # Get the accept header from the request
    accept_header = request.headers.get("accept")

    # Create an event encoder to properly format SSE events
    encoder = EventEncoder(accept=accept_header)

    async def event_generator():

        # Send run started event
        yield encoder.encode(
          RunStartedEvent(
            type=EventType.RUN_STARTED,
            thread_id=input_data.thread_id,
            run_id=input_data.run_id
          ),
        )

        message_id = str(uuid.uuid4())

        yield encoder.encode(
            TextMessageStartEvent(
                type=EventType.TEXT_MESSAGE_START,
                message_id=message_id,
                role="assistant"
            )
        )

        yield encoder.encode(
            TextMessageContentEvent(
                type=EventType.TEXT_MESSAGE_CONTENT,
                message_id=message_id,
                delta="Hello world!"
            )
        )

        yield encoder.encode(
            TextMessageEndEvent(
                type=EventType.TEXT_MESSAGE_END,
                message_id=message_id
            )
        )

        # Send run finished event
        yield encoder.encode(
          RunFinishedEvent(
            type=EventType.RUN_FINISHED,
            thread_id=input_data.thread_id,
            run_id=input_data.run_id
          ),
        )

    return StreamingResponse(
        event_generator(),
        media_type=encoder.get_content_type()
    )
```

## Step 4 – Bridge OpenAI with AG-UI

Let's transform our stub into a real server that streams completions from
OpenAI.

### Install the OpenAI SDK

First, we need the OpenAI SDK:

```bash
cd integrations/openai-server/server/python
poetry add openai
```

### AG-UI recap

An AG-UI server implements the endpoint and emits a sequence of events to
signal:

- lifecycle events (`RUN_STARTED`, `RUN_FINISHED`, `RUN_ERROR`)
- content events (`TEXT_MESSAGE_*`, `TOOL_CALL_*`, and more)

### Implement the streaming server

Now we'll transform our stub server into a real OpenAI integration. The key
difference is that instead of sending a hardcoded "Hello world!" message, we'll
connect to OpenAI's API and stream the response back through AG-UI events.

The implementation follows the same event flow as our stub, but we'll add the
OpenAI client initialization and replace our mock response with actual API
calls. We'll also handle tool calls if they're present in the response, making
our server fully capable of using functions when needed.

```python
import os
import uuid
import uvicorn
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
from ag_ui.core import (
    RunAgentInput,
    EventType,
    RunStartedEvent,
    RunFinishedEvent,
    RunErrorEvent,
)
from ag_ui.encoder import EventEncoder
from openai import OpenAI

app = FastAPI(title="AG-UI OpenAI Server")

# Initialize OpenAI client - uses OPENAI_API_KEY from environment
client = OpenAI()

@app.post("/")
async def agentic_chat_endpoint(input_data: RunAgentInput, request: Request):
    """OpenAI agentic chat endpoint"""
    accept_header = request.headers.get("accept")
    encoder = EventEncoder(accept=accept_header)

    async def event_generator():
        try:
            yield encoder.encode(
                RunStartedEvent(
                    type=EventType.RUN_STARTED,
                    thread_id=input_data.thread_id,
                    run_id=input_data.run_id
                )
            )

            # Call OpenAI's API with streaming enabled
            stream = client.chat.completions.create(
                model="gpt-4o",
                stream=True,
                # Convert AG-UI tools format to OpenAI's expected format
                tools=[
                    {
                        "type": "function",
                        "function": {
                            "name": tool.name,
                            "description": tool.description,
                            "parameters": tool.parameters,
                        }
                    }
                    for tool in input_data.tools
                ] if input_data.tools else None,
                # Transform AG-UI messages to OpenAI's message format
                messages=[
                    {
                        "role": message.role,
                        "content": message.content or "",
                        # Include tool calls if this is an assistant message with tools
                        **({"tool_calls": message.tool_calls} if message.role == "assistant" and hasattr(message, 'tool_calls') and message.tool_calls else {}),
                        # Include tool call ID if this is a tool result message
                        **({"tool_call_id": message.tool_call_id} if message.role == "tool" and hasattr(message, 'tool_call_id') else {}),
                    }
                    for message in input_data.messages
                ],
            )

            message_id = str(uuid.uuid4())

            # Stream each chunk from OpenAI's response
            for chunk in stream:
                # Handle text content chunks
                if chunk.choices[0].delta.content:
                    yield encoder.encode({
                        "type": EventType.TEXT_MESSAGE_CHUNK,
                        "message_id": message_id,
                        "delta": chunk.choices[0].delta.content,
                    })
                # Handle tool call chunks
                elif chunk.choices[0].delta.tool_calls:
                    tool_call = chunk.choices[0].delta.tool_calls[0]

                    yield encoder.encode({
                        "type": EventType.TOOL_CALL_CHUNK,
                        "tool_call_id": tool_call.id,
                        "tool_call_name": tool_call.function.name if tool_call.function else None,
                        "parent_message_id": message_id,
                        "delta": tool_call.function.arguments if tool_call.function else None,
                    })

            yield encoder.encode(
                RunFinishedEvent(
                    type=EventType.RUN_FINISHED,
                    thread_id=input_data.thread_id,
                    run_id=input_data.run_id
                )
            )

        except Exception as error:
            yield encoder.encode(
                RunErrorEvent(
                    type=EventType.RUN_ERROR,
                    message=str(error)
                )
            )

    return StreamingResponse(
        event_generator(),
        media_type=encoder.get_content_type()
    )

def main():
    """Run the uvicorn server."""
    port = int(os.getenv("PORT", "8000"))
    uvicorn.run(
        "example_server:app",
        host="0.0.0.0",
        port=port,
        reload=True
    )

if __name__ == "__main__":
    main()
```

### What happens under the hood?

Let's break down what your server is doing:

1. **Setup** – We create an OpenAI client and emit `RUN_STARTED`
2. **Request** – We send the user's messages to `chat.completions` with
   `stream=True`
3. **Streaming** – We forward each chunk as either `TEXT_MESSAGE_CHUNK` or
   `TOOL_CALL_CHUNK`
4. **Finish** – We emit `RUN_FINISHED` (or `RUN_ERROR` if something goes wrong)

## Step 5 – Chat with your server

Reload the dojo page and start typing. You'll see GPT-4o streaming its answer in
real-time, word by word.

Tools like [CopilotKit](https://docs.copilotkit.ai) already understand AG-UI and
provide plug-and-play React components. Point them at your server endpoint and
you get a full-featured chat UI out of the box.

## Share your integration

Did you build a custom server that others could reuse? We welcome community
contributions!

1. Fork the [AG-UI repository](https://github.com/ag-ui-protocol/ag-ui)
2. Add your package under `typescript-sdk/integrations/`. See
   [Contributing](../development/contributing) for more details and naming
   conventions.
3. Open a pull request describing your use-case and design decisions

If you have questions, need feedback, or want to validate an idea first, start a
thread in the GitHub Discussions board:
[AG-UI GitHub Discussions board](https://github.com/orgs/ag-ui-protocol/discussions).

Your integration might ship in the next release and help the entire AG-UI
ecosystem grow.

## Conclusion

You now have a fully-functional AG-UI server for OpenAI and a local playground
to test it. From here you can:

- Add tool calls to enhance your server
- Deploy your server to production
- Bring AG-UI to any other model or service

Happy building!



================================================
FILE: docs/sdk/js/encoder.mdx
================================================
---
title: "@ag-ui/encoder"
description: ""
---



================================================
FILE: docs/sdk/js/overview.mdx
================================================
[Empty file]


================================================
FILE: docs/sdk/js/proto.mdx
================================================
---
title: "@ag-ui/proto"
description: ""
---



================================================
FILE: docs/sdk/js/client/abstract-agent.mdx
================================================
---
title: "AbstractAgent"
description: "Base agent implementation with core event handling"
---

# AbstractAgent

The `AbstractAgent` class provides the foundation for all agent implementations
in the Agent User Interaction Protocol. It handles the core event stream
processing, state management, and message history.

```typescript
import { AbstractAgent } from "@ag-ui/client"
```

## Configuration

By default, all agents are configured by providing an optional `AgentConfig`
object to the constructor.

```typescript
interface AgentConfig {
  agentId?: string // The identifier of the agent
  description?: string // A description of the agent, used by the LLM
  threadId?: string // The conversation thread identifier
  initialMessages?: Message[] // An array of initial messages
  initialState?: State // The initial state of the agent
}
```

### Adding Configuration Options in your Subclass

To add additional configuration options, it is recommended to extend the
`AgentConfig` interface and call the super constructor with the extended config
from your subclass like this:

```typescript
interface MyAgentConfig extends AgentConfig {
  myConfigOption: string
}

class MyAgent extends AbstractAgent {
  private myConfigOption: string

  constructor(config: MyAgentConfig) {
    super(config)
    this.myConfigOption = config.myConfigOption
  }
}
```

## Core Methods

### runAgent()

The primary method for executing an agent and processing the result.

```typescript
runAgent(parameters?: RunAgentParameters, subscriber?: AgentSubscriber): Promise<RunAgentResult>
```

#### Parameters

```typescript
interface RunAgentParameters {
  runId?: string // Unique ID for this execution run
  tools?: Tool[] // Available tools for the agent
  context?: Context[] // Contextual information
  forwardedProps?: Record<string, any> // Additional properties to forward
}
```

The optional `subscriber` parameter allows you to provide an
[AgentSubscriber](/sdk/js/client/subscriber) for handling events during this
specific run.

#### Return Value

```typescript
interface RunAgentResult {
  result: any // The final result returned by the agent
  newMessages: Message[] // New messages added during this run
}
```

### subscribe()

Adds an [AgentSubscriber](/sdk/js/client/subscriber) to handle events across
multiple agent runs.

```typescript
subscribe(subscriber: AgentSubscriber): { unsubscribe: () => void }
```

Returns an object with an `unsubscribe()` method to remove the subscriber when
no longer needed.

### abortRun()

Cancels the current agent execution.

```typescript
abortRun(): void
```

### clone()

Creates a deep copy of the agent instance.

```typescript
clone(): AbstractAgent
```

## Properties

- `agentId`: Unique identifier for the agent instance
- `description`: Human-readable description
- `threadId`: Conversation thread identifier
- `messages`: Array of conversation messages
- `state`: Current agent state object

## Protected Methods

These methods are meant to be implemented or extended by subclasses:

### run()

Executes the agent and returns an observable event stream.

```typescript
protected abstract run(input: RunAgentInput): RunAgent
```

### apply()

Processes events from the run and updates the agent state.

```typescript
protected apply(input: RunAgentInput): ApplyEvents
```

### prepareRunAgentInput()

Prepares the input parameters for the agent execution.

```typescript
protected prepareRunAgentInput(parameters?: RunAgentParameters): RunAgentInput
```

### onError() and onFinalize()

Lifecycle hooks for error handling and cleanup operations.

```typescript
protected onError(error: Error): void
protected onFinalize(): void
```



================================================
FILE: docs/sdk/js/client/http-agent.mdx
================================================
---
title: "HttpAgent"
description: "HTTP-based agent for connecting to remote AI agents"
---

# HttpAgent

The `HttpAgent` extends `AbstractAgent` to provide HTTP-based connectivity to
remote AI agents. It handles the request/response cycle and transforms the HTTP
event stream into standard Agent User Interaction Protocol events.

```typescript
import { HttpAgent } from "@ag-ui/client"
```

## Configuration

When creating an HTTP agent, you need to provide an `HttpAgentConfig` object:

```typescript
interface HttpAgentConfig extends AgentConfig {
  url: string // Endpoint URL for the agent service
  headers?: Record<string, string> // Optional HTTP headers
}
```

## Creating an HttpAgent

```typescript
const agent = new HttpAgent({
  url: "https://api.example.com/v1/agent",
  headers: {
    Authorization: "Bearer your-api-key",
  },
})
```

## Methods

### runAgent()

Executes the agent by making an HTTP request to the configured endpoint.

```typescript
runAgent(parameters?: RunAgentParameters, subscriber?: AgentSubscriber): Promise<RunAgentResult>
```

#### Parameters

The `parameters` argument follows the standard `RunAgentParameters` interface.
The optional `subscriber` parameter allows you to provide an
[AgentSubscriber](/sdk/js/client/subscriber) for handling events during this
specific run.

#### Return Value

```typescript
interface RunAgentResult {
  result: any // The final result returned by the agent
  newMessages: Message[] // New messages added during this run
}
```

### subscribe()

Adds an [AgentSubscriber](/sdk/js/client/subscriber) to handle events across
multiple agent runs.

```typescript
subscribe(subscriber: AgentSubscriber): { unsubscribe: () => void }
```

Returns an object with an `unsubscribe()` method to remove the subscriber when
no longer needed.

### abortRun()

Cancels the current HTTP request using the AbortController.

```typescript
abortRun(): void
```

## Protected Methods

### requestInit()

Configures the HTTP request. Override this method to customize how requests are
made.

```typescript
protected requestInit(input: RunAgentInput): RequestInit
```

Default implementation:

```typescript
{
  method: "POST",
  headers: {
    ...this.headers,
    "Content-Type": "application/json",
    Accept: "text/event-stream",
  },
  body: JSON.stringify(input),
  signal: this.abortController.signal,
}
```

### run()

Implements the abstract `run()` method from `AbstractAgent` using HTTP requests.

```typescript
protected run(input: RunAgentInput): RunAgent
```

## Properties

- `url`: The endpoint URL for the agent service
- `headers`: HTTP headers to include with requests
- `abortController`: AbortController instance for request cancellation



================================================
FILE: docs/sdk/js/client/overview.mdx
================================================
---
title: "Overview"
description: "Client package overview"
---

# @ag-ui/client

The Agent User Interaction Protocol Client SDK provides agent connectivity
options for AI systems. This package builds on the core types and events to
deliver flexible connection methods to agent implementations.

```bash
npm install @ag-ui/client
```

## AbstractAgent

`AbstractAgent` is the base agent class for implementing custom agent
connectivity. Extending this class and implementing `run()` lets you bridge your
own service or agent implementation to AG-UI.

- [Configuration](/sdk/js/client/abstract-agent#configuration) - Setup with
  agent ID, messages, and state
- [Core Methods](/sdk/js/client/abstract-agent#core-methods) - Run, abort, and
  clone functionality
- [Protected Methods](/sdk/js/client/abstract-agent#protected-methods) -
  Extensible hooks for custom implementations
- [Properties](/sdk/js/client/abstract-agent#properties) - State and message
  tracking

<Card
  title="AbstractAgent Reference"
  icon="cube"
  href="/sdk/js/client/abstract-agent"
  color="#3B82F6"
  iconType="solid"
>
  Base class for creating custom agent connections
</Card>

## HttpAgent

Concrete implementation for HTTP-based agent connectivity:

- [Configuration](/sdk/js/client/http-agent#configuration) - URL and header
  setup
- [Methods](/sdk/js/client/http-agent#methods) - HTTP-specific execution and
  cancellation
- [Protected Methods](/sdk/js/client/http-agent#protected-methods) -
  Customizable HTTP request handling
- [Properties](/sdk/js/client/http-agent#properties) - Connection management

<Card
  title="HttpAgent Reference"
  icon="cube"
  href="/sdk/js/client/http-agent"
  color="#3B82F6"
  iconType="solid"
>
  Ready-to-use HTTP implementation for agent connectivity, using a highly
  efficient event encoding format
</Card>

## AgentSubscriber

Event-driven subscriber system for handling agent lifecycle events and state
mutations during agent execution:

- [Event Handlers](/sdk/js/client/subscriber#event-handlers) - Lifecycle,
  message, tool call, and state events
- [State Management](/sdk/js/client/subscriber#state-management) - Mutation
  control and propagation handling
- [Usage Examples](/sdk/js/client/subscriber#usage-examples) - Logging,
  persistence, and error handling patterns
- [Integration](/sdk/js/client/subscriber#integration-with-agents) - Registering
  and using subscribers with agents

<Card
  title="AgentSubscriber Reference"
  icon="bolt"
  href="/sdk/js/client/subscriber"
  color="#3B82F6"
  iconType="solid"
>
  Comprehensive event system for reactive agent interaction and middleware-style
  functionality
</Card>



================================================
FILE: docs/sdk/js/client/subscriber.mdx
================================================
---
title: "AgentSubscriber"
description:
  "Event-driven subscriber system for agent lifecycle and event handling"
---

# AgentSubscriber

The `AgentSubscriber` interface provides a comprehensive event-driven system for
handling agent lifecycle events, message updates, and state mutations during
agent execution. It allows you to hook into various stages of the agent's
operation and modify its behavior.

```typescript
import { AgentSubscriber } from "@ag-ui/client"
```

## Overview

`AgentSubscriber` defines a collection of optional event handlers and lifecycle
hooks that can respond to different stages of agent execution. All methods in
the interface are optional, allowing you to implement only the events you need
to handle.

All subscriber methods can be either synchronous or asynchronous - if they
return a Promise, the agent will await their completion before proceeding.

## Adding Subscribers to Agents

Subscribers can be added to agents in two ways:

### Permanent Subscription

Use the `subscribe()` method to add a subscriber that will persist across
multiple agent runs:

```typescript
const agent = new HttpAgent({ url: "https://api.example.com/agent" })

const subscriber: AgentSubscriber = {
  onTextMessageContentEvent: ({ textMessageBuffer }) => {
    console.log("Streaming text:", textMessageBuffer)
  },
}

// Add permanent subscriber
const subscription = agent.subscribe(subscriber)

// Later, remove the subscriber if needed
subscription.unsubscribe()
```

### Temporary Subscription

Pass a subscriber directly to `runAgent()` for one-time use:

```typescript
const temporarySubscriber: AgentSubscriber = {
  onRunFinishedEvent: ({ result }) => {
    console.log("Run completed with result:", result)
  },
}

// Use subscriber for this run only
await agent.runAgent({ tools: [myTool] }, temporarySubscriber)
```

## Core Interfaces

### AgentSubscriber

The main interface that defines all available event handlers and lifecycle
hooks. All methods in the interface are optional, allowing you to implement only
the events you need to handle.

### AgentStateMutation

Event handlers can return an `AgentStateMutation` object to modify the agent's
state and control event processing:

```typescript
interface AgentStateMutation {
  messages?: Message[] // Update the message history
  state?: State // Update the agent state
  stopPropagation?: boolean // Prevent further subscribers from processing this event
}
```

When a subscriber returns a mutation:

- **messages**: Replaces the current message history
- **state**: Replaces the current agent state
- **stopPropagation**: If `true`, prevents subsequent subscribers from handling
  the event (useful for overriding default behavior)

### AgentSubscriberParams

Common parameters passed to most subscriber methods:

```typescript
interface AgentSubscriberParams {
  messages: Message[] // Current message history
  state: State // Current agent state
  agent: AbstractAgent // The agent instance
  input: RunAgentInput // The original input parameters
}
```

## Event Handlers

### Lifecycle Events

#### onRunInitialized()

Called when the agent run is first initialized, before any processing begins.

```typescript
onRunInitialized?(params: AgentSubscriberParams): MaybePromise<Omit<AgentStateMutation, "stopPropagation"> | void>
```

#### onRunFailed()

Called when the agent run encounters an error.

```typescript
onRunFailed?(params: { error: Error } & AgentSubscriberParams): MaybePromise<Omit<AgentStateMutation, "stopPropagation"> | void>
```

#### onRunFinalized()

Called when the agent run completes, regardless of success or failure.

```typescript
onRunFinalized?(params: AgentSubscriberParams): MaybePromise<Omit<AgentStateMutation, "stopPropagation"> | void>
```

### Event Handlers

#### onEvent()

General event handler that receives all events during agent execution.

```typescript
onEvent?(params: { event: BaseEvent } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

#### onRunStartedEvent()

Triggered when an agent run begins execution.

```typescript
onRunStartedEvent?(params: { event: RunStartedEvent } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

#### onRunFinishedEvent()

Called when an agent run completes successfully.

```typescript
onRunFinishedEvent?(params: { event: RunFinishedEvent; result?: any } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

#### onRunErrorEvent()

Triggered when an agent run encounters an error.

```typescript
onRunErrorEvent?(params: { event: RunErrorEvent } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

#### onStepStartedEvent()

Called when a step within an agent run begins.

```typescript
onStepStartedEvent?(params: { event: StepStartedEvent } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

#### onStepFinishedEvent()

Triggered when a step within an agent run completes.

```typescript
onStepFinishedEvent?(params: { event: StepFinishedEvent } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

### Message Events

#### onTextMessageStartEvent()

Triggered when a text message starts being generated.

```typescript
onTextMessageStartEvent?(params: { event: TextMessageStartEvent } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

#### onTextMessageContentEvent()

Called for each chunk of text content as it's generated.

```typescript
onTextMessageContentEvent?(params: { event: TextMessageContentEvent; textMessageBuffer: string } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

#### onTextMessageEndEvent()

Called when a text message generation is complete.

```typescript
onTextMessageEndEvent?(params: { event: TextMessageEndEvent; textMessageBuffer: string } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

### Tool Call Events

#### onToolCallStartEvent()

Triggered when a tool call begins.

```typescript
onToolCallStartEvent?(params: { event: ToolCallStartEvent } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

#### onToolCallArgsEvent()

Called as tool call arguments are being parsed, providing both raw and parsed
argument data.

```typescript
onToolCallArgsEvent?(params: { event: ToolCallArgsEvent; toolCallBuffer: string; toolCallName: string; partialToolCallArgs: Record<string, any> } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

#### onToolCallEndEvent()

Called when a tool call is complete with final arguments.

```typescript
onToolCallEndEvent?(params: { event: ToolCallEndEvent; toolCallName: string; toolCallArgs: Record<string, any> } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

#### onToolCallResultEvent()

Triggered when a tool call result is received.

```typescript
onToolCallResultEvent?(params: { event: ToolCallResultEvent } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

### State Events

#### onStateSnapshotEvent()

Called when a complete state snapshot is provided.

```typescript
onStateSnapshotEvent?(params: { event: StateSnapshotEvent } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

#### onStateDeltaEvent()

Triggered when partial state changes are applied.

```typescript
onStateDeltaEvent?(params: { event: StateDeltaEvent } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

#### onMessagesSnapshotEvent()

Called when a complete message history snapshot is provided.

```typescript
onMessagesSnapshotEvent?(params: { event: MessagesSnapshotEvent } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

#### onRawEvent()

Handler for raw, unprocessed events.

```typescript
onRawEvent?(params: { event: RawEvent } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

#### onCustomEvent()

Handler for custom application-specific events.

```typescript
onCustomEvent?(params: { event: CustomEvent } & AgentSubscriberParams): MaybePromise<AgentStateMutation | void>
```

### State Change Handlers

#### onMessagesChanged()

Called when the agent's message history is updated.

```typescript
onMessagesChanged?(params: Omit<AgentSubscriberParams, "input"> & { input?: RunAgentInput }): MaybePromise<void>
```

#### onStateChanged()

Triggered when the agent's state is modified.

```typescript
onStateChanged?(params: Omit<AgentSubscriberParams, "input"> & { input?: RunAgentInput }): MaybePromise<void>
```

#### onNewMessage()

Called when a new message is added to the conversation.

```typescript
onNewMessage?(params: { message: Message } & Omit<AgentSubscriberParams, "input"> & { input?: RunAgentInput }): MaybePromise<void>
```

#### onNewToolCall()

Triggered when a new tool call is added to a message.

```typescript
onNewToolCall?(params: { toolCall: ToolCall } & Omit<AgentSubscriberParams, "input"> & { input?: RunAgentInput }): MaybePromise<void>
```

## Async Support

All subscriber methods support both synchronous and asynchronous execution:

```typescript
const subscriber: AgentSubscriber = {
  // Synchronous handler
  onTextMessageContentEvent: ({ textMessageBuffer }) => {
    updateUI(textMessageBuffer)
  },

  // Asynchronous handler
  onStateChanged: async ({ state }) => {
    await saveStateToDatabase(state)
  },

  // Async handler with mutation
  onRunInitialized: async ({ messages, state }) => {
    const enrichedState = await loadUserPreferences()
    return {
      state: { ...state, ...enrichedState },
    }
  },
}
```

## Multiple Subscribers

Agents can have multiple subscribers, which are processed in the order they were
added:

```typescript
// First subscriber modifies state
const stateEnricher: AgentSubscriber = {
  onRunInitialized: ({ state }) => ({
    state: { ...state, timestamp: new Date().toISOString() },
  }),
}

// Second subscriber sees the modified state
const logger: AgentSubscriber = {
  onRunInitialized: ({ state }) => {
    console.log("State after enrichment:", state)
  },
}

agent.subscribe(stateEnricher)
agent.subscribe(logger)
```

## Integration with Agents

Basic usage pattern:

```typescript
const agent = new HttpAgent({ url: "https://api.example.com/agent" })

// Add persistent subscriber
agent.subscribe({
  onTextMessageContentEvent: ({ textMessageBuffer }) => {
    updateStreamingUI(textMessageBuffer)
  },
  onRunFinishedEvent: ({ result }) => {
    displayFinalResult(result)
  },
})

// Run agent (subscriber will be called automatically)
const result = await agent.runAgent({
  tools: [myTool],
})
```



================================================
FILE: docs/sdk/js/core/events.mdx
================================================
---
title: "Events"
description:
  "Documentation for the events used in the Agent User Interaction Protocol SDK"
---

# Events

The Agent User Interaction Protocol SDK uses a streaming event-based
architecture. Events are the fundamental units of communication between agents
and the frontend. This section documents the event types and their properties.

## EventType Enum

The `EventType` enum defines all possible event types in the system:

```typescript
enum EventType {
  TEXT_MESSAGE_START = "TEXT_MESSAGE_START",
  TEXT_MESSAGE_CONTENT = "TEXT_MESSAGE_CONTENT",
  TEXT_MESSAGE_END = "TEXT_MESSAGE_END",
  TOOL_CALL_START = "TOOL_CALL_START",
  TOOL_CALL_ARGS = "TOOL_CALL_ARGS",
  TOOL_CALL_END = "TOOL_CALL_END",
  TOOL_CALL_RESULT = "TOOL_CALL_RESULT",
  STATE_SNAPSHOT = "STATE_SNAPSHOT",
  STATE_DELTA = "STATE_DELTA",
  MESSAGES_SNAPSHOT = "MESSAGES_SNAPSHOT",
  RAW = "RAW",
  CUSTOM = "CUSTOM",
  RUN_STARTED = "RUN_STARTED",
  RUN_FINISHED = "RUN_FINISHED",
  RUN_ERROR = "RUN_ERROR",
  STEP_STARTED = "STEP_STARTED",
  STEP_FINISHED = "STEP_FINISHED",
}
```

## BaseEvent

All events inherit from the `BaseEvent` type, which provides common properties
shared across all event types.

```typescript
type BaseEvent = {
  type: EventType // Discriminator field
  timestamp?: number
  rawEvent?: any
}
```

| Property    | Type                | Description                                           |
| ----------- | ------------------- | ----------------------------------------------------- |
| `type`      | `EventType`         | The type of event (discriminator field for the union) |
| `timestamp` | `number` (optional) | Timestamp when the event was created                  |
| `rawEvent`  | `any` (optional)    | Original event data if this event was transformed     |

## Lifecycle Events

These events represent the lifecycle of an agent run.

### RunStartedEvent

Signals the start of an agent run.

```typescript
type RunStartedEvent = BaseEvent & {
  type: EventType.RUN_STARTED
  threadId: string
  runId: string
}
```

| Property   | Type     | Description                   |
| ---------- | -------- | ----------------------------- |
| `threadId` | `string` | ID of the conversation thread |
| `runId`    | `string` | ID of the agent run           |

### RunFinishedEvent

Signals the successful completion of an agent run.

```typescript
type RunFinishedEvent = BaseEvent & {
  type: EventType.RUN_FINISHED
  threadId: string
  runId: string
  result?: any
}
```

| Property   | Type             | Description                    |
| ---------- | ---------------- | ------------------------------ |
| `threadId` | `string`         | ID of the conversation thread  |
| `runId`    | `string`         | ID of the agent run            |
| `result`   | `any` (optional) | Result data from the agent run |

### RunErrorEvent

Signals an error during an agent run.

```typescript
type RunErrorEvent = BaseEvent & {
  type: EventType.RUN_ERROR
  message: string
  code?: string
}
```

| Property  | Type                | Description   |
| --------- | ------------------- | ------------- |
| `message` | `string`            | Error message |
| `code`    | `string` (optional) | Error code    |

### StepStartedEvent

Signals the start of a step within an agent run.

```typescript
type StepStartedEvent = BaseEvent & {
  type: EventType.STEP_STARTED
  stepName: string
}
```

| Property   | Type     | Description      |
| ---------- | -------- | ---------------- |
| `stepName` | `string` | Name of the step |

### StepFinishedEvent

Signals the completion of a step within an agent run.

```typescript
type StepFinishedEvent = BaseEvent & {
  type: EventType.STEP_FINISHED
  stepName: string
}
```

| Property   | Type     | Description      |
| ---------- | -------- | ---------------- |
| `stepName` | `string` | Name of the step |

## Text Message Events

These events represent the lifecycle of text messages in a conversation.

### TextMessageStartEvent

Signals the start of a text message.

```typescript
type TextMessageStartEvent = BaseEvent & {
  type: EventType.TEXT_MESSAGE_START
  messageId: string
  role: "assistant"
}
```

| Property    | Type          | Description                       |
| ----------- | ------------- | --------------------------------- |
| `messageId` | `string`      | Unique identifier for the message |
| `role`      | `"assistant"` | Role is always "assistant"        |

### TextMessageContentEvent

Represents a chunk of content in a streaming text message.

```typescript
type TextMessageContentEvent = BaseEvent & {
  type: EventType.TEXT_MESSAGE_CONTENT
  messageId: string
  delta: string // Non-empty string
}
```

| Property    | Type     | Description                               |
| ----------- | -------- | ----------------------------------------- |
| `messageId` | `string` | Matches the ID from TextMessageStartEvent |
| `delta`     | `string` | Text content chunk (non-empty)            |

### TextMessageEndEvent

Signals the end of a text message.

```typescript
type TextMessageEndEvent = BaseEvent & {
  type: EventType.TEXT_MESSAGE_END
  messageId: string
}
```

| Property    | Type     | Description                               |
| ----------- | -------- | ----------------------------------------- |
| `messageId` | `string` | Matches the ID from TextMessageStartEvent |

## Tool Call Events

These events represent the lifecycle of tool calls made by agents.

### ToolCallStartEvent

Signals the start of a tool call.

```typescript
type ToolCallStartEvent = BaseEvent & {
  type: EventType.TOOL_CALL_START
  toolCallId: string
  toolCallName: string
  parentMessageId?: string
}
```

| Property          | Type                | Description                         |
| ----------------- | ------------------- | ----------------------------------- |
| `toolCallId`      | `string`            | Unique identifier for the tool call |
| `toolCallName`    | `string`            | Name of the tool being called       |
| `parentMessageId` | `string` (optional) | ID of the parent message            |

### ToolCallArgsEvent

Represents a chunk of argument data for a tool call.

```typescript
type ToolCallArgsEvent = BaseEvent & {
  type: EventType.TOOL_CALL_ARGS
  toolCallId: string
  delta: string
}
```

| Property     | Type     | Description                            |
| ------------ | -------- | -------------------------------------- |
| `toolCallId` | `string` | Matches the ID from ToolCallStartEvent |
| `delta`      | `string` | Argument data chunk                    |

### ToolCallEndEvent

Signals the end of a tool call.

```typescript
type ToolCallEndEvent = BaseEvent & {
  type: EventType.TOOL_CALL_END
  toolCallId: string
}
```

| Property     | Type     | Description                            |
| ------------ | -------- | -------------------------------------- |
| `toolCallId` | `string` | Matches the ID from ToolCallStartEvent |

### ToolCallResultEvent

Provides the result of a tool call execution.

```typescript
type ToolCallResultEvent = BaseEvent & {
  type: EventType.TOOL_CALL_RESULT
  messageId: string
  toolCallId: string
  content: string
  role?: "tool"
}
```

| Property     | Type                | Description                                                 |
| ------------ | ------------------- | ----------------------------------------------------------- |
| `messageId`  | `string`            | ID of the conversation message this result belongs to       |
| `toolCallId` | `string`            | Matches the ID from the corresponding ToolCallStartEvent    |
| `content`    | `string`            | The actual result/output content from the tool execution    |
| `role`       | `"tool"` (optional) | Optional role identifier, typically "tool" for tool results |

## State Management Events

These events are used to manage agent state.

### StateSnapshotEvent

Provides a complete snapshot of an agent's state.

```typescript
type StateSnapshotEvent = BaseEvent & {
  type: EventType.STATE_SNAPSHOT
  snapshot: any // StateSchema
}
```

| Property   | Type  | Description             |
| ---------- | ----- | ----------------------- |
| `snapshot` | `any` | Complete state snapshot |

### StateDeltaEvent

Provides a partial update to an agent's state using JSON Patch.

```typescript
type StateDeltaEvent = BaseEvent & {
  type: EventType.STATE_DELTA
  delta: any[] // JSON Patch operations (RFC 6902)
}
```

| Property | Type    | Description                    |
| -------- | ------- | ------------------------------ |
| `delta`  | `any[]` | Array of JSON Patch operations |

### MessagesSnapshotEvent

Provides a snapshot of all messages in a conversation.

```typescript
type MessagesSnapshotEvent = BaseEvent & {
  type: EventType.MESSAGES_SNAPSHOT
  messages: Message[]
}
```

| Property   | Type        | Description              |
| ---------- | ----------- | ------------------------ |
| `messages` | `Message[]` | Array of message objects |

## Special Events

### RawEvent

Used to pass through events from external systems.

```typescript
type RawEvent = BaseEvent & {
  type: EventType.RAW
  event: any
  source?: string
}
```

| Property | Type                | Description         |
| -------- | ------------------- | ------------------- |
| `event`  | `any`               | Original event data |
| `source` | `string` (optional) | Source of the event |

### CustomEvent

Used for application-specific custom events.

```typescript
type CustomEvent = BaseEvent & {
  type: EventType.CUSTOM
  name: string
  value: any
}
```

| Property | Type     | Description                     |
| -------- | -------- | ------------------------------- |
| `name`   | `string` | Name of the custom event        |
| `value`  | `any`    | Value associated with the event |

## Event Schemas

The SDK uses Zod schemas to validate events:

```typescript
const EventSchemas = z.discriminatedUnion("type", [
  TextMessageStartEventSchema,
  TextMessageContentEventSchema,
  TextMessageEndEventSchema,
  ToolCallStartEventSchema,
  ToolCallArgsEventSchema,
  ToolCallEndEventSchema,
  ToolCallResultEventSchema,
  StateSnapshotEventSchema,
  StateDeltaEventSchema,
  MessagesSnapshotEventSchema,
  RawEventSchema,
  CustomEventSchema,
  RunStartedEventSchema,
  RunFinishedEventSchema,
  RunErrorEventSchema,
  StepStartedEventSchema,
  StepFinishedEventSchema,
])
```

This allows for runtime validation of events and provides TypeScript type
inference.



================================================
FILE: docs/sdk/js/core/overview.mdx
================================================
---
title: "Overview"
description: "Core concepts in the Agent User Interaction Protocol SDK"
---

# @ag-ui/core

The Agent User Interaction Protocol SDK uses a streaming event-based
architecture with strongly typed data structures. This package provides the
foundation for connecting to agent systems.

```bash
npm install @ag-ui/core
```

## Types

Core data structures that represent the building blocks of the system:

- [RunAgentInput](/sdk/js/core/types#runagentinput) - Input parameters for
  running agents
- [Message](/sdk/js/core/types#message-types) - User assistant communication and
  tool usage
- [Context](/sdk/js/core/types#context) - Contextual information provided to
  agents
- [Tool](/sdk/js/core/types#tool) - Defines functions that agents can call
- [State](/sdk/js/core/types#state) - Agent state management

<Card
  title="Types Reference"
  icon="cube"
  href="/sdk/js/core/types"
  color="#3B82F6"
  iconType="solid"
>
  Complete documentation of all types in the @ag-ui/core package
</Card>

## Events

Events that power communication between agents and frontends:

- [Lifecycle Events](/sdk/js/core/events#lifecycle-events) - Run and step
  tracking
- [Text Message Events](/sdk/js/core/events#text-message-events) - Assistant
  message streaming
- [Tool Call Events](/sdk/js/core/events#tool-call-events) - Function call
  lifecycle
- [State Management Events](/sdk/js/core/events#state-management-events) - Agent
  state updates
- [Special Events](/sdk/js/core/events#special-events) - Raw and custom events

<Card
  title="Events Reference"
  icon="cube"
  href="/sdk/js/core/events"
  color="#3B82F6"
  iconType="solid"
>
  Complete documentation of all events in the @ag-ui/core package
</Card>



================================================
FILE: docs/sdk/js/core/types.mdx
================================================
---
title: "Types"
description:
  "Documentation for the core types used in the Agent User Interaction Protocol
  SDK"
---

# Core Types

The Agent User Interaction Protocol SDK is built on a set of core types that
represent the fundamental structures used throughout the system. This page
documents these types and their properties.

## RunAgentInput

Input parameters for running an agent. In the HTTP API, this is the body of the
`POST` request.

```typescript
type RunAgentInput = {
  threadId: string
  runId: string
  state: any
  messages: Message[]
  tools: Tool[]
  context: Context[]
  forwardedProps: any
}
```

| Property         | Type        | Description                                    |
| ---------------- | ----------- | ---------------------------------------------- |
| `threadId`       | `string`    | ID of the conversation thread                  |
| `runId`          | `string`    | ID of the current run                          |
| `state`          | `any`       | Current state of the agent                     |
| `messages`       | `Message[]` | Array of messages in the conversation          |
| `tools`          | `Tool[]`    | Array of tools available to the agent          |
| `context`        | `Context[]` | Array of context objects provided to the agent |
| `forwardedProps` | `any`       | Additional properties forwarded to the agent   |

## Message Types

The SDK includes several message types that represent different kinds of
messages in the system.

### Role

Represents the possible roles a message sender can have.

```typescript
type Role = "developer" | "system" | "assistant" | "user" | "tool"
```

### DeveloperMessage

Represents a message from a developer.

```typescript
type DeveloperMessage = {
  id: string
  role: "developer"
  content: string
  name?: string
}
```

| Property  | Type          | Description                                      |
| --------- | ------------- | ------------------------------------------------ |
| `id`      | `string`      | Unique identifier for the message                |
| `role`    | `"developer"` | Role of the message sender, fixed as "developer" |
| `content` | `string`      | Text content of the message (required)           |
| `name`    | `string`      | Optional name of the sender                      |

### SystemMessage

Represents a system message.

```typescript
type SystemMessage = {
  id: string
  role: "system"
  content: string
  name?: string
}
```

| Property  | Type       | Description                                   |
| --------- | ---------- | --------------------------------------------- |
| `id`      | `string`   | Unique identifier for the message             |
| `role`    | `"system"` | Role of the message sender, fixed as "system" |
| `content` | `string`   | Text content of the message (required)        |
| `name`    | `string`   | Optional name of the sender                   |

### AssistantMessage

Represents a message from an assistant.

```typescript
type AssistantMessage = {
  id: string
  role: "assistant"
  content?: string
  name?: string
  toolCalls?: ToolCall[]
}
```

| Property    | Type                    | Description                                      |
| ----------- | ----------------------- | ------------------------------------------------ |
| `id`        | `string`                | Unique identifier for the message                |
| `role`      | `"assistant"`           | Role of the message sender, fixed as "assistant" |
| `content`   | `string` (optional)     | Text content of the message                      |
| `name`      | `string` (optional)     | Name of the sender                               |
| `toolCalls` | `ToolCall[]` (optional) | Tool calls made in this message                  |

### UserMessage

Represents a message from a user.

```typescript
type UserMessage = {
  id: string
  role: "user"
  content: string
  name?: string
}
```

| Property  | Type     | Description                                 |
| --------- | -------- | ------------------------------------------- |
| `id`      | `string` | Unique identifier for the message           |
| `role`    | `"user"` | Role of the message sender, fixed as "user" |
| `content` | `string` | Text content of the message (required)      |
| `name`    | `string` | Optional name of the sender                 |

### ToolMessage

Represents a message from a tool.

```typescript
type ToolMessage = {
  id: string
  content: string
  role: "tool"
  toolCallId: string
  error?: string
}
```

| Property     | Type     | Description                                  |
| ------------ | -------- | -------------------------------------------- |
| `id`         | `string` | Unique identifier for the message            |
| `content`    | `string` | Text content of the message                  |
| `role`       | `"tool"` | Role of the message sender, fixed as "tool"  |
| `toolCallId` | `string` | ID of the tool call this message responds to |
| `error`      | `string` | Error message if the tool call failed        |

### Message

A union type representing any type of message in the system.

```typescript
type Message =
  | DeveloperMessage
  | SystemMessage
  | AssistantMessage
  | UserMessage
  | ToolMessage
```

### ToolCall

Represents a tool call made by an agent.

```typescript
type ToolCall = {
  id: string
  type: "function"
  function: FunctionCall
}
```

| Property   | Type           | Description                              |
| ---------- | -------------- | ---------------------------------------- |
| `id`       | `string`       | Unique identifier for the tool call      |
| `type`     | `"function"`   | Type of the tool call, always "function" |
| `function` | `FunctionCall` | Details about the function being called  |

#### FunctionCall

Represents function name and arguments in a tool call.

```typescript
type FunctionCall = {
  name: string
  arguments: string
}
```

| Property    | Type     | Description                                      |
| ----------- | -------- | ------------------------------------------------ |
| `name`      | `string` | Name of the function to call                     |
| `arguments` | `string` | JSON-encoded string of arguments to the function |

## Context

Represents a piece of contextual information provided to an agent.

```typescript
type Context = {
  description: string
  value: string
}
```

| Property      | Type     | Description                                 |
| ------------- | -------- | ------------------------------------------- |
| `description` | `string` | Description of what this context represents |
| `value`       | `string` | The actual context value                    |

## Tool

Defines a tool that can be called by an agent.

```typescript
type Tool = {
  name: string
  description: string
  parameters: any // JSON Schema
}
```

| Property      | Type     | Description                                      |
| ------------- | -------- | ------------------------------------------------ |
| `name`        | `string` | Name of the tool                                 |
| `description` | `string` | Description of what the tool does                |
| `parameters`  | `any`    | JSON Schema defining the parameters for the tool |

## State

Represents the state of an agent during execution.

```typescript
type State = any
```

The state type is flexible and can hold any data structure needed by the agent
implementation.



================================================
FILE: docs/sdk/python/core/events.mdx
================================================
---
title: "Events"
description:
  "Documentation for the events used in the Agent User Interaction Protocol
  Python SDK"
---

# Events

The Agent User Interaction Protocol Python SDK uses a streaming event-based
architecture. Events are the fundamental units of communication between agents
and the frontend. This section documents the event types and their properties.

## EventType Enum

`from ag_ui.core import EventType`

The `EventType` enum defines all possible event types in the system:

```python
class EventType(str, Enum):
    TEXT_MESSAGE_START = "TEXT_MESSAGE_START"
    TEXT_MESSAGE_CONTENT = "TEXT_MESSAGE_CONTENT"
    TEXT_MESSAGE_END = "TEXT_MESSAGE_END"
    TOOL_CALL_START = "TOOL_CALL_START"
    TOOL_CALL_ARGS = "TOOL_CALL_ARGS"
    TOOL_CALL_END = "TOOL_CALL_END"
    TOOL_CALL_RESULT = "TOOL_CALL_RESULT"
    STATE_SNAPSHOT = "STATE_SNAPSHOT"
    STATE_DELTA = "STATE_DELTA"
    MESSAGES_SNAPSHOT = "MESSAGES_SNAPSHOT"
    RAW = "RAW"
    CUSTOM = "CUSTOM"
    RUN_STARTED = "RUN_STARTED"
    RUN_FINISHED = "RUN_FINISHED"
    RUN_ERROR = "RUN_ERROR"
    STEP_STARTED = "STEP_STARTED"
    STEP_FINISHED = "STEP_FINISHED"
```

## BaseEvent

`from ag_ui.core import BaseEvent`

All events inherit from the `BaseEvent` class, which provides common properties
shared across all event types.

```python
class BaseEvent(ConfiguredBaseModel):
    type: EventType
    timestamp: Optional[int] = None
    raw_event: Optional[Any] = None
```

| Property    | Type            | Description                                           |
| ----------- | --------------- | ----------------------------------------------------- |
| `type`      | `EventType`     | The type of event (discriminator field for the union) |
| `timestamp` | `Optional[int]` | Timestamp when the event was created                  |
| `raw_event` | `Optional[Any]` | Original event data if this event was transformed     |

## Lifecycle Events

These events represent the lifecycle of an agent run.

### RunStartedEvent

`from ag_ui.core import RunStartedEvent`

Signals the start of an agent run.

```python
class RunStartedEvent(BaseEvent):
    type: Literal[EventType.RUN_STARTED]
    thread_id: str
    run_id: str
```

| Property    | Type  | Description                   |
| ----------- | ----- | ----------------------------- |
| `thread_id` | `str` | ID of the conversation thread |
| `run_id`    | `str` | ID of the agent run           |

### RunFinishedEvent

`from ag_ui.core import RunFinishedEvent`

Signals the successful completion of an agent run.

```python
class RunFinishedEvent(BaseEvent):
    type: Literal[EventType.RUN_FINISHED]
    thread_id: str
    run_id: str
    result: Optional[Any] = None
```

| Property    | Type            | Description                    |
| ----------- | --------------- | ------------------------------ |
| `thread_id` | `str`           | ID of the conversation thread  |
| `run_id`    | `str`           | ID of the agent run            |
| `result`    | `Optional[Any]` | Result data from the agent run |

### RunErrorEvent

`from ag_ui.core import RunErrorEvent`

Signals an error during an agent run.

```python
class RunErrorEvent(BaseEvent):
    type: Literal[EventType.RUN_ERROR]
    message: str
    code: Optional[str] = None
```

| Property  | Type            | Description   |
| --------- | --------------- | ------------- |
| `message` | `str`           | Error message |
| `code`    | `Optional[str]` | Error code    |

### StepStartedEvent

`from ag_ui.core import StepStartedEvent`

Signals the start of a step within an agent run.

```python
class StepStartedEvent(BaseEvent):
    type: Literal[EventType.STEP_STARTED]
    step_name: str
```

| Property    | Type  | Description      |
| ----------- | ----- | ---------------- |
| `step_name` | `str` | Name of the step |

### StepFinishedEvent

`from ag_ui.core import StepFinishedEvent`

Signals the completion of a step within an agent run.

```python
class StepFinishedEvent(BaseEvent):
    type: Literal[EventType.STEP_FINISHED]
    step_name: str
```

| Property    | Type  | Description      |
| ----------- | ----- | ---------------- |
| `step_name` | `str` | Name of the step |

## Text Message Events

These events represent the lifecycle of text messages in a conversation.

### TextMessageStartEvent

`from ag_ui.core import TextMessageStartEvent`

Signals the start of a text message.

```python
class TextMessageStartEvent(BaseEvent):
    type: Literal[EventType.TEXT_MESSAGE_START]
    message_id: str
    role: Literal["assistant"]
```

| Property     | Type                   | Description                       |
| ------------ | ---------------------- | --------------------------------- |
| `message_id` | `str`                  | Unique identifier for the message |
| `role`       | `Literal["assistant"]` | Role is always "assistant"        |

### TextMessageContentEvent

`from ag_ui.core import TextMessageContentEvent`

Represents a chunk of content in a streaming text message.

```python
class TextMessageContentEvent(BaseEvent):
    type: Literal[EventType.TEXT_MESSAGE_CONTENT]
    message_id: str
    delta: str  # Non-empty string

    def model_post_init(self, __context):
        if len(self.delta) == 0:
            raise ValueError("Delta must not be an empty string")
```

| Property     | Type  | Description                               |
| ------------ | ----- | ----------------------------------------- |
| `message_id` | `str` | Matches the ID from TextMessageStartEvent |
| `delta`      | `str` | Text content chunk (non-empty)            |

### TextMessageEndEvent

`from ag_ui.core import TextMessageEndEvent`

Signals the end of a text message.

```python
class TextMessageEndEvent(BaseEvent):
    type: Literal[EventType.TEXT_MESSAGE_END]
    message_id: str
```

| Property     | Type  | Description                               |
| ------------ | ----- | ----------------------------------------- |
| `message_id` | `str` | Matches the ID from TextMessageStartEvent |

## Tool Call Events

These events represent the lifecycle of tool calls made by agents.

### ToolCallStartEvent

`from ag_ui.core import ToolCallStartEvent`

Signals the start of a tool call.

```python
class ToolCallStartEvent(BaseEvent):
    type: Literal[EventType.TOOL_CALL_START]
    tool_call_id: str
    tool_call_name: str
    parent_message_id: Optional[str] = None
```

| Property            | Type            | Description                         |
| ------------------- | --------------- | ----------------------------------- |
| `tool_call_id`      | `str`           | Unique identifier for the tool call |
| `tool_call_name`    | `str`           | Name of the tool being called       |
| `parent_message_id` | `Optional[str]` | ID of the parent message            |

### ToolCallArgsEvent

`from ag_ui.core import ToolCallArgsEvent`

Represents a chunk of argument data for a tool call.

```python
class ToolCallArgsEvent(BaseEvent):
    type: Literal[EventType.TOOL_CALL_ARGS]
    tool_call_id: str
    delta: str
```

| Property       | Type  | Description                            |
| -------------- | ----- | -------------------------------------- |
| `tool_call_id` | `str` | Matches the ID from ToolCallStartEvent |
| `delta`        | `str` | Argument data chunk                    |

### ToolCallEndEvent

`from ag_ui.core import ToolCallEndEvent`

Signals the end of a tool call.

```python
class ToolCallEndEvent(BaseEvent):
    type: Literal[EventType.TOOL_CALL_END]
    tool_call_id: str
```

| Property       | Type  | Description                            |
| -------------- | ----- | -------------------------------------- |
| `tool_call_id` | `str` | Matches the ID from ToolCallStartEvent |

### ToolCallResultEvent

`from ag_ui.core import ToolCallResultEvent`

Provides the result of a tool call execution.

```python
class ToolCallResultEvent(BaseEvent):
    message_id: str
    type: Literal[EventType.TOOL_CALL_RESULT]
    tool_call_id: str
    content: str
    role: Optional[Literal["tool"]] = None
```

| Property       | Type                        | Description                                                 |
| -------------- | --------------------------- | ----------------------------------------------------------- |
| `message_id`   | `str`                       | ID of the conversation message this result belongs to       |
| `tool_call_id` | `str`                       | Matches the ID from the corresponding ToolCallStartEvent    |
| `content`      | `str`                       | The actual result/output content from the tool execution    |
| `role`         | `Optional[Literal["tool"]]` | Optional role identifier, typically "tool" for tool results |

## State Management Events

These events are used to manage agent state.

### StateSnapshotEvent

`from ag_ui.core import StateSnapshotEvent`

Provides a complete snapshot of an agent's state.

```python
class StateSnapshotEvent(BaseEvent):
    type: Literal[EventType.STATE_SNAPSHOT]
    snapshot: State
```

| Property   | Type    | Description             |
| ---------- | ------- | ----------------------- |
| `snapshot` | `State` | Complete state snapshot |

### StateDeltaEvent

`from ag_ui.core import StateDeltaEvent`

Provides a partial update to an agent's state using JSON Patch.

```python
class StateDeltaEvent(BaseEvent):
    type: Literal[EventType.STATE_DELTA]
    delta: List[Any]  # JSON Patch (RFC 6902)
```

| Property | Type        | Description                    |
| -------- | ----------- | ------------------------------ |
| `delta`  | `List[Any]` | Array of JSON Patch operations |

### MessagesSnapshotEvent

`from ag_ui.core import MessagesSnapshotEvent`

Provides a snapshot of all messages in a conversation.

```python
class MessagesSnapshotEvent(BaseEvent):
    type: Literal[EventType.MESSAGES_SNAPSHOT]
    messages: List[Message]
```

| Property   | Type            | Description              |
| ---------- | --------------- | ------------------------ |
| `messages` | `List[Message]` | Array of message objects |

## Special Events

### RawEvent

`from ag_ui.core import RawEvent`

Used to pass through events from external systems.

```python
class RawEvent(BaseEvent):
    type: Literal[EventType.RAW]
    event: Any
    source: Optional[str] = None
```

| Property | Type            | Description         |
| -------- | --------------- | ------------------- |
| `event`  | `Any`           | Original event data |
| `source` | `Optional[str]` | Source of the event |

### CustomEvent

`from ag_ui.core import CustomEvent`

Used for application-specific custom events.

```python
class CustomEvent(BaseEvent):
    type: Literal[EventType.CUSTOM]
    name: str
    value: Any
```

| Property | Type  | Description                     |
| -------- | ----- | ------------------------------- |
| `name`   | `str` | Name of the custom event        |
| `value`  | `Any` | Value associated with the event |

## Event Discrimination

`from ag_ui.core import Event`

The SDK uses Pydantic's discriminated unions for event validation:

```python
Event = Annotated[
    Union[
        TextMessageStartEvent,
        TextMessageContentEvent,
        TextMessageEndEvent,
        ToolCallStartEvent,
        ToolCallArgsEvent,
        ToolCallEndEvent,
        ToolCallResultEvent,
        StateSnapshotEvent,
        StateDeltaEvent,
        MessagesSnapshotEvent,
        RawEvent,
        CustomEvent,
        RunStartedEvent,
        RunFinishedEvent,
        RunErrorEvent,
        StepStartedEvent,
        StepFinishedEvent,
    ],
    Field(discriminator="type")
]
```

This allows for runtime validation of events and type checking at development
time.



================================================
FILE: docs/sdk/python/core/overview.mdx
================================================
---
title: "Overview"
description: "Core concepts in the Agent User Interaction Protocol SDK"
---

```bash
pip install ag-ui-protocol
```

# ag_ui.core

The Agent User Interaction Protocol SDK uses a streaming event-based
architecture with strongly typed data structures. This package provides the
foundation for connecting to agent systems.

```python
from ag_ui.core import ...
```

## Types

Core data structures that represent the building blocks of the system:

- [RunAgentInput](/sdk/python/core/types#runagentinput) - Input parameters for
  running agents
- [Message](/sdk/python/core/types#message-types) - User assistant communication
  and tool usage
- [Context](/sdk/python/core/types#context) - Contextual information provided to
  agents
- [Tool](/sdk/python/core/types#tool) - Defines functions that agents can call
- [State](/sdk/python/core/types#state) - Agent state management

<Card
  title="Types Reference"
  icon="cube"
  href="/sdk/python/core/types"
  color="#3B82F6"
  iconType="solid"
>
  Complete documentation of all types in the ag_ui.core package
</Card>

## Events

Events that power communication between agents and frontends:

- [Lifecycle Events](/sdk/python/core/events#lifecycle-events) - Run and step
  tracking
- [Text Message Events](/sdk/python/core/events#text-message-events) - Assistant
  message streaming
- [Tool Call Events](/sdk/python/core/events#tool-call-events) - Function call
  lifecycle
- [State Management Events](/sdk/python/core/events#state-management-events) -
  Agent state updates
- [Special Events](/sdk/python/core/events#special-events) - Raw and custom
  events

<Card
  title="Events Reference"
  icon="cube"
  href="/sdk/python/core/events"
  color="#3B82F6"
  iconType="solid"
>
  Complete documentation of all events in the ag_ui.core package
</Card>



================================================
FILE: docs/sdk/python/core/types.mdx
================================================
---
title: "Types"
description:
  "Documentation for the core types used in the Agent User Interaction Protocol
  Python SDK"
---

# Core Types

The Agent User Interaction Protocol Python SDK is built on a set of core types
that represent the fundamental structures used throughout the system. This page
documents these types and their properties.

## RunAgentInput

`from ag_ui.core import RunAgentInput`

Input parameters for running an agent. In the HTTP API, this is the body of the
`POST` request.

```python
class RunAgentInput(ConfiguredBaseModel):
    thread_id: str
    run_id: str
    state: Any
    messages: List[Message]
    tools: List[Tool]
    context: List[Context]
    forwarded_props: Any
```

| Property          | Type            | Description                                   |
| ----------------- | --------------- | --------------------------------------------- |
| `thread_id`       | `str`           | ID of the conversation thread                 |
| `run_id`          | `str`           | ID of the current run                         |
| `state`           | `Any`           | Current state of the agent                    |
| `messages`        | `List[Message]` | List of messages in the conversation          |
| `tools`           | `List[Tool]`    | List of tools available to the agent          |
| `context`         | `List[Context]` | List of context objects provided to the agent |
| `forwarded_props` | `Any`           | Additional properties forwarded to the agent  |

## Message Types

The SDK includes several message types that represent different kinds of
messages in the system.

### Role

`from ag_ui.core import Role`

Represents the possible roles a message sender can have.

```python
Role = Literal["developer", "system", "assistant", "user", "tool"]
```

### DeveloperMessage

`from ag_ui.core import DeveloperMessage`

Represents a message from a developer.

```python
class DeveloperMessage(BaseMessage):
    role: Literal["developer"]
    content: str
```

| Property  | Type                   | Description                                      |
| --------- | ---------------------- | ------------------------------------------------ |
| `id`      | `str`                  | Unique identifier for the message                |
| `role`    | `Literal["developer"]` | Role of the message sender, fixed as "developer" |
| `content` | `str`                  | Text content of the message (required)           |
| `name`    | `Optional[str]`        | Optional name of the sender                      |

### SystemMessage

`from ag_ui.core import SystemMessage`

Represents a system message.

```python
class SystemMessage(BaseMessage):
    role: Literal["system"]
    content: str
```

| Property  | Type                | Description                                   |
| --------- | ------------------- | --------------------------------------------- |
| `id`      | `str`               | Unique identifier for the message             |
| `role`    | `Literal["system"]` | Role of the message sender, fixed as "system" |
| `content` | `str`               | Text content of the message (required)        |
| `name`    | `Optional[str]`     | Optional name of the sender                   |

### AssistantMessage

`from ag_ui.core import AssistantMessage`

Represents a message from an assistant.

```python
class AssistantMessage(BaseMessage):
    role: Literal["assistant"]
    content: Optional[str] = None
    tool_calls: Optional[List[ToolCall]] = None
```

| Property     | Type                       | Description                                      |
| ------------ | -------------------------- | ------------------------------------------------ |
| `id`         | `str`                      | Unique identifier for the message                |
| `role`       | `Literal["assistant"]`     | Role of the message sender, fixed as "assistant" |
| `content`    | `Optional[str]`            | Text content of the message                      |
| `name`       | `Optional[str]`            | Name of the sender                               |
| `tool_calls` | `Optional[List[ToolCall]]` | Tool calls made in this message                  |

### UserMessage

`from ag_ui.core import UserMessage`

Represents a message from a user.

```python
class UserMessage(BaseMessage):
    role: Literal["user"]
    content: str
```

| Property  | Type              | Description                                 |
| --------- | ----------------- | ------------------------------------------- |
| `id`      | `str`             | Unique identifier for the message           |
| `role`    | `Literal["user"]` | Role of the message sender, fixed as "user" |
| `content` | `str`             | Text content of the message (required)      |
| `name`    | `Optional[str]`   | Optional name of the sender                 |

### ToolMessage

`from ag_ui.core import ToolMessage`

Represents a message from a tool.

```python
class ToolMessage(ConfiguredBaseModel):
    id: str
    role: Literal["tool"]
    content: str
    tool_call_id: str
    error: Optional[str] = None
```

| Property       | Type              | Description                                  |
| -------------- | ----------------- | -------------------------------------------- |
| `id`           | `str`             | Unique identifier for the message            |
| `content`      | `str`             | Text content of the message                  |
| `role`         | `Literal["tool"]` | Role of the message sender, fixed as "tool"  |
| `tool_call_id` | `str`             | ID of the tool call this message responds to |
| `error`        | `Optional[str]`   | Error message if the tool call failed        |

### Message

`from ag_ui.core import Message`

A union type representing any type of message in the system.

```python
Message = Annotated[
    Union[DeveloperMessage, SystemMessage, AssistantMessage, UserMessage, ToolMessage],
    Field(discriminator="role")
]
```

### ToolCall

`from ag_ui.core import ToolCall`

Represents a tool call made by an agent.

```python
class ToolCall(ConfiguredBaseModel):
    id: str
    type: Literal["function"]
    function: FunctionCall
```

| Property   | Type                  | Description                              |
| ---------- | --------------------- | ---------------------------------------- |
| `id`       | `str`                 | Unique identifier for the tool call      |
| `type`     | `Literal["function"]` | Type of the tool call, always "function" |
| `function` | `FunctionCall`        | Details about the function being called  |

#### FunctionCall

`from ag_ui.core import FunctionCall`

Represents function name and arguments in a tool call.

```python
class FunctionCall(ConfiguredBaseModel):
    name: str
    arguments: str
```

| Property    | Type  | Description                                      |
| ----------- | ----- | ------------------------------------------------ |
| `name`      | `str` | Name of the function to call                     |
| `arguments` | `str` | JSON-encoded string of arguments to the function |

## Context

`from ag_ui.core import Context`

Represents a piece of contextual information provided to an agent.

```python
class Context(ConfiguredBaseModel):
    description: str
    value: str
```

| Property      | Type  | Description                                 |
| ------------- | ----- | ------------------------------------------- |
| `description` | `str` | Description of what this context represents |
| `value`       | `str` | The actual context value                    |

## Tool

`from ag_ui.core import Tool`

Defines a tool that can be called by an agent.

```python
class Tool(ConfiguredBaseModel):
    name: str
    description: str
    parameters: Any  # JSON Schema
```

| Property      | Type  | Description                                      |
| ------------- | ----- | ------------------------------------------------ |
| `name`        | `str` | Name of the tool                                 |
| `description` | `str` | Description of what the tool does                |
| `parameters`  | `Any` | JSON Schema defining the parameters for the tool |

## State

`from ag_ui.core import State`

Represents the state of an agent during execution.

```python
State = Any
```

The state type is flexible and can hold any data structure needed by the agent
implementation.



================================================
FILE: docs/sdk/python/encoder/overview.mdx
================================================
---
title: "Overview"
description: "Documentation for encoding Agent User Interaction Protocol events"
---

```bash
pip install ag-ui-protocol
```

# Event Encoder

The Agent User Interaction Protocol uses a streaming approach to send events
from agents to clients. The `EventEncoder` class provides the functionality to
encode events into a format that can be sent over HTTP.

## EventEncoder

`from ag_ui.encoder import EventEncoder`

The `EventEncoder` class is responsible for encoding `BaseEvent` objects into
string representations that can be transmitted to clients.

```python
from ag_ui.core import BaseEvent
from ag_ui.encoder import EventEncoder

# Initialize the encoder
encoder = EventEncoder()

# Encode an event
encoded_event = encoder.encode(event)
```

### Usage

The `EventEncoder` is typically used in HTTP handlers to convert event objects
into a stream of data. The current implementation encodes events as Server-Sent
Events (SSE), which can be consumed by clients using the EventSource API.

### Methods

#### `__init__(accept: str = None)`

Creates a new encoder instance.

| Parameter | Type             | Description                         |
| --------- | ---------------- | ----------------------------------- |
| `accept`  | `str` (optional) | Content type accepted by the client |

#### `encode(event: BaseEvent) -> str`

Encodes an event into a string representation.

| Parameter | Type        | Description         |
| --------- | ----------- | ------------------- |
| `event`   | `BaseEvent` | The event to encode |

**Returns**: A string representation of the event in SSE format.

### Example

```python
from ag_ui.core import TextMessageContentEvent, EventType
from ag_ui.encoder import EventEncoder

# Create an event
event = TextMessageContentEvent(
    type=EventType.TEXT_MESSAGE_CONTENT,
    message_id="msg_123",
    delta="Hello, world!"
)

# Initialize the encoder
encoder = EventEncoder()

# Encode the event
encoded_event = encoder.encode(event)
print(encoded_event)
# Output: data: {"type":"TEXT_MESSAGE_CONTENT","messageId":"msg_123","delta":"Hello, world!"}\n\n
```

### Implementation Details

Internally, the encoder converts events to JSON and formats them as Server-Sent
Events with the following structure:

```
data: {json-serialized event}\n\n
```

This format allows clients to receive a continuous stream of events and process
them as they arrive.



================================================
FILE: docs/snippets/snippet-intro.mdx
================================================
One of the core principles of software development is DRY (Don't Repeat
Yourself). This is a principle that apply to documentation as well. If you find
yourself repeating the same content in multiple places, you should consider
creating a custom snippet to keep your content in sync.



================================================
FILE: docs/tutorials/cursor.mdx
================================================
---
title: "Developing with Cursor"
description: "Use Cursor to build AG-UI implementations faster"
---

This guide will help you set up Cursor to help you build custom Agent User
Interaction Protocol (AG-UI) servers and clients faster. The same principles
apply to other IDE's like Windsurf, VSCode, etc.

## Adding the documentation to Cursor

1. Open up the Cursor settings
2. Go to Features > Docs and click "+ Add new Doc"
3. Paste in the following URL: https://docs.ag-ui.com/llms-full.txt
4. Click "Add"

## Using the documentation

Now you can use the documentation to help you build your AG-UI project. Load the
docs into the current prompt by typing the `@` symbol, selecting "Docs" and then
selecting "Agent User Interaction Protocol" from the list. Happy coding!

## Best practices

When building AG-UI servers with Cursor:

- Break down complex problems into smaller steps
- Have a look at what the agent was doing by checking which files it edited
  (above the chat input)
- Let the agent write unit tests to verify your implementation
- Follow AG-UI protocol specifications carefully



================================================
FILE: docs/tutorials/debugging.mdx
================================================
---
title: Debugging
description:
  A comprehensive guide to debugging Agent User Interaction Protocol (AG-UI)
  integrations
---

# Debugging AG-UI Integrations

Debugging agent-based applications can be challenging, especially when working
with real-time, event-driven protocols like AG-UI. This guide introduces you to
the AG-UI Dojo, a powerful tool for learning, testing, and debugging your AG-UI
implementations.

## The AG-UI Dojo

The AG-UI Dojo is the best way to bring AG-UI to a new surface, and is also an
excellent resource for learning about the protocol's basic capabilities. It
provides a structured environment where you can test and validate each component
of the AG-UI protocol.

### What is the Dojo?

The Dojo consists of a series of "hello world"-sized demonstrations for the
different building blocks available via AG-UI. Each demonstration:

1. Shows a specific AG-UI capability in action
2. Presents both the user-visible interaction and the underlying code side by
   side
3. Allows you to test and verify your implementation

### Using the Dojo as an Implementation Checklist

When working on bringing AG-UI to a new surface or platform, you can use the
Dojo as a comprehensive "todo list":

1. Work through each demonstration one by one
2. Implement and test each AG-UI building block in your environment
3. When all demonstrations work correctly in your implementation, you can be
   confident that full-featured copilots and agent-native applications can be
   built on your new surface

This methodical approach ensures you've covered all the necessary functionality
required for a complete AG-UI implementation.

### Using the Dojo as a Learning Resource

For developers new to AG-UI, the Dojo serves as an interactive learning
resource:

- Each item demonstrates a specific AG-UI capability
- You can see both what the interaction looks like from a user perspective
- The underlying code is shown alongside, helping you understand how it works
- The incremental complexity helps build understanding from basics to advanced
  features

### Common Debugging Patterns

When using the Dojo for debugging your AG-UI implementation, keep these patterns
in mind:

1. **Event Sequence Issues**: Verify that events are being emitted in the
   correct order and with proper nesting (e.g., `TEXT_MESSAGE_START` before
   `TEXT_MESSAGE_CONTENT`)

2. **Data Format Problems**: Ensure your event payloads match the expected
   structure for each event type

3. **Transport Layer Debugging**: Check that your chosen transport mechanism
   (SSE, WebSockets, etc.) is correctly delivering events

4. **State Synchronization**: Confirm that state updates are correctly applied
   using snapshots and deltas

5. **Tool Execution**: Verify that tool calls and responses are properly
   formatted and processed

## Getting Started with the Dojo

To start using the AG-UI Dojo:

1. Clone the repository from
   [github.com/ag-ui-protocol/ag-ui](https://github.com/ag-ui-protocol/ag-ui)
2. Follow the setup instructions in the README
3. Start working through the demonstrations in order
4. Use the provided test cases to validate your implementation

The Dojo's structured approach makes it an invaluable resource for both learning
AG-UI and ensuring your implementation meets all requirements.


